{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.0.2 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/Users/apple/Library/Python/3.9/lib/python/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/Users/apple/Library/Python/3.9/lib/python/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/Users/apple/Library/Python/3.9/lib/python/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/Users/apple/Library/Python/3.9/lib/python/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py\", line 596, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py\", line 1890, in _run_once\n",
      "    handle._run()\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/Users/apple/Library/Python/3.9/lib/python/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/Users/apple/Library/Python/3.9/lib/python/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/Users/apple/Library/Python/3.9/lib/python/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/Users/apple/Library/Python/3.9/lib/python/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/Users/apple/Library/Python/3.9/lib/python/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/Users/apple/Library/Python/3.9/lib/python/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/Users/apple/Library/Python/3.9/lib/python/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/Users/apple/Library/Python/3.9/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3048, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/Users/apple/Library/Python/3.9/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3103, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/Users/apple/Library/Python/3.9/lib/python/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/Users/apple/Library/Python/3.9/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3308, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/Users/apple/Library/Python/3.9/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3490, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/Users/apple/Library/Python/3.9/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3550, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/9g/8dsxjdvs77q89tllxqcd765m0000gn/T/ipykernel_52814/931806832.py\", line 3, in <module>\n",
      "    import torch\n",
      "  File \"/Users/apple/Library/Python/3.9/lib/python/site-packages/torch/__init__.py\", line 1477, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"/Users/apple/Library/Python/3.9/lib/python/site-packages/torch/functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"/Users/apple/Library/Python/3.9/lib/python/site-packages/torch/nn/__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"/Users/apple/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"/Users/apple/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "/Users/apple/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
     ]
    }
   ],
   "source": [
    "#!matplotlib inline\n",
    "import numpy as np\n",
    "import torch\n",
    "torch.set_printoptions(edgeitems=2, linewidth=75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_c = [0.5,  14.0, 15.0, 28.0, 11.0,  8.0,  3.0, -4.0,  6.0, 13.0, 21.0]\n",
    "t_u = [35.7, 55.9, 58.2, 81.9, 56.3, 48.9, 33.9, 21.8, 48.4, 60.4, 68.4]\n",
    "t_c = torch.tensor(t_c)\n",
    "t_u = torch.tensor(t_u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(t_u, w, b):\n",
    "    return w * t_u + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 손실 함수 설정 (mean_squared_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(t_p, t_c):\n",
    "    squared_diffs = (t_p - t_c)**2\n",
    "    return squared_diffs.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([35.7000, 55.9000, 58.2000, 81.9000, 56.3000, 48.9000, 33.9000,\n",
       "        21.8000, 48.4000, 60.4000, 68.4000])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = torch.ones(())\n",
    "b = torch.zeros(())\n",
    "\n",
    "t_p = model(t_u, w , b)\n",
    "t_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1763.8848)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = loss_fn(t_p, t_c)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 손실 줄이기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 편미분 공식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bias 값\n",
    "delta = 0.1\n",
    "\n",
    "loss_rate_of_change_w = (loss_fn(model(t_u, w + delta,b), t_c) - loss_fn(model(t_u, w + delta,b), t_c)) / (2.0 * delta) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "learning_rate = 1e-2\n",
    "\n",
    "w = w - learning_rate * loss_rate_of_change_w\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.8260)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_rate_of_change_b = (loss_fn(model(t_u, w, b + delta), t_c) - loss_fn(model(t_u, w,b - delta), t_c)) / (2.0 * delta) \n",
    "b = b - learning_rate * loss_rate_of_change_b\n",
    "b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* MSE 공식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(t_p , t_c):\n",
    "    squared_diffs = (t_p - t_c) ** 2\n",
    "    return squared_diffs.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSE를 미분하여 모델 파라미터의 기울기를 구할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dloss_fn(t_p , t_c):\n",
    "    dsq_diff = 2 * (t_p - t_c)  / t_p.size(0)\n",
    "    return dsq_diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델에 미분 적용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 생성\n",
    "def model(t_u , w , b):\n",
    "    return t_u * w + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 가중치 w 에 대한 편미분\n",
    "    - t_u * w  + b 공식에서 w 에 대한 편미분을 진행했으므로 t_u 만 남는다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dmodel_dw(t_u, w , b):\n",
    "    return t_u\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 편향 b에 대한 편미분\n",
    "    - t_u * w 공식에서 b에 대한 편미분을 진행햤는데 b는 상수이므로 1.0 만 남는다 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dmodel_db(t_u, w , b):\n",
    "    return 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 경사 함수 정의\n",
    "- 모델이 학습할 때 필요한 기울기 계산\n",
    "- 모델의 파라미터에 대한 손실 함수의 기울기를 계산\n",
    "- 미분 연쇄 법칙이 적용\n",
    "    - 복합함수의 미분을 구할 때 각 함수의 미분을 곱하는 방법\n",
    "    - 여기서는 weight 와 bias 가 기울기(mse의 미분값)와 곱해진다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_fn(t_u ,t_c, t_p, w , b):\n",
    "    dloss_dtp = dloss_fn(t_p, t_c)\n",
    "    dloss_dw = dloss_dtp * dmodel_dw(t_u, w , b)\n",
    "    dloss_db = dloss_dtp *  dmodel_db(t_u, w , b)\n",
    "    return torch.stack([dloss_dw.sum(), dloss_db.sum()])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 적합을 위하여 반복"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs , learning_rate , params, t_u, t_c):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        w , b = params\n",
    "        \n",
    "        t_p = model(t_u , w , b) #순방향 전달\n",
    "        loss = loss_fn(t_p , t_c) #Mean squeared Error\n",
    "        grad = grad_fn(t_u ,t_c, t_p, w , b) #역방향 전달\n",
    "\n",
    "        # 파라미터 조정\n",
    "        params = params - learning_rate * grad\n",
    "\n",
    "        print(f'Epoch : {epoch}  Loss : {loss:.2f}  Grad : {grad}')\n",
    "    return params\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef training_loop(n_epochs, learning_rate, params, t_u, t_c,\\n                  print_params=True):\\n    for epoch in range(1, n_epochs + 1):\\n        w, b = params\\n\\n        t_p = model(t_u, w, b)  # <1>\\n        loss = loss_fn(t_p, t_c)\\n        grad = grad_fn(t_u, t_c, t_p, w, b)  # <2>\\n\\n        params = params - learning_rate * grad\\n\\n        if epoch in {1, 2, 3, 10, 11, 99, 100, 4000, 5000}:  # <3>\\n            print('Epoch %d, Loss %f' % (epoch, float(loss)))\\n            if print_params:\\n                print('    Params:', params)\\n                print('    Grad:  ', grad)\\n        if epoch in {4, 12, 101}:\\n            print('...')\\n\\n        if not torch.isfinite(loss).all():\\n            break  # <3>\\n            \\n    return params\\n\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def training_loop(n_epochs, learning_rate, params, t_u, t_c,\n",
    "                  print_params=True):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        w, b = params\n",
    "\n",
    "        t_p = model(t_u, w, b)  # <1>\n",
    "        loss = loss_fn(t_p, t_c)\n",
    "        grad = grad_fn(t_u, t_c, t_p, w, b)  # <2>\n",
    "\n",
    "        params = params - learning_rate * grad\n",
    "\n",
    "        if epoch in {1, 2, 3, 10, 11, 99, 100, 4000, 5000}:  # <3>\n",
    "            print('Epoch %d, Loss %f' % (epoch, float(loss)))\n",
    "            if print_params:\n",
    "                print('    Params:', params)\n",
    "                print('    Grad:  ', grad)\n",
    "        if epoch in {4, 12, 101}:\n",
    "            print('...')\n",
    "\n",
    "        if not torch.isfinite(loss).all():\n",
    "            break  # <3>\n",
    "            \n",
    "    return params\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 휸련 진행\n",
    "- loss가 무한대가 되어버린다\n",
    "- params 조정이 너무 크다는 신호\n",
    "    - 값이 앞뒤로 진동하면서 조정 값이 점점 커진다\n",
    "- 최적화는 불안정해지고 , 수렴이 아닌 발산 해버린다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1  Loss : 1763.88  Grad : tensor([4517.2964,   82.6000])\n",
      "Epoch : 2  Loss : 5802484.50  Grad : tensor([-261257.4062,   -4598.9702])\n",
      "Epoch : 3  Loss : 19408029696.00  Grad : tensor([15109614.0000,   266155.6875])\n",
      "Epoch : 4  Loss : 64915905708032.00  Grad : tensor([-8.7385e+08, -1.5393e+07])\n",
      "Epoch : 5  Loss : 217130525461053440.00  Grad : tensor([5.0539e+10, 8.9023e+08])\n",
      "Epoch : 6  Loss : 726257583152928129024.00  Grad : tensor([-2.9229e+12, -5.1486e+10])\n",
      "Epoch : 7  Loss : 2429183416467662896627712.00  Grad : tensor([1.6904e+14, 2.9776e+12])\n",
      "Epoch : 8  Loss : 8125122549611731432050262016.00  Grad : tensor([-9.7764e+15, -1.7221e+14])\n",
      "Epoch : 9  Loss : 27176882120842590626938030653440.00  Grad : tensor([5.6541e+17, 9.9596e+15])\n",
      "Epoch : 10  Loss : 90901105189019073810297959556841472.00  Grad : tensor([-3.2700e+19, -5.7600e+17])\n",
      "Epoch : 11  Loss : inf  Grad : tensor([1.8912e+21, 3.3313e+19])\n",
      "Epoch : 12  Loss : inf  Grad : tensor([-1.0937e+23, -1.9266e+21])\n",
      "Epoch : 13  Loss : inf  Grad : tensor([6.3256e+24, 1.1142e+23])\n",
      "Epoch : 14  Loss : inf  Grad : tensor([-3.6584e+26, -6.4441e+24])\n",
      "Epoch : 15  Loss : inf  Grad : tensor([2.1158e+28, 3.7269e+26])\n",
      "Epoch : 16  Loss : inf  Grad : tensor([-1.2236e+30, -2.1554e+28])\n",
      "Epoch : 17  Loss : inf  Grad : tensor([7.0769e+31, 1.2466e+30])\n",
      "Epoch : 18  Loss : inf  Grad : tensor([-4.0929e+33, -7.2095e+31])\n",
      "Epoch : 19  Loss : inf  Grad : tensor([2.3671e+35, 4.1695e+33])\n",
      "Epoch : 20  Loss : inf  Grad : tensor([-1.3690e+37, -2.4114e+35])\n",
      "Epoch : 21  Loss : inf  Grad : tensor([       inf, 1.3946e+37])\n",
      "Epoch : 22  Loss : inf  Grad : tensor([-inf, -inf])\n",
      "Epoch : 23  Loss : nan  Grad : tensor([nan, nan])\n",
      "Epoch : 24  Loss : nan  Grad : tensor([nan, nan])\n",
      "Epoch : 25  Loss : nan  Grad : tensor([nan, nan])\n",
      "Epoch : 26  Loss : nan  Grad : tensor([nan, nan])\n",
      "Epoch : 27  Loss : nan  Grad : tensor([nan, nan])\n",
      "Epoch : 28  Loss : nan  Grad : tensor([nan, nan])\n",
      "Epoch : 29  Loss : nan  Grad : tensor([nan, nan])\n",
      "Epoch : 30  Loss : nan  Grad : tensor([nan, nan])\n",
      "Epoch : 31  Loss : nan  Grad : tensor([nan, nan])\n",
      "Epoch : 32  Loss : nan  Grad : tensor([nan, nan])\n",
      "Epoch : 33  Loss : nan  Grad : tensor([nan, nan])\n",
      "Epoch : 34  Loss : nan  Grad : tensor([nan, nan])\n",
      "Epoch : 35  Loss : nan  Grad : tensor([nan, nan])\n",
      "Epoch : 36  Loss : nan  Grad : tensor([nan, nan])\n",
      "Epoch : 37  Loss : nan  Grad : tensor([nan, nan])\n",
      "Epoch : 38  Loss : nan  Grad : tensor([nan, nan])\n",
      "Epoch : 39  Loss : nan  Grad : tensor([nan, nan])\n",
      "Epoch : 40  Loss : nan  Grad : tensor([nan, nan])\n",
      "Epoch : 41  Loss : nan  Grad : tensor([nan, nan])\n",
      "Epoch : 42  Loss : nan  Grad : tensor([nan, nan])\n",
      "Epoch : 43  Loss : nan  Grad : tensor([nan, nan])\n",
      "Epoch : 44  Loss : nan  Grad : tensor([nan, nan])\n",
      "Epoch : 45  Loss : nan  Grad : tensor([nan, nan])\n",
      "Epoch : 46  Loss : nan  Grad : tensor([nan, nan])\n",
      "Epoch : 47  Loss : nan  Grad : tensor([nan, nan])\n",
      "Epoch : 48  Loss : nan  Grad : tensor([nan, nan])\n",
      "Epoch : 49  Loss : nan  Grad : tensor([nan, nan])\n",
      "Epoch : 50  Loss : nan  Grad : tensor([nan, nan])\n",
      "Epoch : 51  Loss : nan  Grad : tensor([nan, nan])\n",
      "Epoch : 52  Loss : nan  Grad : tensor([nan, nan])\n",
      "Epoch : 53  Loss : nan  Grad : tensor([nan, nan])\n",
      "Epoch : 54  Loss : nan  Grad : tensor([nan, nan])\n",
      "Epoch : 55  Loss : nan  Grad : tensor([nan, nan])\n",
      "Epoch : 56  Loss : nan  Grad : tensor([nan, nan])\n",
      "Epoch : 57  Loss : nan  Grad : tensor([nan, nan])\n",
      "Epoch : 58  Loss : nan  Grad : tensor([nan, nan])\n",
      "Epoch : 59  Loss : nan  Grad : tensor([nan, nan])\n",
      "Epoch : 60  Loss : nan  Grad : tensor([nan, nan])\n",
      "Epoch : 61  Loss : nan  Grad : tensor([nan, nan])\n",
      "Epoch : 62  Loss : nan  Grad : tensor([nan, nan])\n",
      "Epoch : 63  Loss : nan  Grad : tensor([nan, nan])\n",
      "Epoch : 64  Loss : nan  Grad : tensor([nan, nan])\n",
      "Epoch : 65  Loss : nan  Grad : tensor([nan, nan])\n",
      "Epoch : 66  Loss : nan  Grad : tensor([nan, nan])\n",
      "Epoch : 67  Loss : nan  Grad : tensor([nan, nan])\n",
      "Epoch : 68  Loss : nan  Grad : tensor([nan, nan])\n",
      "Epoch : 69  Loss : nan  Grad : tensor([nan, nan])\n",
      "Epoch : 70  Loss : nan  Grad : tensor([nan, nan])\n",
      "Epoch : 71  Loss : nan  Grad : tensor([nan, nan])\n",
      "Epoch : 72  Loss : nan  Grad : tensor([nan, nan])\n",
      "Epoch : 73  Loss : nan  Grad : tensor([nan, nan])\n",
      "Epoch : 74  Loss : nan  Grad : tensor([nan, nan])\n",
      "Epoch : 75  Loss : nan  Grad : tensor([nan, nan])\n",
      "Epoch : 76  Loss : nan  Grad : tensor([nan, nan])\n",
      "Epoch : 77  Loss : nan  Grad : tensor([nan, nan])\n",
      "Epoch : 78  Loss : nan  Grad : tensor([nan, nan])\n",
      "Epoch : 79  Loss : nan  Grad : tensor([nan, nan])\n",
      "Epoch : 80  Loss : nan  Grad : tensor([nan, nan])\n",
      "Epoch : 81  Loss : nan  Grad : tensor([nan, nan])\n",
      "Epoch : 82  Loss : nan  Grad : tensor([nan, nan])\n",
      "Epoch : 83  Loss : nan  Grad : tensor([nan, nan])\n",
      "Epoch : 84  Loss : nan  Grad : tensor([nan, nan])\n",
      "Epoch : 85  Loss : nan  Grad : tensor([nan, nan])\n",
      "Epoch : 86  Loss : nan  Grad : tensor([nan, nan])\n",
      "Epoch : 87  Loss : nan  Grad : tensor([nan, nan])\n",
      "Epoch : 88  Loss : nan  Grad : tensor([nan, nan])\n",
      "Epoch : 89  Loss : nan  Grad : tensor([nan, nan])\n",
      "Epoch : 90  Loss : nan  Grad : tensor([nan, nan])\n",
      "Epoch : 91  Loss : nan  Grad : tensor([nan, nan])\n",
      "Epoch : 92  Loss : nan  Grad : tensor([nan, nan])\n",
      "Epoch : 93  Loss : nan  Grad : tensor([nan, nan])\n",
      "Epoch : 94  Loss : nan  Grad : tensor([nan, nan])\n",
      "Epoch : 95  Loss : nan  Grad : tensor([nan, nan])\n",
      "Epoch : 96  Loss : nan  Grad : tensor([nan, nan])\n",
      "Epoch : 97  Loss : nan  Grad : tensor([nan, nan])\n",
      "Epoch : 98  Loss : nan  Grad : tensor([nan, nan])\n",
      "Epoch : 99  Loss : nan  Grad : tensor([nan, nan])\n",
      "Epoch : 100  Loss : nan  Grad : tensor([nan, nan])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([nan, nan])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_loop(\n",
    "    n_epochs = 100 ,\n",
    "    learning_rate = 1e-2 , \n",
    "    params = torch.tensor([1.0 ,0.0]), \n",
    "    t_u = t_u, \n",
    "    t_c = t_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 해결방법\n",
    "- learning_rate 를 줄인다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1  Loss : 1763.88  Grad : tensor([4517.2964,   82.6000])\n",
      "Epoch : 2  Loss : 323.09  Grad : tensor([1859.5493,   35.7843])\n",
      "Epoch : 3  Loss : 78.93  Grad : tensor([765.4666,  16.5122])\n",
      "Epoch : 4  Loss : 37.55  Grad : tensor([315.0790,   8.5787])\n",
      "Epoch : 5  Loss : 30.54  Grad : tensor([129.6733,   5.3127])\n",
      "Epoch : 6  Loss : 29.35  Grad : tensor([53.3495,  3.9682])\n",
      "Epoch : 7  Loss : 29.15  Grad : tensor([21.9304,  3.4148])\n",
      "Epoch : 8  Loss : 29.11  Grad : tensor([8.9964, 3.1869])\n",
      "Epoch : 9  Loss : 29.11  Grad : tensor([3.6721, 3.0930])\n",
      "Epoch : 10  Loss : 29.11  Grad : tensor([1.4803, 3.0544])\n",
      "Epoch : 11  Loss : 29.10  Grad : tensor([0.5781, 3.0384])\n",
      "Epoch : 12  Loss : 29.10  Grad : tensor([0.2066, 3.0318])\n",
      "Epoch : 13  Loss : 29.10  Grad : tensor([0.0537, 3.0291])\n",
      "Epoch : 14  Loss : 29.10  Grad : tensor([-0.0093,  3.0279])\n",
      "Epoch : 15  Loss : 29.10  Grad : tensor([-0.0353,  3.0274])\n",
      "Epoch : 16  Loss : 29.10  Grad : tensor([-0.0459,  3.0272])\n",
      "Epoch : 17  Loss : 29.10  Grad : tensor([-0.0502,  3.0270])\n",
      "Epoch : 18  Loss : 29.10  Grad : tensor([-0.0520,  3.0270])\n",
      "Epoch : 19  Loss : 29.10  Grad : tensor([-0.0528,  3.0269])\n",
      "Epoch : 20  Loss : 29.10  Grad : tensor([-0.0531,  3.0268])\n",
      "Epoch : 21  Loss : 29.09  Grad : tensor([-0.0533,  3.0268])\n",
      "Epoch : 22  Loss : 29.09  Grad : tensor([-0.0533,  3.0267])\n",
      "Epoch : 23  Loss : 29.09  Grad : tensor([-0.0533,  3.0267])\n",
      "Epoch : 24  Loss : 29.09  Grad : tensor([-0.0533,  3.0266])\n",
      "Epoch : 25  Loss : 29.09  Grad : tensor([-0.0533,  3.0266])\n",
      "Epoch : 26  Loss : 29.09  Grad : tensor([-0.0533,  3.0265])\n",
      "Epoch : 27  Loss : 29.09  Grad : tensor([-0.0533,  3.0265])\n",
      "Epoch : 28  Loss : 29.09  Grad : tensor([-0.0532,  3.0264])\n",
      "Epoch : 29  Loss : 29.09  Grad : tensor([-0.0533,  3.0264])\n",
      "Epoch : 30  Loss : 29.09  Grad : tensor([-0.0533,  3.0263])\n",
      "Epoch : 31  Loss : 29.09  Grad : tensor([-0.0532,  3.0262])\n",
      "Epoch : 32  Loss : 29.08  Grad : tensor([-0.0533,  3.0262])\n",
      "Epoch : 33  Loss : 29.08  Grad : tensor([-0.0533,  3.0261])\n",
      "Epoch : 34  Loss : 29.08  Grad : tensor([-0.0533,  3.0261])\n",
      "Epoch : 35  Loss : 29.08  Grad : tensor([-0.0532,  3.0260])\n",
      "Epoch : 36  Loss : 29.08  Grad : tensor([-0.0533,  3.0260])\n",
      "Epoch : 37  Loss : 29.08  Grad : tensor([-0.0533,  3.0259])\n",
      "Epoch : 38  Loss : 29.08  Grad : tensor([-0.0532,  3.0259])\n",
      "Epoch : 39  Loss : 29.08  Grad : tensor([-0.0533,  3.0258])\n",
      "Epoch : 40  Loss : 29.08  Grad : tensor([-0.0533,  3.0258])\n",
      "Epoch : 41  Loss : 29.08  Grad : tensor([-0.0533,  3.0257])\n",
      "Epoch : 42  Loss : 29.08  Grad : tensor([-0.0532,  3.0257])\n",
      "Epoch : 43  Loss : 29.07  Grad : tensor([-0.0533,  3.0256])\n",
      "Epoch : 44  Loss : 29.07  Grad : tensor([-0.0533,  3.0256])\n",
      "Epoch : 45  Loss : 29.07  Grad : tensor([-0.0533,  3.0255])\n",
      "Epoch : 46  Loss : 29.07  Grad : tensor([-0.0533,  3.0254])\n",
      "Epoch : 47  Loss : 29.07  Grad : tensor([-0.0533,  3.0254])\n",
      "Epoch : 48  Loss : 29.07  Grad : tensor([-0.0533,  3.0253])\n",
      "Epoch : 49  Loss : 29.07  Grad : tensor([-0.0533,  3.0253])\n",
      "Epoch : 50  Loss : 29.07  Grad : tensor([-0.0532,  3.0252])\n",
      "Epoch : 51  Loss : 29.07  Grad : tensor([-0.0533,  3.0252])\n",
      "Epoch : 52  Loss : 29.07  Grad : tensor([-0.0533,  3.0251])\n",
      "Epoch : 53  Loss : 29.07  Grad : tensor([-0.0533,  3.0251])\n",
      "Epoch : 54  Loss : 29.06  Grad : tensor([-0.0533,  3.0250])\n",
      "Epoch : 55  Loss : 29.06  Grad : tensor([-0.0532,  3.0250])\n",
      "Epoch : 56  Loss : 29.06  Grad : tensor([-0.0533,  3.0249])\n",
      "Epoch : 57  Loss : 29.06  Grad : tensor([-0.0532,  3.0249])\n",
      "Epoch : 58  Loss : 29.06  Grad : tensor([-0.0533,  3.0248])\n",
      "Epoch : 59  Loss : 29.06  Grad : tensor([-0.0533,  3.0248])\n",
      "Epoch : 60  Loss : 29.06  Grad : tensor([-0.0533,  3.0247])\n",
      "Epoch : 61  Loss : 29.06  Grad : tensor([-0.0533,  3.0247])\n",
      "Epoch : 62  Loss : 29.06  Grad : tensor([-0.0534,  3.0246])\n",
      "Epoch : 63  Loss : 29.06  Grad : tensor([-0.0533,  3.0245])\n",
      "Epoch : 64  Loss : 29.06  Grad : tensor([-0.0532,  3.0245])\n",
      "Epoch : 65  Loss : 29.05  Grad : tensor([-0.0533,  3.0244])\n",
      "Epoch : 66  Loss : 29.05  Grad : tensor([-0.0533,  3.0244])\n",
      "Epoch : 67  Loss : 29.05  Grad : tensor([-0.0533,  3.0243])\n",
      "Epoch : 68  Loss : 29.05  Grad : tensor([-0.0532,  3.0243])\n",
      "Epoch : 69  Loss : 29.05  Grad : tensor([-0.0533,  3.0242])\n",
      "Epoch : 70  Loss : 29.05  Grad : tensor([-0.0532,  3.0242])\n",
      "Epoch : 71  Loss : 29.05  Grad : tensor([-0.0533,  3.0241])\n",
      "Epoch : 72  Loss : 29.05  Grad : tensor([-0.0533,  3.0241])\n",
      "Epoch : 73  Loss : 29.05  Grad : tensor([-0.0532,  3.0240])\n",
      "Epoch : 74  Loss : 29.05  Grad : tensor([-0.0533,  3.0240])\n",
      "Epoch : 75  Loss : 29.05  Grad : tensor([-0.0532,  3.0239])\n",
      "Epoch : 76  Loss : 29.04  Grad : tensor([-0.0533,  3.0239])\n",
      "Epoch : 77  Loss : 29.04  Grad : tensor([-0.0533,  3.0238])\n",
      "Epoch : 78  Loss : 29.04  Grad : tensor([-0.0533,  3.0238])\n",
      "Epoch : 79  Loss : 29.04  Grad : tensor([-0.0533,  3.0237])\n",
      "Epoch : 80  Loss : 29.04  Grad : tensor([-0.0532,  3.0236])\n",
      "Epoch : 81  Loss : 29.04  Grad : tensor([-0.0534,  3.0236])\n",
      "Epoch : 82  Loss : 29.04  Grad : tensor([-0.0533,  3.0235])\n",
      "Epoch : 83  Loss : 29.04  Grad : tensor([-0.0532,  3.0235])\n",
      "Epoch : 84  Loss : 29.04  Grad : tensor([-0.0533,  3.0234])\n",
      "Epoch : 85  Loss : 29.04  Grad : tensor([-0.0533,  3.0234])\n",
      "Epoch : 86  Loss : 29.04  Grad : tensor([-0.0532,  3.0233])\n",
      "Epoch : 87  Loss : 29.03  Grad : tensor([-0.0533,  3.0233])\n",
      "Epoch : 88  Loss : 29.03  Grad : tensor([-0.0532,  3.0232])\n",
      "Epoch : 89  Loss : 29.03  Grad : tensor([-0.0533,  3.0232])\n",
      "Epoch : 90  Loss : 29.03  Grad : tensor([-0.0533,  3.0231])\n",
      "Epoch : 91  Loss : 29.03  Grad : tensor([-0.0532,  3.0231])\n",
      "Epoch : 92  Loss : 29.03  Grad : tensor([-0.0532,  3.0230])\n",
      "Epoch : 93  Loss : 29.03  Grad : tensor([-0.0533,  3.0230])\n",
      "Epoch : 94  Loss : 29.03  Grad : tensor([-0.0532,  3.0229])\n",
      "Epoch : 95  Loss : 29.03  Grad : tensor([-0.0533,  3.0229])\n",
      "Epoch : 96  Loss : 29.03  Grad : tensor([-0.0533,  3.0228])\n",
      "Epoch : 97  Loss : 29.03  Grad : tensor([-0.0532,  3.0227])\n",
      "Epoch : 98  Loss : 29.02  Grad : tensor([-0.0532,  3.0227])\n",
      "Epoch : 99  Loss : 29.02  Grad : tensor([-0.0533,  3.0226])\n",
      "Epoch : 100  Loss : 29.02  Grad : tensor([-0.0532,  3.0226])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 0.2327, -0.0438])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as no\n",
    "training_loop(\n",
    "    n_epochs = 100 ,\n",
    "    learning_rate = 1e-4 , \n",
    "    params = torch.tensor([1.0 ,0.0]), \n",
    "    t_u = t_u, \n",
    "    t_c = t_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 위의 코드에서 가중치에 대한 기울기는 편향에 대한 기울기보다 약 50배정도 큼\n",
    "    - 가중치와 편향값의 범위가 다르다는 뜻\n",
    "    - 각각 범위를 조정하지 않고 t_u 값을 정규화 해준다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_un = t_u * 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1  Loss : 80.36  Grad : tensor([-77.6140, -10.6400])\n",
      "Epoch : 2  Loss : 37.57  Grad : tensor([-30.8623,  -2.3864])\n",
      "Epoch : 3  Loss : 30.87  Grad : tensor([-12.4631,   0.8587])\n",
      "Epoch : 4  Loss : 29.76  Grad : tensor([-5.2218,  2.1327])\n",
      "Epoch : 5  Loss : 29.51  Grad : tensor([-2.3715,  2.6310])\n",
      "Epoch : 6  Loss : 29.39  Grad : tensor([-1.2492,  2.8241])\n",
      "Epoch : 7  Loss : 29.30  Grad : tensor([-0.8071,  2.8970])\n",
      "Epoch : 8  Loss : 29.21  Grad : tensor([-0.6325,  2.9227])\n",
      "Epoch : 9  Loss : 29.12  Grad : tensor([-0.5633,  2.9298])\n",
      "Epoch : 10  Loss : 29.03  Grad : tensor([-0.5355,  2.9295])\n",
      "Epoch : 11  Loss : 28.94  Grad : tensor([-0.5240,  2.9264])\n",
      "Epoch : 12  Loss : 28.85  Grad : tensor([-0.5190,  2.9222])\n",
      "Epoch : 13  Loss : 28.77  Grad : tensor([-0.5165,  2.9175])\n",
      "Epoch : 14  Loss : 28.68  Grad : tensor([-0.5150,  2.9126])\n",
      "Epoch : 15  Loss : 28.59  Grad : tensor([-0.5138,  2.9077])\n",
      "Epoch : 16  Loss : 28.50  Grad : tensor([-0.5129,  2.9028])\n",
      "Epoch : 17  Loss : 28.42  Grad : tensor([-0.5120,  2.8979])\n",
      "Epoch : 18  Loss : 28.33  Grad : tensor([-0.5111,  2.8930])\n",
      "Epoch : 19  Loss : 28.24  Grad : tensor([-0.5102,  2.8881])\n",
      "Epoch : 20  Loss : 28.16  Grad : tensor([-0.5093,  2.8832])\n",
      "Epoch : 21  Loss : 28.07  Grad : tensor([-0.5084,  2.8783])\n",
      "Epoch : 22  Loss : 27.99  Grad : tensor([-0.5076,  2.8734])\n",
      "Epoch : 23  Loss : 27.90  Grad : tensor([-0.5067,  2.8685])\n",
      "Epoch : 24  Loss : 27.82  Grad : tensor([-0.5059,  2.8636])\n",
      "Epoch : 25  Loss : 27.73  Grad : tensor([-0.5050,  2.8588])\n",
      "Epoch : 26  Loss : 27.65  Grad : tensor([-0.5042,  2.8539])\n",
      "Epoch : 27  Loss : 27.56  Grad : tensor([-0.5033,  2.8490])\n",
      "Epoch : 28  Loss : 27.48  Grad : tensor([-0.5024,  2.8442])\n",
      "Epoch : 29  Loss : 27.40  Grad : tensor([-0.5016,  2.8394])\n",
      "Epoch : 30  Loss : 27.31  Grad : tensor([-0.5007,  2.8346])\n",
      "Epoch : 31  Loss : 27.23  Grad : tensor([-0.4999,  2.8297])\n",
      "Epoch : 32  Loss : 27.15  Grad : tensor([-0.4990,  2.8249])\n",
      "Epoch : 33  Loss : 27.07  Grad : tensor([-0.4982,  2.8201])\n",
      "Epoch : 34  Loss : 26.98  Grad : tensor([-0.4973,  2.8153])\n",
      "Epoch : 35  Loss : 26.90  Grad : tensor([-0.4965,  2.8106])\n",
      "Epoch : 36  Loss : 26.82  Grad : tensor([-0.4957,  2.8058])\n",
      "Epoch : 37  Loss : 26.74  Grad : tensor([-0.4948,  2.8010])\n",
      "Epoch : 38  Loss : 26.66  Grad : tensor([-0.4940,  2.7963])\n",
      "Epoch : 39  Loss : 26.58  Grad : tensor([-0.4931,  2.7915])\n",
      "Epoch : 40  Loss : 26.50  Grad : tensor([-0.4923,  2.7868])\n",
      "Epoch : 41  Loss : 26.42  Grad : tensor([-0.4915,  2.7820])\n",
      "Epoch : 42  Loss : 26.34  Grad : tensor([-0.4906,  2.7773])\n",
      "Epoch : 43  Loss : 26.26  Grad : tensor([-0.4898,  2.7726])\n",
      "Epoch : 44  Loss : 26.18  Grad : tensor([-0.4890,  2.7679])\n",
      "Epoch : 45  Loss : 26.10  Grad : tensor([-0.4881,  2.7632])\n",
      "Epoch : 46  Loss : 26.02  Grad : tensor([-0.4873,  2.7585])\n",
      "Epoch : 47  Loss : 25.94  Grad : tensor([-0.4865,  2.7538])\n",
      "Epoch : 48  Loss : 25.87  Grad : tensor([-0.4856,  2.7491])\n",
      "Epoch : 49  Loss : 25.79  Grad : tensor([-0.4848,  2.7444])\n",
      "Epoch : 50  Loss : 25.71  Grad : tensor([-0.4840,  2.7398])\n",
      "Epoch : 51  Loss : 25.63  Grad : tensor([-0.4832,  2.7351])\n",
      "Epoch : 52  Loss : 25.56  Grad : tensor([-0.4823,  2.7305])\n",
      "Epoch : 53  Loss : 25.48  Grad : tensor([-0.4815,  2.7258])\n",
      "Epoch : 54  Loss : 25.40  Grad : tensor([-0.4807,  2.7212])\n",
      "Epoch : 55  Loss : 25.33  Grad : tensor([-0.4799,  2.7166])\n",
      "Epoch : 56  Loss : 25.25  Grad : tensor([-0.4791,  2.7120])\n",
      "Epoch : 57  Loss : 25.18  Grad : tensor([-0.4783,  2.7074])\n",
      "Epoch : 58  Loss : 25.10  Grad : tensor([-0.4775,  2.7028])\n",
      "Epoch : 59  Loss : 25.02  Grad : tensor([-0.4766,  2.6982])\n",
      "Epoch : 60  Loss : 24.95  Grad : tensor([-0.4758,  2.6936])\n",
      "Epoch : 61  Loss : 24.87  Grad : tensor([-0.4750,  2.6890])\n",
      "Epoch : 62  Loss : 24.80  Grad : tensor([-0.4742,  2.6845])\n",
      "Epoch : 63  Loss : 24.73  Grad : tensor([-0.4734,  2.6799])\n",
      "Epoch : 64  Loss : 24.65  Grad : tensor([-0.4726,  2.6753])\n",
      "Epoch : 65  Loss : 24.58  Grad : tensor([-0.4718,  2.6708])\n",
      "Epoch : 66  Loss : 24.50  Grad : tensor([-0.4710,  2.6663])\n",
      "Epoch : 67  Loss : 24.43  Grad : tensor([-0.4702,  2.6617])\n",
      "Epoch : 68  Loss : 24.36  Grad : tensor([-0.4694,  2.6572])\n",
      "Epoch : 69  Loss : 24.29  Grad : tensor([-0.4686,  2.6527])\n",
      "Epoch : 70  Loss : 24.21  Grad : tensor([-0.4678,  2.6482])\n",
      "Epoch : 71  Loss : 24.14  Grad : tensor([-0.4670,  2.6437])\n",
      "Epoch : 72  Loss : 24.07  Grad : tensor([-0.4662,  2.6392])\n",
      "Epoch : 73  Loss : 24.00  Grad : tensor([-0.4654,  2.6347])\n",
      "Epoch : 74  Loss : 23.93  Grad : tensor([-0.4646,  2.6302])\n",
      "Epoch : 75  Loss : 23.85  Grad : tensor([-0.4638,  2.6258])\n",
      "Epoch : 76  Loss : 23.78  Grad : tensor([-0.4631,  2.6213])\n",
      "Epoch : 77  Loss : 23.71  Grad : tensor([-0.4623,  2.6169])\n",
      "Epoch : 78  Loss : 23.64  Grad : tensor([-0.4615,  2.6124])\n",
      "Epoch : 79  Loss : 23.57  Grad : tensor([-0.4607,  2.6080])\n",
      "Epoch : 80  Loss : 23.50  Grad : tensor([-0.4599,  2.6035])\n",
      "Epoch : 81  Loss : 23.43  Grad : tensor([-0.4591,  2.5991])\n",
      "Epoch : 82  Loss : 23.36  Grad : tensor([-0.4584,  2.5947])\n",
      "Epoch : 83  Loss : 23.29  Grad : tensor([-0.4576,  2.5903])\n",
      "Epoch : 84  Loss : 23.22  Grad : tensor([-0.4568,  2.5859])\n",
      "Epoch : 85  Loss : 23.15  Grad : tensor([-0.4560,  2.5815])\n",
      "Epoch : 86  Loss : 23.09  Grad : tensor([-0.4553,  2.5771])\n",
      "Epoch : 87  Loss : 23.02  Grad : tensor([-0.4545,  2.5727])\n",
      "Epoch : 88  Loss : 22.95  Grad : tensor([-0.4537,  2.5684])\n",
      "Epoch : 89  Loss : 22.88  Grad : tensor([-0.4529,  2.5640])\n",
      "Epoch : 90  Loss : 22.81  Grad : tensor([-0.4522,  2.5597])\n",
      "Epoch : 91  Loss : 22.75  Grad : tensor([-0.4514,  2.5553])\n",
      "Epoch : 92  Loss : 22.68  Grad : tensor([-0.4506,  2.5510])\n",
      "Epoch : 93  Loss : 22.61  Grad : tensor([-0.4499,  2.5466])\n",
      "Epoch : 94  Loss : 22.54  Grad : tensor([-0.4491,  2.5423])\n",
      "Epoch : 95  Loss : 22.48  Grad : tensor([-0.4483,  2.5380])\n",
      "Epoch : 96  Loss : 22.41  Grad : tensor([-0.4476,  2.5337])\n",
      "Epoch : 97  Loss : 22.35  Grad : tensor([-0.4468,  2.5294])\n",
      "Epoch : 98  Loss : 22.28  Grad : tensor([-0.4461,  2.5251])\n",
      "Epoch : 99  Loss : 22.21  Grad : tensor([-0.4453,  2.5208])\n",
      "Epoch : 100  Loss : 22.15  Grad : tensor([-0.4446,  2.5165])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 2.7553, -2.5162])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as no\n",
    "training_loop(\n",
    "    n_epochs = 100 ,\n",
    "    learning_rate = 1e-2 , \n",
    "    params = torch.tensor([1.0 ,0.0]), \n",
    "    t_u = t_un, \n",
    "    t_c = t_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* parmams 값의 변화량이 줄어들 때까지 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1  Loss : 80.36  Grad : tensor([-77.6140, -10.6400])\n",
      "Epoch : 2  Loss : 37.57  Grad : tensor([-30.8623,  -2.3864])\n",
      "Epoch : 3  Loss : 30.87  Grad : tensor([-12.4631,   0.8587])\n",
      "Epoch : 4  Loss : 29.76  Grad : tensor([-5.2218,  2.1327])\n",
      "Epoch : 5  Loss : 29.51  Grad : tensor([-2.3715,  2.6310])\n",
      "Epoch : 6  Loss : 29.39  Grad : tensor([-1.2492,  2.8241])\n",
      "Epoch : 7  Loss : 29.30  Grad : tensor([-0.8071,  2.8970])\n",
      "Epoch : 8  Loss : 29.21  Grad : tensor([-0.6325,  2.9227])\n",
      "Epoch : 9  Loss : 29.12  Grad : tensor([-0.5633,  2.9298])\n",
      "Epoch : 10  Loss : 29.03  Grad : tensor([-0.5355,  2.9295])\n",
      "Epoch : 11  Loss : 28.94  Grad : tensor([-0.5240,  2.9264])\n",
      "Epoch : 12  Loss : 28.85  Grad : tensor([-0.5190,  2.9222])\n",
      "Epoch : 13  Loss : 28.77  Grad : tensor([-0.5165,  2.9175])\n",
      "Epoch : 14  Loss : 28.68  Grad : tensor([-0.5150,  2.9126])\n",
      "Epoch : 15  Loss : 28.59  Grad : tensor([-0.5138,  2.9077])\n",
      "Epoch : 16  Loss : 28.50  Grad : tensor([-0.5129,  2.9028])\n",
      "Epoch : 17  Loss : 28.42  Grad : tensor([-0.5120,  2.8979])\n",
      "Epoch : 18  Loss : 28.33  Grad : tensor([-0.5111,  2.8930])\n",
      "Epoch : 19  Loss : 28.24  Grad : tensor([-0.5102,  2.8881])\n",
      "Epoch : 20  Loss : 28.16  Grad : tensor([-0.5093,  2.8832])\n",
      "Epoch : 21  Loss : 28.07  Grad : tensor([-0.5084,  2.8783])\n",
      "Epoch : 22  Loss : 27.99  Grad : tensor([-0.5076,  2.8734])\n",
      "Epoch : 23  Loss : 27.90  Grad : tensor([-0.5067,  2.8685])\n",
      "Epoch : 24  Loss : 27.82  Grad : tensor([-0.5059,  2.8636])\n",
      "Epoch : 25  Loss : 27.73  Grad : tensor([-0.5050,  2.8588])\n",
      "Epoch : 26  Loss : 27.65  Grad : tensor([-0.5042,  2.8539])\n",
      "Epoch : 27  Loss : 27.56  Grad : tensor([-0.5033,  2.8490])\n",
      "Epoch : 28  Loss : 27.48  Grad : tensor([-0.5024,  2.8442])\n",
      "Epoch : 29  Loss : 27.40  Grad : tensor([-0.5016,  2.8394])\n",
      "Epoch : 30  Loss : 27.31  Grad : tensor([-0.5007,  2.8346])\n",
      "Epoch : 31  Loss : 27.23  Grad : tensor([-0.4999,  2.8297])\n",
      "Epoch : 32  Loss : 27.15  Grad : tensor([-0.4990,  2.8249])\n",
      "Epoch : 33  Loss : 27.07  Grad : tensor([-0.4982,  2.8201])\n",
      "Epoch : 34  Loss : 26.98  Grad : tensor([-0.4973,  2.8153])\n",
      "Epoch : 35  Loss : 26.90  Grad : tensor([-0.4965,  2.8106])\n",
      "Epoch : 36  Loss : 26.82  Grad : tensor([-0.4957,  2.8058])\n",
      "Epoch : 37  Loss : 26.74  Grad : tensor([-0.4948,  2.8010])\n",
      "Epoch : 38  Loss : 26.66  Grad : tensor([-0.4940,  2.7963])\n",
      "Epoch : 39  Loss : 26.58  Grad : tensor([-0.4931,  2.7915])\n",
      "Epoch : 40  Loss : 26.50  Grad : tensor([-0.4923,  2.7868])\n",
      "Epoch : 41  Loss : 26.42  Grad : tensor([-0.4915,  2.7820])\n",
      "Epoch : 42  Loss : 26.34  Grad : tensor([-0.4906,  2.7773])\n",
      "Epoch : 43  Loss : 26.26  Grad : tensor([-0.4898,  2.7726])\n",
      "Epoch : 44  Loss : 26.18  Grad : tensor([-0.4890,  2.7679])\n",
      "Epoch : 45  Loss : 26.10  Grad : tensor([-0.4881,  2.7632])\n",
      "Epoch : 46  Loss : 26.02  Grad : tensor([-0.4873,  2.7585])\n",
      "Epoch : 47  Loss : 25.94  Grad : tensor([-0.4865,  2.7538])\n",
      "Epoch : 48  Loss : 25.87  Grad : tensor([-0.4856,  2.7491])\n",
      "Epoch : 49  Loss : 25.79  Grad : tensor([-0.4848,  2.7444])\n",
      "Epoch : 50  Loss : 25.71  Grad : tensor([-0.4840,  2.7398])\n",
      "Epoch : 51  Loss : 25.63  Grad : tensor([-0.4832,  2.7351])\n",
      "Epoch : 52  Loss : 25.56  Grad : tensor([-0.4823,  2.7305])\n",
      "Epoch : 53  Loss : 25.48  Grad : tensor([-0.4815,  2.7258])\n",
      "Epoch : 54  Loss : 25.40  Grad : tensor([-0.4807,  2.7212])\n",
      "Epoch : 55  Loss : 25.33  Grad : tensor([-0.4799,  2.7166])\n",
      "Epoch : 56  Loss : 25.25  Grad : tensor([-0.4791,  2.7120])\n",
      "Epoch : 57  Loss : 25.18  Grad : tensor([-0.4783,  2.7074])\n",
      "Epoch : 58  Loss : 25.10  Grad : tensor([-0.4775,  2.7028])\n",
      "Epoch : 59  Loss : 25.02  Grad : tensor([-0.4766,  2.6982])\n",
      "Epoch : 60  Loss : 24.95  Grad : tensor([-0.4758,  2.6936])\n",
      "Epoch : 61  Loss : 24.87  Grad : tensor([-0.4750,  2.6890])\n",
      "Epoch : 62  Loss : 24.80  Grad : tensor([-0.4742,  2.6845])\n",
      "Epoch : 63  Loss : 24.73  Grad : tensor([-0.4734,  2.6799])\n",
      "Epoch : 64  Loss : 24.65  Grad : tensor([-0.4726,  2.6753])\n",
      "Epoch : 65  Loss : 24.58  Grad : tensor([-0.4718,  2.6708])\n",
      "Epoch : 66  Loss : 24.50  Grad : tensor([-0.4710,  2.6663])\n",
      "Epoch : 67  Loss : 24.43  Grad : tensor([-0.4702,  2.6617])\n",
      "Epoch : 68  Loss : 24.36  Grad : tensor([-0.4694,  2.6572])\n",
      "Epoch : 69  Loss : 24.29  Grad : tensor([-0.4686,  2.6527])\n",
      "Epoch : 70  Loss : 24.21  Grad : tensor([-0.4678,  2.6482])\n",
      "Epoch : 71  Loss : 24.14  Grad : tensor([-0.4670,  2.6437])\n",
      "Epoch : 72  Loss : 24.07  Grad : tensor([-0.4662,  2.6392])\n",
      "Epoch : 73  Loss : 24.00  Grad : tensor([-0.4654,  2.6347])\n",
      "Epoch : 74  Loss : 23.93  Grad : tensor([-0.4646,  2.6302])\n",
      "Epoch : 75  Loss : 23.85  Grad : tensor([-0.4638,  2.6258])\n",
      "Epoch : 76  Loss : 23.78  Grad : tensor([-0.4631,  2.6213])\n",
      "Epoch : 77  Loss : 23.71  Grad : tensor([-0.4623,  2.6169])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 78  Loss : 23.64  Grad : tensor([-0.4615,  2.6124])\n",
      "Epoch : 79  Loss : 23.57  Grad : tensor([-0.4607,  2.6080])\n",
      "Epoch : 80  Loss : 23.50  Grad : tensor([-0.4599,  2.6035])\n",
      "Epoch : 81  Loss : 23.43  Grad : tensor([-0.4591,  2.5991])\n",
      "Epoch : 82  Loss : 23.36  Grad : tensor([-0.4584,  2.5947])\n",
      "Epoch : 83  Loss : 23.29  Grad : tensor([-0.4576,  2.5903])\n",
      "Epoch : 84  Loss : 23.22  Grad : tensor([-0.4568,  2.5859])\n",
      "Epoch : 85  Loss : 23.15  Grad : tensor([-0.4560,  2.5815])\n",
      "Epoch : 86  Loss : 23.09  Grad : tensor([-0.4553,  2.5771])\n",
      "Epoch : 87  Loss : 23.02  Grad : tensor([-0.4545,  2.5727])\n",
      "Epoch : 88  Loss : 22.95  Grad : tensor([-0.4537,  2.5684])\n",
      "Epoch : 89  Loss : 22.88  Grad : tensor([-0.4529,  2.5640])\n",
      "Epoch : 90  Loss : 22.81  Grad : tensor([-0.4522,  2.5597])\n",
      "Epoch : 91  Loss : 22.75  Grad : tensor([-0.4514,  2.5553])\n",
      "Epoch : 92  Loss : 22.68  Grad : tensor([-0.4506,  2.5510])\n",
      "Epoch : 93  Loss : 22.61  Grad : tensor([-0.4499,  2.5466])\n",
      "Epoch : 94  Loss : 22.54  Grad : tensor([-0.4491,  2.5423])\n",
      "Epoch : 95  Loss : 22.48  Grad : tensor([-0.4483,  2.5380])\n",
      "Epoch : 96  Loss : 22.41  Grad : tensor([-0.4476,  2.5337])\n",
      "Epoch : 97  Loss : 22.35  Grad : tensor([-0.4468,  2.5294])\n",
      "Epoch : 98  Loss : 22.28  Grad : tensor([-0.4461,  2.5251])\n",
      "Epoch : 99  Loss : 22.21  Grad : tensor([-0.4453,  2.5208])\n",
      "Epoch : 100  Loss : 22.15  Grad : tensor([-0.4446,  2.5165])\n",
      "Epoch : 101  Loss : 22.08  Grad : tensor([-0.4438,  2.5122])\n",
      "Epoch : 102  Loss : 22.02  Grad : tensor([-0.4430,  2.5080])\n",
      "Epoch : 103  Loss : 21.95  Grad : tensor([-0.4423,  2.5037])\n",
      "Epoch : 104  Loss : 21.89  Grad : tensor([-0.4415,  2.4994])\n",
      "Epoch : 105  Loss : 21.82  Grad : tensor([-0.4408,  2.4952])\n",
      "Epoch : 106  Loss : 21.76  Grad : tensor([-0.4400,  2.4910])\n",
      "Epoch : 107  Loss : 21.70  Grad : tensor([-0.4393,  2.4867])\n",
      "Epoch : 108  Loss : 21.63  Grad : tensor([-0.4385,  2.4825])\n",
      "Epoch : 109  Loss : 21.57  Grad : tensor([-0.4378,  2.4783])\n",
      "Epoch : 110  Loss : 21.51  Grad : tensor([-0.4370,  2.4741])\n",
      "Epoch : 111  Loss : 21.44  Grad : tensor([-0.4363,  2.4699])\n",
      "Epoch : 112  Loss : 21.38  Grad : tensor([-0.4356,  2.4657])\n",
      "Epoch : 113  Loss : 21.32  Grad : tensor([-0.4348,  2.4615])\n",
      "Epoch : 114  Loss : 21.26  Grad : tensor([-0.4341,  2.4573])\n",
      "Epoch : 115  Loss : 21.19  Grad : tensor([-0.4334,  2.4531])\n",
      "Epoch : 116  Loss : 21.13  Grad : tensor([-0.4326,  2.4490])\n",
      "Epoch : 117  Loss : 21.07  Grad : tensor([-0.4319,  2.4448])\n",
      "Epoch : 118  Loss : 21.01  Grad : tensor([-0.4311,  2.4407])\n",
      "Epoch : 119  Loss : 20.95  Grad : tensor([-0.4304,  2.4365])\n",
      "Epoch : 120  Loss : 20.88  Grad : tensor([-0.4297,  2.4324])\n",
      "Epoch : 121  Loss : 20.82  Grad : tensor([-0.4290,  2.4282])\n",
      "Epoch : 122  Loss : 20.76  Grad : tensor([-0.4282,  2.4241])\n",
      "Epoch : 123  Loss : 20.70  Grad : tensor([-0.4275,  2.4200])\n",
      "Epoch : 124  Loss : 20.64  Grad : tensor([-0.4268,  2.4159])\n",
      "Epoch : 125  Loss : 20.58  Grad : tensor([-0.4260,  2.4118])\n",
      "Epoch : 126  Loss : 20.52  Grad : tensor([-0.4253,  2.4077])\n",
      "Epoch : 127  Loss : 20.46  Grad : tensor([-0.4246,  2.4036])\n",
      "Epoch : 128  Loss : 20.40  Grad : tensor([-0.4239,  2.3995])\n",
      "Epoch : 129  Loss : 20.34  Grad : tensor([-0.4232,  2.3954])\n",
      "Epoch : 130  Loss : 20.28  Grad : tensor([-0.4224,  2.3914])\n",
      "Epoch : 131  Loss : 20.23  Grad : tensor([-0.4217,  2.3873])\n",
      "Epoch : 132  Loss : 20.17  Grad : tensor([-0.4210,  2.3832])\n",
      "Epoch : 133  Loss : 20.11  Grad : tensor([-0.4203,  2.3792])\n",
      "Epoch : 134  Loss : 20.05  Grad : tensor([-0.4196,  2.3752])\n",
      "Epoch : 135  Loss : 19.99  Grad : tensor([-0.4189,  2.3711])\n",
      "Epoch : 136  Loss : 19.93  Grad : tensor([-0.4182,  2.3671])\n",
      "Epoch : 137  Loss : 19.88  Grad : tensor([-0.4174,  2.3631])\n",
      "Epoch : 138  Loss : 19.82  Grad : tensor([-0.4167,  2.3591])\n",
      "Epoch : 139  Loss : 19.76  Grad : tensor([-0.4160,  2.3550])\n",
      "Epoch : 140  Loss : 19.70  Grad : tensor([-0.4153,  2.3510])\n",
      "Epoch : 141  Loss : 19.65  Grad : tensor([-0.4146,  2.3471])\n",
      "Epoch : 142  Loss : 19.59  Grad : tensor([-0.4139,  2.3431])\n",
      "Epoch : 143  Loss : 19.53  Grad : tensor([-0.4132,  2.3391])\n",
      "Epoch : 144  Loss : 19.48  Grad : tensor([-0.4125,  2.3351])\n",
      "Epoch : 145  Loss : 19.42  Grad : tensor([-0.4118,  2.3311])\n",
      "Epoch : 146  Loss : 19.37  Grad : tensor([-0.4111,  2.3272])\n",
      "Epoch : 147  Loss : 19.31  Grad : tensor([-0.4104,  2.3232])\n",
      "Epoch : 148  Loss : 19.25  Grad : tensor([-0.4097,  2.3193])\n",
      "Epoch : 149  Loss : 19.20  Grad : tensor([-0.4090,  2.3153])\n",
      "Epoch : 150  Loss : 19.14  Grad : tensor([-0.4083,  2.3114])\n",
      "Epoch : 151  Loss : 19.09  Grad : tensor([-0.4076,  2.3075])\n",
      "Epoch : 152  Loss : 19.03  Grad : tensor([-0.4069,  2.3036])\n",
      "Epoch : 153  Loss : 18.98  Grad : tensor([-0.4062,  2.2997])\n",
      "Epoch : 154  Loss : 18.92  Grad : tensor([-0.4056,  2.2957])\n",
      "Epoch : 155  Loss : 18.87  Grad : tensor([-0.4049,  2.2918])\n",
      "Epoch : 156  Loss : 18.82  Grad : tensor([-0.4042,  2.2880])\n",
      "Epoch : 157  Loss : 18.76  Grad : tensor([-0.4035,  2.2841])\n",
      "Epoch : 158  Loss : 18.71  Grad : tensor([-0.4028,  2.2802])\n",
      "Epoch : 159  Loss : 18.65  Grad : tensor([-0.4021,  2.2763])\n",
      "Epoch : 160  Loss : 18.60  Grad : tensor([-0.4014,  2.2724])\n",
      "Epoch : 161  Loss : 18.55  Grad : tensor([-0.4007,  2.2686])\n",
      "Epoch : 162  Loss : 18.50  Grad : tensor([-0.4001,  2.2647])\n",
      "Epoch : 163  Loss : 18.44  Grad : tensor([-0.3994,  2.2609])\n",
      "Epoch : 164  Loss : 18.39  Grad : tensor([-0.3987,  2.2570])\n",
      "Epoch : 165  Loss : 18.34  Grad : tensor([-0.3980,  2.2532])\n",
      "Epoch : 166  Loss : 18.28  Grad : tensor([-0.3974,  2.2494])\n",
      "Epoch : 167  Loss : 18.23  Grad : tensor([-0.3967,  2.2456])\n",
      "Epoch : 168  Loss : 18.18  Grad : tensor([-0.3960,  2.2417])\n",
      "Epoch : 169  Loss : 18.13  Grad : tensor([-0.3953,  2.2379])\n",
      "Epoch : 170  Loss : 18.08  Grad : tensor([-0.3947,  2.2341])\n",
      "Epoch : 171  Loss : 18.03  Grad : tensor([-0.3940,  2.2303])\n",
      "Epoch : 172  Loss : 17.97  Grad : tensor([-0.3933,  2.2266])\n",
      "Epoch : 173  Loss : 17.92  Grad : tensor([-0.3927,  2.2228])\n",
      "Epoch : 174  Loss : 17.87  Grad : tensor([-0.3920,  2.2190])\n",
      "Epoch : 175  Loss : 17.82  Grad : tensor([-0.3913,  2.2152])\n",
      "Epoch : 176  Loss : 17.77  Grad : tensor([-0.3907,  2.2115])\n",
      "Epoch : 177  Loss : 17.72  Grad : tensor([-0.3900,  2.2077])\n",
      "Epoch : 178  Loss : 17.67  Grad : tensor([-0.3893,  2.2040])\n",
      "Epoch : 179  Loss : 17.62  Grad : tensor([-0.3887,  2.2002])\n",
      "Epoch : 180  Loss : 17.57  Grad : tensor([-0.3880,  2.1965])\n",
      "Epoch : 181  Loss : 17.52  Grad : tensor([-0.3873,  2.1927])\n",
      "Epoch : 182  Loss : 17.47  Grad : tensor([-0.3867,  2.1890])\n",
      "Epoch : 183  Loss : 17.42  Grad : tensor([-0.3860,  2.1853])\n",
      "Epoch : 184  Loss : 17.37  Grad : tensor([-0.3854,  2.1816])\n",
      "Epoch : 185  Loss : 17.32  Grad : tensor([-0.3847,  2.1779])\n",
      "Epoch : 186  Loss : 17.28  Grad : tensor([-0.3841,  2.1742])\n",
      "Epoch : 187  Loss : 17.23  Grad : tensor([-0.3834,  2.1705])\n",
      "Epoch : 188  Loss : 17.18  Grad : tensor([-0.3828,  2.1668])\n",
      "Epoch : 189  Loss : 17.13  Grad : tensor([-0.3821,  2.1631])\n",
      "Epoch : 190  Loss : 17.08  Grad : tensor([-0.3815,  2.1594])\n",
      "Epoch : 191  Loss : 17.03  Grad : tensor([-0.3808,  2.1558])\n",
      "Epoch : 192  Loss : 16.99  Grad : tensor([-0.3802,  2.1521])\n",
      "Epoch : 193  Loss : 16.94  Grad : tensor([-0.3795,  2.1485])\n",
      "Epoch : 194  Loss : 16.89  Grad : tensor([-0.3789,  2.1448])\n",
      "Epoch : 195  Loss : 16.84  Grad : tensor([-0.3782,  2.1412])\n",
      "Epoch : 196  Loss : 16.80  Grad : tensor([-0.3776,  2.1375])\n",
      "Epoch : 197  Loss : 16.75  Grad : tensor([-0.3770,  2.1339])\n",
      "Epoch : 198  Loss : 16.70  Grad : tensor([-0.3763,  2.1303])\n",
      "Epoch : 199  Loss : 16.65  Grad : tensor([-0.3757,  2.1267])\n",
      "Epoch : 200  Loss : 16.61  Grad : tensor([-0.3750,  2.1230])\n",
      "Epoch : 201  Loss : 16.56  Grad : tensor([-0.3744,  2.1194])\n",
      "Epoch : 202  Loss : 16.52  Grad : tensor([-0.3738,  2.1158])\n",
      "Epoch : 203  Loss : 16.47  Grad : tensor([-0.3731,  2.1122])\n",
      "Epoch : 204  Loss : 16.42  Grad : tensor([-0.3725,  2.1087])\n",
      "Epoch : 205  Loss : 16.38  Grad : tensor([-0.3719,  2.1051])\n",
      "Epoch : 206  Loss : 16.33  Grad : tensor([-0.3712,  2.1015])\n",
      "Epoch : 207  Loss : 16.29  Grad : tensor([-0.3706,  2.0979])\n",
      "Epoch : 208  Loss : 16.24  Grad : tensor([-0.3700,  2.0944])\n",
      "Epoch : 209  Loss : 16.20  Grad : tensor([-0.3694,  2.0908])\n",
      "Epoch : 210  Loss : 16.15  Grad : tensor([-0.3687,  2.0873])\n",
      "Epoch : 211  Loss : 16.11  Grad : tensor([-0.3681,  2.0837])\n",
      "Epoch : 212  Loss : 16.06  Grad : tensor([-0.3675,  2.0802])\n",
      "Epoch : 213  Loss : 16.02  Grad : tensor([-0.3668,  2.0766])\n",
      "Epoch : 214  Loss : 15.97  Grad : tensor([-0.3662,  2.0731])\n",
      "Epoch : 215  Loss : 15.93  Grad : tensor([-0.3656,  2.0696])\n",
      "Epoch : 216  Loss : 15.88  Grad : tensor([-0.3650,  2.0661])\n",
      "Epoch : 217  Loss : 15.84  Grad : tensor([-0.3644,  2.0626])\n",
      "Epoch : 218  Loss : 15.80  Grad : tensor([-0.3637,  2.0591])\n",
      "Epoch : 219  Loss : 15.75  Grad : tensor([-0.3631,  2.0556])\n",
      "Epoch : 220  Loss : 15.71  Grad : tensor([-0.3625,  2.0521])\n",
      "Epoch : 221  Loss : 15.67  Grad : tensor([-0.3619,  2.0486])\n",
      "Epoch : 222  Loss : 15.62  Grad : tensor([-0.3613,  2.0451])\n",
      "Epoch : 223  Loss : 15.58  Grad : tensor([-0.3607,  2.0416])\n",
      "Epoch : 224  Loss : 15.54  Grad : tensor([-0.3601,  2.0382])\n",
      "Epoch : 225  Loss : 15.49  Grad : tensor([-0.3594,  2.0347])\n",
      "Epoch : 226  Loss : 15.45  Grad : tensor([-0.3588,  2.0312])\n",
      "Epoch : 227  Loss : 15.41  Grad : tensor([-0.3582,  2.0278])\n",
      "Epoch : 228  Loss : 15.37  Grad : tensor([-0.3576,  2.0243])\n",
      "Epoch : 229  Loss : 15.32  Grad : tensor([-0.3570,  2.0209])\n",
      "Epoch : 230  Loss : 15.28  Grad : tensor([-0.3564,  2.0175])\n",
      "Epoch : 231  Loss : 15.24  Grad : tensor([-0.3558,  2.0140])\n",
      "Epoch : 232  Loss : 15.20  Grad : tensor([-0.3552,  2.0106])\n",
      "Epoch : 233  Loss : 15.16  Grad : tensor([-0.3546,  2.0072])\n",
      "Epoch : 234  Loss : 15.11  Grad : tensor([-0.3540,  2.0038])\n",
      "Epoch : 235  Loss : 15.07  Grad : tensor([-0.3534,  2.0004])\n",
      "Epoch : 236  Loss : 15.03  Grad : tensor([-0.3528,  1.9970])\n",
      "Epoch : 237  Loss : 14.99  Grad : tensor([-0.3522,  1.9936])\n",
      "Epoch : 238  Loss : 14.95  Grad : tensor([-0.3516,  1.9902])\n",
      "Epoch : 239  Loss : 14.91  Grad : tensor([-0.3510,  1.9868])\n",
      "Epoch : 240  Loss : 14.87  Grad : tensor([-0.3504,  1.9835])\n",
      "Epoch : 241  Loss : 14.83  Grad : tensor([-0.3498,  1.9801])\n",
      "Epoch : 242  Loss : 14.79  Grad : tensor([-0.3492,  1.9767])\n",
      "Epoch : 243  Loss : 14.75  Grad : tensor([-0.3486,  1.9734])\n",
      "Epoch : 244  Loss : 14.71  Grad : tensor([-0.3480,  1.9700])\n",
      "Epoch : 245  Loss : 14.67  Grad : tensor([-0.3474,  1.9667])\n",
      "Epoch : 246  Loss : 14.63  Grad : tensor([-0.3468,  1.9633])\n",
      "Epoch : 247  Loss : 14.59  Grad : tensor([-0.3462,  1.9600])\n",
      "Epoch : 248  Loss : 14.55  Grad : tensor([-0.3456,  1.9567])\n",
      "Epoch : 249  Loss : 14.51  Grad : tensor([-0.3451,  1.9533])\n",
      "Epoch : 250  Loss : 14.47  Grad : tensor([-0.3445,  1.9500])\n",
      "Epoch : 251  Loss : 14.43  Grad : tensor([-0.3439,  1.9467])\n",
      "Epoch : 252  Loss : 14.39  Grad : tensor([-0.3433,  1.9434])\n",
      "Epoch : 253  Loss : 14.35  Grad : tensor([-0.3427,  1.9401])\n",
      "Epoch : 254  Loss : 14.31  Grad : tensor([-0.3421,  1.9368])\n",
      "Epoch : 255  Loss : 14.27  Grad : tensor([-0.3416,  1.9335])\n",
      "Epoch : 256  Loss : 14.24  Grad : tensor([-0.3410,  1.9302])\n",
      "Epoch : 257  Loss : 14.20  Grad : tensor([-0.3404,  1.9269])\n",
      "Epoch : 258  Loss : 14.16  Grad : tensor([-0.3398,  1.9237])\n",
      "Epoch : 259  Loss : 14.12  Grad : tensor([-0.3392,  1.9204])\n",
      "Epoch : 260  Loss : 14.08  Grad : tensor([-0.3387,  1.9171])\n",
      "Epoch : 261  Loss : 14.05  Grad : tensor([-0.3381,  1.9139])\n",
      "Epoch : 262  Loss : 14.01  Grad : tensor([-0.3375,  1.9106])\n",
      "Epoch : 263  Loss : 13.97  Grad : tensor([-0.3369,  1.9074])\n",
      "Epoch : 264  Loss : 13.93  Grad : tensor([-0.3364,  1.9041])\n",
      "Epoch : 265  Loss : 13.90  Grad : tensor([-0.3358,  1.9009])\n",
      "Epoch : 266  Loss : 13.86  Grad : tensor([-0.3352,  1.8977])\n",
      "Epoch : 267  Loss : 13.82  Grad : tensor([-0.3347,  1.8945])\n",
      "Epoch : 268  Loss : 13.78  Grad : tensor([-0.3341,  1.8912])\n",
      "Epoch : 269  Loss : 13.75  Grad : tensor([-0.3335,  1.8880])\n",
      "Epoch : 270  Loss : 13.71  Grad : tensor([-0.3330,  1.8848])\n",
      "Epoch : 271  Loss : 13.67  Grad : tensor([-0.3324,  1.8816])\n",
      "Epoch : 272  Loss : 13.64  Grad : tensor([-0.3318,  1.8784])\n",
      "Epoch : 273  Loss : 13.60  Grad : tensor([-0.3313,  1.8752])\n",
      "Epoch : 274  Loss : 13.56  Grad : tensor([-0.3307,  1.8720])\n",
      "Epoch : 275  Loss : 13.53  Grad : tensor([-0.3301,  1.8689])\n",
      "Epoch : 276  Loss : 13.49  Grad : tensor([-0.3296,  1.8657])\n",
      "Epoch : 277  Loss : 13.46  Grad : tensor([-0.3290,  1.8625])\n",
      "Epoch : 278  Loss : 13.42  Grad : tensor([-0.3285,  1.8594])\n",
      "Epoch : 279  Loss : 13.39  Grad : tensor([-0.3279,  1.8562])\n",
      "Epoch : 280  Loss : 13.35  Grad : tensor([-0.3274,  1.8530])\n",
      "Epoch : 281  Loss : 13.31  Grad : tensor([-0.3268,  1.8499])\n",
      "Epoch : 282  Loss : 13.28  Grad : tensor([-0.3262,  1.8468])\n",
      "Epoch : 283  Loss : 13.24  Grad : tensor([-0.3257,  1.8436])\n",
      "Epoch : 284  Loss : 13.21  Grad : tensor([-0.3251,  1.8405])\n",
      "Epoch : 285  Loss : 13.17  Grad : tensor([-0.3246,  1.8374])\n",
      "Epoch : 286  Loss : 13.14  Grad : tensor([-0.3240,  1.8342])\n",
      "Epoch : 287  Loss : 13.10  Grad : tensor([-0.3235,  1.8311])\n",
      "Epoch : 288  Loss : 13.07  Grad : tensor([-0.3229,  1.8280])\n",
      "Epoch : 289  Loss : 13.04  Grad : tensor([-0.3224,  1.8249])\n",
      "Epoch : 290  Loss : 13.00  Grad : tensor([-0.3218,  1.8218])\n",
      "Epoch : 291  Loss : 12.97  Grad : tensor([-0.3213,  1.8187])\n",
      "Epoch : 292  Loss : 12.93  Grad : tensor([-0.3207,  1.8156])\n",
      "Epoch : 293  Loss : 12.90  Grad : tensor([-0.3202,  1.8125])\n",
      "Epoch : 294  Loss : 12.87  Grad : tensor([-0.3196,  1.8095])\n",
      "Epoch : 295  Loss : 12.83  Grad : tensor([-0.3191,  1.8064])\n",
      "Epoch : 296  Loss : 12.80  Grad : tensor([-0.3186,  1.8033])\n",
      "Epoch : 297  Loss : 12.76  Grad : tensor([-0.3180,  1.8003])\n",
      "Epoch : 298  Loss : 12.73  Grad : tensor([-0.3175,  1.7972])\n",
      "Epoch : 299  Loss : 12.70  Grad : tensor([-0.3169,  1.7941])\n",
      "Epoch : 300  Loss : 12.66  Grad : tensor([-0.3164,  1.7911])\n",
      "Epoch : 301  Loss : 12.63  Grad : tensor([-0.3159,  1.7881])\n",
      "Epoch : 302  Loss : 12.60  Grad : tensor([-0.3153,  1.7850])\n",
      "Epoch : 303  Loss : 12.57  Grad : tensor([-0.3148,  1.7820])\n",
      "Epoch : 304  Loss : 12.53  Grad : tensor([-0.3143,  1.7790])\n",
      "Epoch : 305  Loss : 12.50  Grad : tensor([-0.3137,  1.7759])\n",
      "Epoch : 306  Loss : 12.47  Grad : tensor([-0.3132,  1.7729])\n",
      "Epoch : 307  Loss : 12.44  Grad : tensor([-0.3127,  1.7699])\n",
      "Epoch : 308  Loss : 12.40  Grad : tensor([-0.3121,  1.7669])\n",
      "Epoch : 309  Loss : 12.37  Grad : tensor([-0.3116,  1.7639])\n",
      "Epoch : 310  Loss : 12.34  Grad : tensor([-0.3111,  1.7609])\n",
      "Epoch : 311  Loss : 12.31  Grad : tensor([-0.3105,  1.7579])\n",
      "Epoch : 312  Loss : 12.28  Grad : tensor([-0.3100,  1.7549])\n",
      "Epoch : 313  Loss : 12.24  Grad : tensor([-0.3095,  1.7519])\n",
      "Epoch : 314  Loss : 12.21  Grad : tensor([-0.3090,  1.7490])\n",
      "Epoch : 315  Loss : 12.18  Grad : tensor([-0.3084,  1.7460])\n",
      "Epoch : 316  Loss : 12.15  Grad : tensor([-0.3079,  1.7430])\n",
      "Epoch : 317  Loss : 12.12  Grad : tensor([-0.3074,  1.7401])\n",
      "Epoch : 318  Loss : 12.09  Grad : tensor([-0.3069,  1.7371])\n",
      "Epoch : 319  Loss : 12.06  Grad : tensor([-0.3063,  1.7342])\n",
      "Epoch : 320  Loss : 12.02  Grad : tensor([-0.3058,  1.7312])\n",
      "Epoch : 321  Loss : 11.99  Grad : tensor([-0.3053,  1.7283])\n",
      "Epoch : 322  Loss : 11.96  Grad : tensor([-0.3048,  1.7253])\n",
      "Epoch : 323  Loss : 11.93  Grad : tensor([-0.3043,  1.7224])\n",
      "Epoch : 324  Loss : 11.90  Grad : tensor([-0.3037,  1.7195])\n",
      "Epoch : 325  Loss : 11.87  Grad : tensor([-0.3032,  1.7166])\n",
      "Epoch : 326  Loss : 11.84  Grad : tensor([-0.3027,  1.7136])\n",
      "Epoch : 327  Loss : 11.81  Grad : tensor([-0.3022,  1.7107])\n",
      "Epoch : 328  Loss : 11.78  Grad : tensor([-0.3017,  1.7078])\n",
      "Epoch : 329  Loss : 11.75  Grad : tensor([-0.3012,  1.7049])\n",
      "Epoch : 330  Loss : 11.72  Grad : tensor([-0.3007,  1.7020])\n",
      "Epoch : 331  Loss : 11.69  Grad : tensor([-0.3002,  1.6991])\n",
      "Epoch : 332  Loss : 11.66  Grad : tensor([-0.2996,  1.6963])\n",
      "Epoch : 333  Loss : 11.63  Grad : tensor([-0.2991,  1.6934])\n",
      "Epoch : 334  Loss : 11.60  Grad : tensor([-0.2986,  1.6905])\n",
      "Epoch : 335  Loss : 11.57  Grad : tensor([-0.2981,  1.6876])\n",
      "Epoch : 336  Loss : 11.54  Grad : tensor([-0.2976,  1.6848])\n",
      "Epoch : 337  Loss : 11.51  Grad : tensor([-0.2971,  1.6819])\n",
      "Epoch : 338  Loss : 11.48  Grad : tensor([-0.2966,  1.6790])\n",
      "Epoch : 339  Loss : 11.46  Grad : tensor([-0.2961,  1.6762])\n",
      "Epoch : 340  Loss : 11.43  Grad : tensor([-0.2956,  1.6733])\n",
      "Epoch : 341  Loss : 11.40  Grad : tensor([-0.2951,  1.6705])\n",
      "Epoch : 342  Loss : 11.37  Grad : tensor([-0.2946,  1.6677])\n",
      "Epoch : 343  Loss : 11.34  Grad : tensor([-0.2941,  1.6648])\n",
      "Epoch : 344  Loss : 11.31  Grad : tensor([-0.2936,  1.6620])\n",
      "Epoch : 345  Loss : 11.28  Grad : tensor([-0.2931,  1.6592])\n",
      "Epoch : 346  Loss : 11.25  Grad : tensor([-0.2926,  1.6564])\n",
      "Epoch : 347  Loss : 11.23  Grad : tensor([-0.2921,  1.6535])\n",
      "Epoch : 348  Loss : 11.20  Grad : tensor([-0.2916,  1.6507])\n",
      "Epoch : 349  Loss : 11.17  Grad : tensor([-0.2911,  1.6479])\n",
      "Epoch : 350  Loss : 11.14  Grad : tensor([-0.2906,  1.6451])\n",
      "Epoch : 351  Loss : 11.11  Grad : tensor([-0.2901,  1.6423])\n",
      "Epoch : 352  Loss : 11.09  Grad : tensor([-0.2896,  1.6395])\n",
      "Epoch : 353  Loss : 11.06  Grad : tensor([-0.2892,  1.6368])\n",
      "Epoch : 354  Loss : 11.03  Grad : tensor([-0.2886,  1.6340])\n",
      "Epoch : 355  Loss : 11.00  Grad : tensor([-0.2882,  1.6312])\n",
      "Epoch : 356  Loss : 10.98  Grad : tensor([-0.2877,  1.6284])\n",
      "Epoch : 357  Loss : 10.95  Grad : tensor([-0.2872,  1.6257])\n",
      "Epoch : 358  Loss : 10.92  Grad : tensor([-0.2867,  1.6229])\n",
      "Epoch : 359  Loss : 10.89  Grad : tensor([-0.2862,  1.6201])\n",
      "Epoch : 360  Loss : 10.87  Grad : tensor([-0.2857,  1.6174])\n",
      "Epoch : 361  Loss : 10.84  Grad : tensor([-0.2852,  1.6146])\n",
      "Epoch : 362  Loss : 10.81  Grad : tensor([-0.2847,  1.6119])\n",
      "Epoch : 363  Loss : 10.79  Grad : tensor([-0.2843,  1.6092])\n",
      "Epoch : 364  Loss : 10.76  Grad : tensor([-0.2838,  1.6064])\n",
      "Epoch : 365  Loss : 10.73  Grad : tensor([-0.2833,  1.6037])\n",
      "Epoch : 366  Loss : 10.71  Grad : tensor([-0.2828,  1.6010])\n",
      "Epoch : 367  Loss : 10.68  Grad : tensor([-0.2823,  1.5983])\n",
      "Epoch : 368  Loss : 10.65  Grad : tensor([-0.2819,  1.5955])\n",
      "Epoch : 369  Loss : 10.63  Grad : tensor([-0.2814,  1.5928])\n",
      "Epoch : 370  Loss : 10.60  Grad : tensor([-0.2809,  1.5901])\n",
      "Epoch : 371  Loss : 10.58  Grad : tensor([-0.2804,  1.5874])\n",
      "Epoch : 372  Loss : 10.55  Grad : tensor([-0.2799,  1.5847])\n",
      "Epoch : 373  Loss : 10.52  Grad : tensor([-0.2795,  1.5820])\n",
      "Epoch : 374  Loss : 10.50  Grad : tensor([-0.2790,  1.5794])\n",
      "Epoch : 375  Loss : 10.47  Grad : tensor([-0.2785,  1.5767])\n",
      "Epoch : 376  Loss : 10.45  Grad : tensor([-0.2780,  1.5740])\n",
      "Epoch : 377  Loss : 10.42  Grad : tensor([-0.2776,  1.5713])\n",
      "Epoch : 378  Loss : 10.40  Grad : tensor([-0.2771,  1.5686])\n",
      "Epoch : 379  Loss : 10.37  Grad : tensor([-0.2766,  1.5660])\n",
      "Epoch : 380  Loss : 10.35  Grad : tensor([-0.2762,  1.5633])\n",
      "Epoch : 381  Loss : 10.32  Grad : tensor([-0.2757,  1.5607])\n",
      "Epoch : 382  Loss : 10.30  Grad : tensor([-0.2752,  1.5580])\n",
      "Epoch : 383  Loss : 10.27  Grad : tensor([-0.2748,  1.5554])\n",
      "Epoch : 384  Loss : 10.25  Grad : tensor([-0.2743,  1.5527])\n",
      "Epoch : 385  Loss : 10.22  Grad : tensor([-0.2738,  1.5501])\n",
      "Epoch : 386  Loss : 10.20  Grad : tensor([-0.2734,  1.5475])\n",
      "Epoch : 387  Loss : 10.17  Grad : tensor([-0.2729,  1.5448])\n",
      "Epoch : 388  Loss : 10.15  Grad : tensor([-0.2724,  1.5422])\n",
      "Epoch : 389  Loss : 10.12  Grad : tensor([-0.2720,  1.5396])\n",
      "Epoch : 390  Loss : 10.10  Grad : tensor([-0.2715,  1.5370])\n",
      "Epoch : 391  Loss : 10.07  Grad : tensor([-0.2711,  1.5344])\n",
      "Epoch : 392  Loss : 10.05  Grad : tensor([-0.2706,  1.5317])\n",
      "Epoch : 393  Loss : 10.02  Grad : tensor([-0.2701,  1.5291])\n",
      "Epoch : 394  Loss : 10.00  Grad : tensor([-0.2697,  1.5265])\n",
      "Epoch : 395  Loss : 9.98  Grad : tensor([-0.2692,  1.5240])\n",
      "Epoch : 396  Loss : 9.95  Grad : tensor([-0.2688,  1.5214])\n",
      "Epoch : 397  Loss : 9.93  Grad : tensor([-0.2683,  1.5188])\n",
      "Epoch : 398  Loss : 9.91  Grad : tensor([-0.2678,  1.5162])\n",
      "Epoch : 399  Loss : 9.88  Grad : tensor([-0.2674,  1.5136])\n",
      "Epoch : 400  Loss : 9.86  Grad : tensor([-0.2669,  1.5111])\n",
      "Epoch : 401  Loss : 9.83  Grad : tensor([-0.2665,  1.5085])\n",
      "Epoch : 402  Loss : 9.81  Grad : tensor([-0.2660,  1.5059])\n",
      "Epoch : 403  Loss : 9.79  Grad : tensor([-0.2656,  1.5034])\n",
      "Epoch : 404  Loss : 9.76  Grad : tensor([-0.2651,  1.5008])\n",
      "Epoch : 405  Loss : 9.74  Grad : tensor([-0.2647,  1.4983])\n",
      "Epoch : 406  Loss : 9.72  Grad : tensor([-0.2642,  1.4957])\n",
      "Epoch : 407  Loss : 9.69  Grad : tensor([-0.2638,  1.4932])\n",
      "Epoch : 408  Loss : 9.67  Grad : tensor([-0.2633,  1.4906])\n",
      "Epoch : 409  Loss : 9.65  Grad : tensor([-0.2629,  1.4881])\n",
      "Epoch : 410  Loss : 9.63  Grad : tensor([-0.2624,  1.4856])\n",
      "Epoch : 411  Loss : 9.60  Grad : tensor([-0.2620,  1.4831])\n",
      "Epoch : 412  Loss : 9.58  Grad : tensor([-0.2615,  1.4805])\n",
      "Epoch : 413  Loss : 9.56  Grad : tensor([-0.2611,  1.4780])\n",
      "Epoch : 414  Loss : 9.54  Grad : tensor([-0.2606,  1.4755])\n",
      "Epoch : 415  Loss : 9.51  Grad : tensor([-0.2602,  1.4730])\n",
      "Epoch : 416  Loss : 9.49  Grad : tensor([-0.2598,  1.4705])\n",
      "Epoch : 417  Loss : 9.47  Grad : tensor([-0.2593,  1.4680])\n",
      "Epoch : 418  Loss : 9.45  Grad : tensor([-0.2589,  1.4655])\n",
      "Epoch : 419  Loss : 9.42  Grad : tensor([-0.2584,  1.4630])\n",
      "Epoch : 420  Loss : 9.40  Grad : tensor([-0.2580,  1.4605])\n",
      "Epoch : 421  Loss : 9.38  Grad : tensor([-0.2576,  1.4581])\n",
      "Epoch : 422  Loss : 9.36  Grad : tensor([-0.2571,  1.4556])\n",
      "Epoch : 423  Loss : 9.34  Grad : tensor([-0.2567,  1.4531])\n",
      "Epoch : 424  Loss : 9.31  Grad : tensor([-0.2563,  1.4506])\n",
      "Epoch : 425  Loss : 9.29  Grad : tensor([-0.2558,  1.4482])\n",
      "Epoch : 426  Loss : 9.27  Grad : tensor([-0.2554,  1.4457])\n",
      "Epoch : 427  Loss : 9.25  Grad : tensor([-0.2550,  1.4433])\n",
      "Epoch : 428  Loss : 9.23  Grad : tensor([-0.2545,  1.4408])\n",
      "Epoch : 429  Loss : 9.21  Grad : tensor([-0.2541,  1.4384])\n",
      "Epoch : 430  Loss : 9.19  Grad : tensor([-0.2537,  1.4359])\n",
      "Epoch : 431  Loss : 9.16  Grad : tensor([-0.2532,  1.4335])\n",
      "Epoch : 432  Loss : 9.14  Grad : tensor([-0.2528,  1.4310])\n",
      "Epoch : 433  Loss : 9.12  Grad : tensor([-0.2524,  1.4286])\n",
      "Epoch : 434  Loss : 9.10  Grad : tensor([-0.2519,  1.4262])\n",
      "Epoch : 435  Loss : 9.08  Grad : tensor([-0.2515,  1.4238])\n",
      "Epoch : 436  Loss : 9.06  Grad : tensor([-0.2511,  1.4213])\n",
      "Epoch : 437  Loss : 9.04  Grad : tensor([-0.2507,  1.4189])\n",
      "Epoch : 438  Loss : 9.02  Grad : tensor([-0.2502,  1.4165])\n",
      "Epoch : 439  Loss : 9.00  Grad : tensor([-0.2498,  1.4141])\n",
      "Epoch : 440  Loss : 8.98  Grad : tensor([-0.2494,  1.4117])\n",
      "Epoch : 441  Loss : 8.96  Grad : tensor([-0.2489,  1.4093])\n",
      "Epoch : 442  Loss : 8.94  Grad : tensor([-0.2485,  1.4069])\n",
      "Epoch : 443  Loss : 8.92  Grad : tensor([-0.2481,  1.4045])\n",
      "Epoch : 444  Loss : 8.89  Grad : tensor([-0.2477,  1.4021])\n",
      "Epoch : 445  Loss : 8.87  Grad : tensor([-0.2473,  1.3998])\n",
      "Epoch : 446  Loss : 8.85  Grad : tensor([-0.2468,  1.3974])\n",
      "Epoch : 447  Loss : 8.83  Grad : tensor([-0.2464,  1.3950])\n",
      "Epoch : 448  Loss : 8.81  Grad : tensor([-0.2460,  1.3926])\n",
      "Epoch : 449  Loss : 8.79  Grad : tensor([-0.2456,  1.3903])\n",
      "Epoch : 450  Loss : 8.77  Grad : tensor([-0.2452,  1.3879])\n",
      "Epoch : 451  Loss : 8.75  Grad : tensor([-0.2448,  1.3856])\n",
      "Epoch : 452  Loss : 8.73  Grad : tensor([-0.2443,  1.3832])\n",
      "Epoch : 453  Loss : 8.71  Grad : tensor([-0.2439,  1.3808])\n",
      "Epoch : 454  Loss : 8.70  Grad : tensor([-0.2435,  1.3785])\n",
      "Epoch : 455  Loss : 8.68  Grad : tensor([-0.2431,  1.3762])\n",
      "Epoch : 456  Loss : 8.66  Grad : tensor([-0.2427,  1.3738])\n",
      "Epoch : 457  Loss : 8.64  Grad : tensor([-0.2423,  1.3715])\n",
      "Epoch : 458  Loss : 8.62  Grad : tensor([-0.2419,  1.3692])\n",
      "Epoch : 459  Loss : 8.60  Grad : tensor([-0.2414,  1.3668])\n",
      "Epoch : 460  Loss : 8.58  Grad : tensor([-0.2410,  1.3645])\n",
      "Epoch : 461  Loss : 8.56  Grad : tensor([-0.2406,  1.3622])\n",
      "Epoch : 462  Loss : 8.54  Grad : tensor([-0.2402,  1.3599])\n",
      "Epoch : 463  Loss : 8.52  Grad : tensor([-0.2398,  1.3576])\n",
      "Epoch : 464  Loss : 8.50  Grad : tensor([-0.2394,  1.3553])\n",
      "Epoch : 465  Loss : 8.48  Grad : tensor([-0.2390,  1.3530])\n",
      "Epoch : 466  Loss : 8.46  Grad : tensor([-0.2386,  1.3507])\n",
      "Epoch : 467  Loss : 8.45  Grad : tensor([-0.2382,  1.3484])\n",
      "Epoch : 468  Loss : 8.43  Grad : tensor([-0.2378,  1.3461])\n",
      "Epoch : 469  Loss : 8.41  Grad : tensor([-0.2374,  1.3438])\n",
      "Epoch : 470  Loss : 8.39  Grad : tensor([-0.2370,  1.3415])\n",
      "Epoch : 471  Loss : 8.37  Grad : tensor([-0.2366,  1.3392])\n",
      "Epoch : 472  Loss : 8.35  Grad : tensor([-0.2362,  1.3370])\n",
      "Epoch : 473  Loss : 8.33  Grad : tensor([-0.2358,  1.3347])\n",
      "Epoch : 474  Loss : 8.32  Grad : tensor([-0.2354,  1.3324])\n",
      "Epoch : 475  Loss : 8.30  Grad : tensor([-0.2350,  1.3301])\n",
      "Epoch : 476  Loss : 8.28  Grad : tensor([-0.2346,  1.3279])\n",
      "Epoch : 477  Loss : 8.26  Grad : tensor([-0.2342,  1.3256])\n",
      "Epoch : 478  Loss : 8.24  Grad : tensor([-0.2338,  1.3234])\n",
      "Epoch : 479  Loss : 8.23  Grad : tensor([-0.2334,  1.3211])\n",
      "Epoch : 480  Loss : 8.21  Grad : tensor([-0.2330,  1.3189])\n",
      "Epoch : 481  Loss : 8.19  Grad : tensor([-0.2326,  1.3166])\n",
      "Epoch : 482  Loss : 8.17  Grad : tensor([-0.2322,  1.3144])\n",
      "Epoch : 483  Loss : 8.15  Grad : tensor([-0.2318,  1.3122])\n",
      "Epoch : 484  Loss : 8.14  Grad : tensor([-0.2314,  1.3100])\n",
      "Epoch : 485  Loss : 8.12  Grad : tensor([-0.2310,  1.3077])\n",
      "Epoch : 486  Loss : 8.10  Grad : tensor([-0.2306,  1.3055])\n",
      "Epoch : 487  Loss : 8.08  Grad : tensor([-0.2302,  1.3033])\n",
      "Epoch : 488  Loss : 8.07  Grad : tensor([-0.2298,  1.3011])\n",
      "Epoch : 489  Loss : 8.05  Grad : tensor([-0.2295,  1.2989])\n",
      "Epoch : 490  Loss : 8.03  Grad : tensor([-0.2291,  1.2967])\n",
      "Epoch : 491  Loss : 8.01  Grad : tensor([-0.2287,  1.2945])\n",
      "Epoch : 492  Loss : 8.00  Grad : tensor([-0.2283,  1.2923])\n",
      "Epoch : 493  Loss : 7.98  Grad : tensor([-0.2279,  1.2901])\n",
      "Epoch : 494  Loss : 7.96  Grad : tensor([-0.2275,  1.2879])\n",
      "Epoch : 495  Loss : 7.94  Grad : tensor([-0.2271,  1.2857])\n",
      "Epoch : 496  Loss : 7.93  Grad : tensor([-0.2267,  1.2835])\n",
      "Epoch : 497  Loss : 7.91  Grad : tensor([-0.2263,  1.2813])\n",
      "Epoch : 498  Loss : 7.89  Grad : tensor([-0.2260,  1.2791])\n",
      "Epoch : 499  Loss : 7.88  Grad : tensor([-0.2256,  1.2770])\n",
      "Epoch : 500  Loss : 7.86  Grad : tensor([-0.2252,  1.2748])\n",
      "Epoch : 501  Loss : 7.84  Grad : tensor([-0.2248,  1.2726])\n",
      "Epoch : 502  Loss : 7.83  Grad : tensor([-0.2244,  1.2705])\n",
      "Epoch : 503  Loss : 7.81  Grad : tensor([-0.2241,  1.2683])\n",
      "Epoch : 504  Loss : 7.79  Grad : tensor([-0.2237,  1.2662])\n",
      "Epoch : 505  Loss : 7.78  Grad : tensor([-0.2233,  1.2640])\n",
      "Epoch : 506  Loss : 7.76  Grad : tensor([-0.2229,  1.2619])\n",
      "Epoch : 507  Loss : 7.74  Grad : tensor([-0.2225,  1.2597])\n",
      "Epoch : 508  Loss : 7.73  Grad : tensor([-0.2222,  1.2576])\n",
      "Epoch : 509  Loss : 7.71  Grad : tensor([-0.2218,  1.2554])\n",
      "Epoch : 510  Loss : 7.70  Grad : tensor([-0.2214,  1.2533])\n",
      "Epoch : 511  Loss : 7.68  Grad : tensor([-0.2210,  1.2512])\n",
      "Epoch : 512  Loss : 7.66  Grad : tensor([-0.2207,  1.2490])\n",
      "Epoch : 513  Loss : 7.65  Grad : tensor([-0.2203,  1.2469])\n",
      "Epoch : 514  Loss : 7.63  Grad : tensor([-0.2199,  1.2448])\n",
      "Epoch : 515  Loss : 7.61  Grad : tensor([-0.2195,  1.2427])\n",
      "Epoch : 516  Loss : 7.60  Grad : tensor([-0.2192,  1.2406])\n",
      "Epoch : 517  Loss : 7.58  Grad : tensor([-0.2188,  1.2385])\n",
      "Epoch : 518  Loss : 7.57  Grad : tensor([-0.2184,  1.2364])\n",
      "Epoch : 519  Loss : 7.55  Grad : tensor([-0.2180,  1.2343])\n",
      "Epoch : 520  Loss : 7.54  Grad : tensor([-0.2177,  1.2322])\n",
      "Epoch : 521  Loss : 7.52  Grad : tensor([-0.2173,  1.2301])\n",
      "Epoch : 522  Loss : 7.50  Grad : tensor([-0.2169,  1.2280])\n",
      "Epoch : 523  Loss : 7.49  Grad : tensor([-0.2165,  1.2259])\n",
      "Epoch : 524  Loss : 7.47  Grad : tensor([-0.2162,  1.2238])\n",
      "Epoch : 525  Loss : 7.46  Grad : tensor([-0.2158,  1.2217])\n",
      "Epoch : 526  Loss : 7.44  Grad : tensor([-0.2155,  1.2197])\n",
      "Epoch : 527  Loss : 7.43  Grad : tensor([-0.2151,  1.2176])\n",
      "Epoch : 528  Loss : 7.41  Grad : tensor([-0.2147,  1.2155])\n",
      "Epoch : 529  Loss : 7.40  Grad : tensor([-0.2144,  1.2135])\n",
      "Epoch : 530  Loss : 7.38  Grad : tensor([-0.2140,  1.2114])\n",
      "Epoch : 531  Loss : 7.37  Grad : tensor([-0.2136,  1.2093])\n",
      "Epoch : 532  Loss : 7.35  Grad : tensor([-0.2133,  1.2073])\n",
      "Epoch : 533  Loss : 7.34  Grad : tensor([-0.2129,  1.2052])\n",
      "Epoch : 534  Loss : 7.32  Grad : tensor([-0.2125,  1.2032])\n",
      "Epoch : 535  Loss : 7.31  Grad : tensor([-0.2122,  1.2012])\n",
      "Epoch : 536  Loss : 7.29  Grad : tensor([-0.2118,  1.1991])\n",
      "Epoch : 537  Loss : 7.28  Grad : tensor([-0.2115,  1.1971])\n",
      "Epoch : 538  Loss : 7.26  Grad : tensor([-0.2111,  1.1950])\n",
      "Epoch : 539  Loss : 7.25  Grad : tensor([-0.2108,  1.1930])\n",
      "Epoch : 540  Loss : 7.23  Grad : tensor([-0.2104,  1.1910])\n",
      "Epoch : 541  Loss : 7.22  Grad : tensor([-0.2100,  1.1890])\n",
      "Epoch : 542  Loss : 7.20  Grad : tensor([-0.2097,  1.1869])\n",
      "Epoch : 543  Loss : 7.19  Grad : tensor([-0.2093,  1.1849])\n",
      "Epoch : 544  Loss : 7.17  Grad : tensor([-0.2090,  1.1829])\n",
      "Epoch : 545  Loss : 7.16  Grad : tensor([-0.2086,  1.1809])\n",
      "Epoch : 546  Loss : 7.15  Grad : tensor([-0.2083,  1.1789])\n",
      "Epoch : 547  Loss : 7.13  Grad : tensor([-0.2079,  1.1769])\n",
      "Epoch : 548  Loss : 7.12  Grad : tensor([-0.2075,  1.1749])\n",
      "Epoch : 549  Loss : 7.10  Grad : tensor([-0.2072,  1.1729])\n",
      "Epoch : 550  Loss : 7.09  Grad : tensor([-0.2068,  1.1709])\n",
      "Epoch : 551  Loss : 7.07  Grad : tensor([-0.2065,  1.1689])\n",
      "Epoch : 552  Loss : 7.06  Grad : tensor([-0.2062,  1.1669])\n",
      "Epoch : 553  Loss : 7.05  Grad : tensor([-0.2058,  1.1649])\n",
      "Epoch : 554  Loss : 7.03  Grad : tensor([-0.2054,  1.1630])\n",
      "Epoch : 555  Loss : 7.02  Grad : tensor([-0.2051,  1.1610])\n",
      "Epoch : 556  Loss : 7.00  Grad : tensor([-0.2047,  1.1590])\n",
      "Epoch : 557  Loss : 6.99  Grad : tensor([-0.2044,  1.1571])\n",
      "Epoch : 558  Loss : 6.98  Grad : tensor([-0.2041,  1.1551])\n",
      "Epoch : 559  Loss : 6.96  Grad : tensor([-0.2037,  1.1531])\n",
      "Epoch : 560  Loss : 6.95  Grad : tensor([-0.2034,  1.1512])\n",
      "Epoch : 561  Loss : 6.94  Grad : tensor([-0.2030,  1.1492])\n",
      "Epoch : 562  Loss : 6.92  Grad : tensor([-0.2027,  1.1473])\n",
      "Epoch : 563  Loss : 6.91  Grad : tensor([-0.2023,  1.1453])\n",
      "Epoch : 564  Loss : 6.90  Grad : tensor([-0.2020,  1.1434])\n",
      "Epoch : 565  Loss : 6.88  Grad : tensor([-0.2016,  1.1414])\n",
      "Epoch : 566  Loss : 6.87  Grad : tensor([-0.2013,  1.1395])\n",
      "Epoch : 567  Loss : 6.86  Grad : tensor([-0.2010,  1.1375])\n",
      "Epoch : 568  Loss : 6.84  Grad : tensor([-0.2006,  1.1356])\n",
      "Epoch : 569  Loss : 6.83  Grad : tensor([-0.2003,  1.1337])\n",
      "Epoch : 570  Loss : 6.82  Grad : tensor([-0.1999,  1.1318])\n",
      "Epoch : 571  Loss : 6.80  Grad : tensor([-0.1996,  1.1298])\n",
      "Epoch : 572  Loss : 6.79  Grad : tensor([-0.1993,  1.1279])\n",
      "Epoch : 573  Loss : 6.78  Grad : tensor([-0.1989,  1.1260])\n",
      "Epoch : 574  Loss : 6.76  Grad : tensor([-0.1986,  1.1241])\n",
      "Epoch : 575  Loss : 6.75  Grad : tensor([-0.1982,  1.1222])\n",
      "Epoch : 576  Loss : 6.74  Grad : tensor([-0.1979,  1.1203])\n",
      "Epoch : 577  Loss : 6.72  Grad : tensor([-0.1976,  1.1184])\n",
      "Epoch : 578  Loss : 6.71  Grad : tensor([-0.1972,  1.1165])\n",
      "Epoch : 579  Loss : 6.70  Grad : tensor([-0.1969,  1.1146])\n",
      "Epoch : 580  Loss : 6.69  Grad : tensor([-0.1966,  1.1127])\n",
      "Epoch : 581  Loss : 6.67  Grad : tensor([-0.1962,  1.1108])\n",
      "Epoch : 582  Loss : 6.66  Grad : tensor([-0.1959,  1.1089])\n",
      "Epoch : 583  Loss : 6.65  Grad : tensor([-0.1956,  1.1070])\n",
      "Epoch : 584  Loss : 6.63  Grad : tensor([-0.1952,  1.1051])\n",
      "Epoch : 585  Loss : 6.62  Grad : tensor([-0.1949,  1.1033])\n",
      "Epoch : 586  Loss : 6.61  Grad : tensor([-0.1946,  1.1014])\n",
      "Epoch : 587  Loss : 6.60  Grad : tensor([-0.1942,  1.0995])\n",
      "Epoch : 588  Loss : 6.58  Grad : tensor([-0.1939,  1.0976])\n",
      "Epoch : 589  Loss : 6.57  Grad : tensor([-0.1936,  1.0958])\n",
      "Epoch : 590  Loss : 6.56  Grad : tensor([-0.1932,  1.0939])\n",
      "Epoch : 591  Loss : 6.55  Grad : tensor([-0.1929,  1.0921])\n",
      "Epoch : 592  Loss : 6.54  Grad : tensor([-0.1926,  1.0902])\n",
      "Epoch : 593  Loss : 6.52  Grad : tensor([-0.1923,  1.0884])\n",
      "Epoch : 594  Loss : 6.51  Grad : tensor([-0.1919,  1.0865])\n",
      "Epoch : 595  Loss : 6.50  Grad : tensor([-0.1916,  1.0847])\n",
      "Epoch : 596  Loss : 6.49  Grad : tensor([-0.1913,  1.0828])\n",
      "Epoch : 597  Loss : 6.47  Grad : tensor([-0.1910,  1.0810])\n",
      "Epoch : 598  Loss : 6.46  Grad : tensor([-0.1906,  1.0791])\n",
      "Epoch : 599  Loss : 6.45  Grad : tensor([-0.1903,  1.0773])\n",
      "Epoch : 600  Loss : 6.44  Grad : tensor([-0.1900,  1.0755])\n",
      "Epoch : 601  Loss : 6.43  Grad : tensor([-0.1897,  1.0737])\n",
      "Epoch : 602  Loss : 6.41  Grad : tensor([-0.1893,  1.0718])\n",
      "Epoch : 603  Loss : 6.40  Grad : tensor([-0.1890,  1.0700])\n",
      "Epoch : 604  Loss : 6.39  Grad : tensor([-0.1887,  1.0682])\n",
      "Epoch : 605  Loss : 6.38  Grad : tensor([-0.1884,  1.0664])\n",
      "Epoch : 606  Loss : 6.37  Grad : tensor([-0.1880,  1.0646])\n",
      "Epoch : 607  Loss : 6.36  Grad : tensor([-0.1877,  1.0628])\n",
      "Epoch : 608  Loss : 6.34  Grad : tensor([-0.1874,  1.0609])\n",
      "Epoch : 609  Loss : 6.33  Grad : tensor([-0.1871,  1.0591])\n",
      "Epoch : 610  Loss : 6.32  Grad : tensor([-0.1868,  1.0573])\n",
      "Epoch : 611  Loss : 6.31  Grad : tensor([-0.1865,  1.0555])\n",
      "Epoch : 612  Loss : 6.30  Grad : tensor([-0.1861,  1.0538])\n",
      "Epoch : 613  Loss : 6.29  Grad : tensor([-0.1858,  1.0520])\n",
      "Epoch : 614  Loss : 6.28  Grad : tensor([-0.1855,  1.0502])\n",
      "Epoch : 615  Loss : 6.26  Grad : tensor([-0.1852,  1.0484])\n",
      "Epoch : 616  Loss : 6.25  Grad : tensor([-0.1849,  1.0466])\n",
      "Epoch : 617  Loss : 6.24  Grad : tensor([-0.1846,  1.0448])\n",
      "Epoch : 618  Loss : 6.23  Grad : tensor([-0.1843,  1.0431])\n",
      "Epoch : 619  Loss : 6.22  Grad : tensor([-0.1840,  1.0413])\n",
      "Epoch : 620  Loss : 6.21  Grad : tensor([-0.1836,  1.0395])\n",
      "Epoch : 621  Loss : 6.20  Grad : tensor([-0.1833,  1.0378])\n",
      "Epoch : 622  Loss : 6.19  Grad : tensor([-0.1830,  1.0360])\n",
      "Epoch : 623  Loss : 6.17  Grad : tensor([-0.1827,  1.0342])\n",
      "Epoch : 624  Loss : 6.16  Grad : tensor([-0.1824,  1.0325])\n",
      "Epoch : 625  Loss : 6.15  Grad : tensor([-0.1821,  1.0307])\n",
      "Epoch : 626  Loss : 6.14  Grad : tensor([-0.1818,  1.0290])\n",
      "Epoch : 627  Loss : 6.13  Grad : tensor([-0.1815,  1.0272])\n",
      "Epoch : 628  Loss : 6.12  Grad : tensor([-0.1811,  1.0255])\n",
      "Epoch : 629  Loss : 6.11  Grad : tensor([-0.1808,  1.0237])\n",
      "Epoch : 630  Loss : 6.10  Grad : tensor([-0.1805,  1.0220])\n",
      "Epoch : 631  Loss : 6.09  Grad : tensor([-0.1802,  1.0203])\n",
      "Epoch : 632  Loss : 6.08  Grad : tensor([-0.1799,  1.0185])\n",
      "Epoch : 633  Loss : 6.07  Grad : tensor([-0.1796,  1.0168])\n",
      "Epoch : 634  Loss : 6.05  Grad : tensor([-0.1793,  1.0151])\n",
      "Epoch : 635  Loss : 6.04  Grad : tensor([-0.1790,  1.0133])\n",
      "Epoch : 636  Loss : 6.03  Grad : tensor([-0.1787,  1.0116])\n",
      "Epoch : 637  Loss : 6.02  Grad : tensor([-0.1784,  1.0099])\n",
      "Epoch : 638  Loss : 6.01  Grad : tensor([-0.1781,  1.0082])\n",
      "Epoch : 639  Loss : 6.00  Grad : tensor([-0.1778,  1.0065])\n",
      "Epoch : 640  Loss : 5.99  Grad : tensor([-0.1775,  1.0048])\n",
      "Epoch : 641  Loss : 5.98  Grad : tensor([-0.1772,  1.0031])\n",
      "Epoch : 642  Loss : 5.97  Grad : tensor([-0.1769,  1.0014])\n",
      "Epoch : 643  Loss : 5.96  Grad : tensor([-0.1766,  0.9997])\n",
      "Epoch : 644  Loss : 5.95  Grad : tensor([-0.1763,  0.9980])\n",
      "Epoch : 645  Loss : 5.94  Grad : tensor([-0.1760,  0.9963])\n",
      "Epoch : 646  Loss : 5.93  Grad : tensor([-0.1757,  0.9946])\n",
      "Epoch : 647  Loss : 5.92  Grad : tensor([-0.1754,  0.9929])\n",
      "Epoch : 648  Loss : 5.91  Grad : tensor([-0.1751,  0.9912])\n",
      "Epoch : 649  Loss : 5.90  Grad : tensor([-0.1748,  0.9895])\n",
      "Epoch : 650  Loss : 5.89  Grad : tensor([-0.1745,  0.9878])\n",
      "Epoch : 651  Loss : 5.88  Grad : tensor([-0.1742,  0.9862])\n",
      "Epoch : 652  Loss : 5.87  Grad : tensor([-0.1739,  0.9845])\n",
      "Epoch : 653  Loss : 5.86  Grad : tensor([-0.1736,  0.9828])\n",
      "Epoch : 654  Loss : 5.85  Grad : tensor([-0.1733,  0.9811])\n",
      "Epoch : 655  Loss : 5.84  Grad : tensor([-0.1730,  0.9795])\n",
      "Epoch : 656  Loss : 5.83  Grad : tensor([-0.1727,  0.9778])\n",
      "Epoch : 657  Loss : 5.82  Grad : tensor([-0.1724,  0.9761])\n",
      "Epoch : 658  Loss : 5.81  Grad : tensor([-0.1722,  0.9745])\n",
      "Epoch : 659  Loss : 5.80  Grad : tensor([-0.1719,  0.9728])\n",
      "Epoch : 660  Loss : 5.79  Grad : tensor([-0.1716,  0.9712])\n",
      "Epoch : 661  Loss : 5.78  Grad : tensor([-0.1713,  0.9695])\n",
      "Epoch : 662  Loss : 5.77  Grad : tensor([-0.1710,  0.9679])\n",
      "Epoch : 663  Loss : 5.76  Grad : tensor([-0.1707,  0.9662])\n",
      "Epoch : 664  Loss : 5.75  Grad : tensor([-0.1704,  0.9646])\n",
      "Epoch : 665  Loss : 5.74  Grad : tensor([-0.1701,  0.9630])\n",
      "Epoch : 666  Loss : 5.73  Grad : tensor([-0.1698,  0.9613])\n",
      "Epoch : 667  Loss : 5.72  Grad : tensor([-0.1695,  0.9597])\n",
      "Epoch : 668  Loss : 5.71  Grad : tensor([-0.1692,  0.9581])\n",
      "Epoch : 669  Loss : 5.70  Grad : tensor([-0.1690,  0.9564])\n",
      "Epoch : 670  Loss : 5.69  Grad : tensor([-0.1687,  0.9548])\n",
      "Epoch : 671  Loss : 5.69  Grad : tensor([-0.1684,  0.9532])\n",
      "Epoch : 672  Loss : 5.68  Grad : tensor([-0.1681,  0.9516])\n",
      "Epoch : 673  Loss : 5.67  Grad : tensor([-0.1678,  0.9499])\n",
      "Epoch : 674  Loss : 5.66  Grad : tensor([-0.1675,  0.9483])\n",
      "Epoch : 675  Loss : 5.65  Grad : tensor([-0.1673,  0.9467])\n",
      "Epoch : 676  Loss : 5.64  Grad : tensor([-0.1670,  0.9451])\n",
      "Epoch : 677  Loss : 5.63  Grad : tensor([-0.1667,  0.9435])\n",
      "Epoch : 678  Loss : 5.62  Grad : tensor([-0.1664,  0.9419])\n",
      "Epoch : 679  Loss : 5.61  Grad : tensor([-0.1661,  0.9403])\n",
      "Epoch : 680  Loss : 5.60  Grad : tensor([-0.1658,  0.9387])\n",
      "Epoch : 681  Loss : 5.59  Grad : tensor([-0.1656,  0.9371])\n",
      "Epoch : 682  Loss : 5.58  Grad : tensor([-0.1653,  0.9355])\n",
      "Epoch : 683  Loss : 5.58  Grad : tensor([-0.1650,  0.9339])\n",
      "Epoch : 684  Loss : 5.57  Grad : tensor([-0.1647,  0.9323])\n",
      "Epoch : 685  Loss : 5.56  Grad : tensor([-0.1644,  0.9308])\n",
      "Epoch : 686  Loss : 5.55  Grad : tensor([-0.1641,  0.9292])\n",
      "Epoch : 687  Loss : 5.54  Grad : tensor([-0.1639,  0.9276])\n",
      "Epoch : 688  Loss : 5.53  Grad : tensor([-0.1636,  0.9260])\n",
      "Epoch : 689  Loss : 5.52  Grad : tensor([-0.1633,  0.9245])\n",
      "Epoch : 690  Loss : 5.51  Grad : tensor([-0.1630,  0.9229])\n",
      "Epoch : 691  Loss : 5.50  Grad : tensor([-0.1628,  0.9213])\n",
      "Epoch : 692  Loss : 5.50  Grad : tensor([-0.1625,  0.9197])\n",
      "Epoch : 693  Loss : 5.49  Grad : tensor([-0.1622,  0.9182])\n",
      "Epoch : 694  Loss : 5.48  Grad : tensor([-0.1619,  0.9166])\n",
      "Epoch : 695  Loss : 5.47  Grad : tensor([-0.1617,  0.9151])\n",
      "Epoch : 696  Loss : 5.46  Grad : tensor([-0.1614,  0.9135])\n",
      "Epoch : 697  Loss : 5.45  Grad : tensor([-0.1611,  0.9120])\n",
      "Epoch : 698  Loss : 5.44  Grad : tensor([-0.1608,  0.9104])\n",
      "Epoch : 699  Loss : 5.43  Grad : tensor([-0.1605,  0.9089])\n",
      "Epoch : 700  Loss : 5.43  Grad : tensor([-0.1603,  0.9073])\n",
      "Epoch : 701  Loss : 5.42  Grad : tensor([-0.1600,  0.9058])\n",
      "Epoch : 702  Loss : 5.41  Grad : tensor([-0.1597,  0.9042])\n",
      "Epoch : 703  Loss : 5.40  Grad : tensor([-0.1595,  0.9027])\n",
      "Epoch : 704  Loss : 5.39  Grad : tensor([-0.1592,  0.9012])\n",
      "Epoch : 705  Loss : 5.38  Grad : tensor([-0.1589,  0.8996])\n",
      "Epoch : 706  Loss : 5.38  Grad : tensor([-0.1586,  0.8981])\n",
      "Epoch : 707  Loss : 5.37  Grad : tensor([-0.1584,  0.8966])\n",
      "Epoch : 708  Loss : 5.36  Grad : tensor([-0.1581,  0.8951])\n",
      "Epoch : 709  Loss : 5.35  Grad : tensor([-0.1578,  0.8935])\n",
      "Epoch : 710  Loss : 5.34  Grad : tensor([-0.1576,  0.8920])\n",
      "Epoch : 711  Loss : 5.33  Grad : tensor([-0.1573,  0.8905])\n",
      "Epoch : 712  Loss : 5.33  Grad : tensor([-0.1570,  0.8890])\n",
      "Epoch : 713  Loss : 5.32  Grad : tensor([-0.1568,  0.8875])\n",
      "Epoch : 714  Loss : 5.31  Grad : tensor([-0.1565,  0.8860])\n",
      "Epoch : 715  Loss : 5.30  Grad : tensor([-0.1562,  0.8845])\n",
      "Epoch : 716  Loss : 5.29  Grad : tensor([-0.1560,  0.8830])\n",
      "Epoch : 717  Loss : 5.29  Grad : tensor([-0.1557,  0.8815])\n",
      "Epoch : 718  Loss : 5.28  Grad : tensor([-0.1555,  0.8800])\n",
      "Epoch : 719  Loss : 5.27  Grad : tensor([-0.1552,  0.8785])\n",
      "Epoch : 720  Loss : 5.26  Grad : tensor([-0.1549,  0.8770])\n",
      "Epoch : 721  Loss : 5.25  Grad : tensor([-0.1547,  0.8755])\n",
      "Epoch : 722  Loss : 5.25  Grad : tensor([-0.1544,  0.8740])\n",
      "Epoch : 723  Loss : 5.24  Grad : tensor([-0.1541,  0.8725])\n",
      "Epoch : 724  Loss : 5.23  Grad : tensor([-0.1539,  0.8710])\n",
      "Epoch : 725  Loss : 5.22  Grad : tensor([-0.1536,  0.8696])\n",
      "Epoch : 726  Loss : 5.21  Grad : tensor([-0.1533,  0.8681])\n",
      "Epoch : 727  Loss : 5.21  Grad : tensor([-0.1531,  0.8666])\n",
      "Epoch : 728  Loss : 5.20  Grad : tensor([-0.1528,  0.8651])\n",
      "Epoch : 729  Loss : 5.19  Grad : tensor([-0.1526,  0.8637])\n",
      "Epoch : 730  Loss : 5.18  Grad : tensor([-0.1523,  0.8622])\n",
      "Epoch : 731  Loss : 5.18  Grad : tensor([-0.1520,  0.8607])\n",
      "Epoch : 732  Loss : 5.17  Grad : tensor([-0.1518,  0.8593])\n",
      "Epoch : 733  Loss : 5.16  Grad : tensor([-0.1515,  0.8578])\n",
      "Epoch : 734  Loss : 5.15  Grad : tensor([-0.1513,  0.8564])\n",
      "Epoch : 735  Loss : 5.15  Grad : tensor([-0.1510,  0.8549])\n",
      "Epoch : 736  Loss : 5.14  Grad : tensor([-0.1508,  0.8535])\n",
      "Epoch : 737  Loss : 5.13  Grad : tensor([-0.1505,  0.8520])\n",
      "Epoch : 738  Loss : 5.12  Grad : tensor([-0.1502,  0.8506])\n",
      "Epoch : 739  Loss : 5.12  Grad : tensor([-0.1500,  0.8491])\n",
      "Epoch : 740  Loss : 5.11  Grad : tensor([-0.1497,  0.8477])\n",
      "Epoch : 741  Loss : 5.10  Grad : tensor([-0.1495,  0.8462])\n",
      "Epoch : 742  Loss : 5.09  Grad : tensor([-0.1492,  0.8448])\n",
      "Epoch : 743  Loss : 5.09  Grad : tensor([-0.1490,  0.8434])\n",
      "Epoch : 744  Loss : 5.08  Grad : tensor([-0.1487,  0.8419])\n",
      "Epoch : 745  Loss : 5.07  Grad : tensor([-0.1485,  0.8405])\n",
      "Epoch : 746  Loss : 5.06  Grad : tensor([-0.1482,  0.8391])\n",
      "Epoch : 747  Loss : 5.06  Grad : tensor([-0.1480,  0.8376])\n",
      "Epoch : 748  Loss : 5.05  Grad : tensor([-0.1477,  0.8362])\n",
      "Epoch : 749  Loss : 5.04  Grad : tensor([-0.1475,  0.8348])\n",
      "Epoch : 750  Loss : 5.04  Grad : tensor([-0.1472,  0.8334])\n",
      "Epoch : 751  Loss : 5.03  Grad : tensor([-0.1470,  0.8320])\n",
      "Epoch : 752  Loss : 5.02  Grad : tensor([-0.1467,  0.8305])\n",
      "Epoch : 753  Loss : 5.01  Grad : tensor([-0.1465,  0.8291])\n",
      "Epoch : 754  Loss : 5.01  Grad : tensor([-0.1462,  0.8277])\n",
      "Epoch : 755  Loss : 5.00  Grad : tensor([-0.1460,  0.8263])\n",
      "Epoch : 756  Loss : 4.99  Grad : tensor([-0.1457,  0.8249])\n",
      "Epoch : 757  Loss : 4.99  Grad : tensor([-0.1455,  0.8235])\n",
      "Epoch : 758  Loss : 4.98  Grad : tensor([-0.1452,  0.8221])\n",
      "Epoch : 759  Loss : 4.97  Grad : tensor([-0.1450,  0.8207])\n",
      "Epoch : 760  Loss : 4.97  Grad : tensor([-0.1447,  0.8193])\n",
      "Epoch : 761  Loss : 4.96  Grad : tensor([-0.1445,  0.8179])\n",
      "Epoch : 762  Loss : 4.95  Grad : tensor([-0.1443,  0.8165])\n",
      "Epoch : 763  Loss : 4.94  Grad : tensor([-0.1440,  0.8152])\n",
      "Epoch : 764  Loss : 4.94  Grad : tensor([-0.1438,  0.8138])\n",
      "Epoch : 765  Loss : 4.93  Grad : tensor([-0.1435,  0.8124])\n",
      "Epoch : 766  Loss : 4.92  Grad : tensor([-0.1433,  0.8110])\n",
      "Epoch : 767  Loss : 4.92  Grad : tensor([-0.1430,  0.8096])\n",
      "Epoch : 768  Loss : 4.91  Grad : tensor([-0.1428,  0.8083])\n",
      "Epoch : 769  Loss : 4.90  Grad : tensor([-0.1426,  0.8069])\n",
      "Epoch : 770  Loss : 4.90  Grad : tensor([-0.1423,  0.8055])\n",
      "Epoch : 771  Loss : 4.89  Grad : tensor([-0.1420,  0.8042])\n",
      "Epoch : 772  Loss : 4.88  Grad : tensor([-0.1418,  0.8028])\n",
      "Epoch : 773  Loss : 4.88  Grad : tensor([-0.1416,  0.8014])\n",
      "Epoch : 774  Loss : 4.87  Grad : tensor([-0.1413,  0.8001])\n",
      "Epoch : 775  Loss : 4.86  Grad : tensor([-0.1411,  0.7987])\n",
      "Epoch : 776  Loss : 4.86  Grad : tensor([-0.1408,  0.7973])\n",
      "Epoch : 777  Loss : 4.85  Grad : tensor([-0.1406,  0.7960])\n",
      "Epoch : 778  Loss : 4.84  Grad : tensor([-0.1404,  0.7946])\n",
      "Epoch : 779  Loss : 4.84  Grad : tensor([-0.1401,  0.7933])\n",
      "Epoch : 780  Loss : 4.83  Grad : tensor([-0.1399,  0.7919])\n",
      "Epoch : 781  Loss : 4.82  Grad : tensor([-0.1397,  0.7906])\n",
      "Epoch : 782  Loss : 4.82  Grad : tensor([-0.1394,  0.7893])\n",
      "Epoch : 783  Loss : 4.81  Grad : tensor([-0.1392,  0.7879])\n",
      "Epoch : 784  Loss : 4.81  Grad : tensor([-0.1389,  0.7866])\n",
      "Epoch : 785  Loss : 4.80  Grad : tensor([-0.1387,  0.7852])\n",
      "Epoch : 786  Loss : 4.79  Grad : tensor([-0.1385,  0.7839])\n",
      "Epoch : 787  Loss : 4.79  Grad : tensor([-0.1383,  0.7826])\n",
      "Epoch : 788  Loss : 4.78  Grad : tensor([-0.1380,  0.7812])\n",
      "Epoch : 789  Loss : 4.77  Grad : tensor([-0.1378,  0.7799])\n",
      "Epoch : 790  Loss : 4.77  Grad : tensor([-0.1375,  0.7786])\n",
      "Epoch : 791  Loss : 4.76  Grad : tensor([-0.1373,  0.7773])\n",
      "Epoch : 792  Loss : 4.76  Grad : tensor([-0.1371,  0.7759])\n",
      "Epoch : 793  Loss : 4.75  Grad : tensor([-0.1368,  0.7746])\n",
      "Epoch : 794  Loss : 4.74  Grad : tensor([-0.1366,  0.7733])\n",
      "Epoch : 795  Loss : 4.74  Grad : tensor([-0.1364,  0.7720])\n",
      "Epoch : 796  Loss : 4.73  Grad : tensor([-0.1361,  0.7707])\n",
      "Epoch : 797  Loss : 4.72  Grad : tensor([-0.1359,  0.7694])\n",
      "Epoch : 798  Loss : 4.72  Grad : tensor([-0.1357,  0.7681])\n",
      "Epoch : 799  Loss : 4.71  Grad : tensor([-0.1354,  0.7668])\n",
      "Epoch : 800  Loss : 4.71  Grad : tensor([-0.1352,  0.7655])\n",
      "Epoch : 801  Loss : 4.70  Grad : tensor([-0.1350,  0.7642])\n",
      "Epoch : 802  Loss : 4.69  Grad : tensor([-0.1347,  0.7629])\n",
      "Epoch : 803  Loss : 4.69  Grad : tensor([-0.1345,  0.7616])\n",
      "Epoch : 804  Loss : 4.68  Grad : tensor([-0.1343,  0.7603])\n",
      "Epoch : 805  Loss : 4.68  Grad : tensor([-0.1341,  0.7590])\n",
      "Epoch : 806  Loss : 4.67  Grad : tensor([-0.1338,  0.7577])\n",
      "Epoch : 807  Loss : 4.66  Grad : tensor([-0.1336,  0.7564])\n",
      "Epoch : 808  Loss : 4.66  Grad : tensor([-0.1334,  0.7551])\n",
      "Epoch : 809  Loss : 4.65  Grad : tensor([-0.1332,  0.7538])\n",
      "Epoch : 810  Loss : 4.65  Grad : tensor([-0.1330,  0.7526])\n",
      "Epoch : 811  Loss : 4.64  Grad : tensor([-0.1327,  0.7513])\n",
      "Epoch : 812  Loss : 4.63  Grad : tensor([-0.1325,  0.7500])\n",
      "Epoch : 813  Loss : 4.63  Grad : tensor([-0.1323,  0.7487])\n",
      "Epoch : 814  Loss : 4.62  Grad : tensor([-0.1320,  0.7475])\n",
      "Epoch : 815  Loss : 4.62  Grad : tensor([-0.1318,  0.7462])\n",
      "Epoch : 816  Loss : 4.61  Grad : tensor([-0.1316,  0.7449])\n",
      "Epoch : 817  Loss : 4.61  Grad : tensor([-0.1314,  0.7437])\n",
      "Epoch : 818  Loss : 4.60  Grad : tensor([-0.1311,  0.7424])\n",
      "Epoch : 819  Loss : 4.59  Grad : tensor([-0.1309,  0.7411])\n",
      "Epoch : 820  Loss : 4.59  Grad : tensor([-0.1307,  0.7399])\n",
      "Epoch : 821  Loss : 4.58  Grad : tensor([-0.1305,  0.7386])\n",
      "Epoch : 822  Loss : 4.58  Grad : tensor([-0.1303,  0.7374])\n",
      "Epoch : 823  Loss : 4.57  Grad : tensor([-0.1300,  0.7361])\n",
      "Epoch : 824  Loss : 4.57  Grad : tensor([-0.1298,  0.7349])\n",
      "Epoch : 825  Loss : 4.56  Grad : tensor([-0.1296,  0.7336])\n",
      "Epoch : 826  Loss : 4.56  Grad : tensor([-0.1294,  0.7324])\n",
      "Epoch : 827  Loss : 4.55  Grad : tensor([-0.1292,  0.7311])\n",
      "Epoch : 828  Loss : 4.54  Grad : tensor([-0.1289,  0.7299])\n",
      "Epoch : 829  Loss : 4.54  Grad : tensor([-0.1287,  0.7286])\n",
      "Epoch : 830  Loss : 4.53  Grad : tensor([-0.1285,  0.7274])\n",
      "Epoch : 831  Loss : 4.53  Grad : tensor([-0.1283,  0.7262])\n",
      "Epoch : 832  Loss : 4.52  Grad : tensor([-0.1280,  0.7249])\n",
      "Epoch : 833  Loss : 4.52  Grad : tensor([-0.1278,  0.7237])\n",
      "Epoch : 834  Loss : 4.51  Grad : tensor([-0.1276,  0.7225])\n",
      "Epoch : 835  Loss : 4.51  Grad : tensor([-0.1274,  0.7212])\n",
      "Epoch : 836  Loss : 4.50  Grad : tensor([-0.1272,  0.7200])\n",
      "Epoch : 837  Loss : 4.50  Grad : tensor([-0.1270,  0.7188])\n",
      "Epoch : 838  Loss : 4.49  Grad : tensor([-0.1268,  0.7176])\n",
      "Epoch : 839  Loss : 4.49  Grad : tensor([-0.1266,  0.7163])\n",
      "Epoch : 840  Loss : 4.48  Grad : tensor([-0.1263,  0.7151])\n",
      "Epoch : 841  Loss : 4.47  Grad : tensor([-0.1261,  0.7139])\n",
      "Epoch : 842  Loss : 4.47  Grad : tensor([-0.1259,  0.7127])\n",
      "Epoch : 843  Loss : 4.46  Grad : tensor([-0.1257,  0.7115])\n",
      "Epoch : 844  Loss : 4.46  Grad : tensor([-0.1255,  0.7103])\n",
      "Epoch : 845  Loss : 4.45  Grad : tensor([-0.1253,  0.7091])\n",
      "Epoch : 846  Loss : 4.45  Grad : tensor([-0.1250,  0.7079])\n",
      "Epoch : 847  Loss : 4.44  Grad : tensor([-0.1249,  0.7067])\n",
      "Epoch : 848  Loss : 4.44  Grad : tensor([-0.1246,  0.7055])\n",
      "Epoch : 849  Loss : 4.43  Grad : tensor([-0.1244,  0.7043])\n",
      "Epoch : 850  Loss : 4.43  Grad : tensor([-0.1242,  0.7031])\n",
      "Epoch : 851  Loss : 4.42  Grad : tensor([-0.1240,  0.7019])\n",
      "Epoch : 852  Loss : 4.42  Grad : tensor([-0.1238,  0.7007])\n",
      "Epoch : 853  Loss : 4.41  Grad : tensor([-0.1236,  0.6995])\n",
      "Epoch : 854  Loss : 4.41  Grad : tensor([-0.1234,  0.6983])\n",
      "Epoch : 855  Loss : 4.40  Grad : tensor([-0.1232,  0.6971])\n",
      "Epoch : 856  Loss : 4.40  Grad : tensor([-0.1229,  0.6959])\n",
      "Epoch : 857  Loss : 4.39  Grad : tensor([-0.1227,  0.6948])\n",
      "Epoch : 858  Loss : 4.39  Grad : tensor([-0.1225,  0.6936])\n",
      "Epoch : 859  Loss : 4.38  Grad : tensor([-0.1223,  0.6924])\n",
      "Epoch : 860  Loss : 4.38  Grad : tensor([-0.1221,  0.6912])\n",
      "Epoch : 861  Loss : 4.37  Grad : tensor([-0.1219,  0.6901])\n",
      "Epoch : 862  Loss : 4.37  Grad : tensor([-0.1217,  0.6889])\n",
      "Epoch : 863  Loss : 4.36  Grad : tensor([-0.1215,  0.6877])\n",
      "Epoch : 864  Loss : 4.36  Grad : tensor([-0.1213,  0.6865])\n",
      "Epoch : 865  Loss : 4.35  Grad : tensor([-0.1211,  0.6854])\n",
      "Epoch : 866  Loss : 4.35  Grad : tensor([-0.1209,  0.6842])\n",
      "Epoch : 867  Loss : 4.34  Grad : tensor([-0.1207,  0.6830])\n",
      "Epoch : 868  Loss : 4.34  Grad : tensor([-0.1205,  0.6819])\n",
      "Epoch : 869  Loss : 4.33  Grad : tensor([-0.1203,  0.6807])\n",
      "Epoch : 870  Loss : 4.33  Grad : tensor([-0.1201,  0.6796])\n",
      "Epoch : 871  Loss : 4.32  Grad : tensor([-0.1198,  0.6784])\n",
      "Epoch : 872  Loss : 4.32  Grad : tensor([-0.1196,  0.6773])\n",
      "Epoch : 873  Loss : 4.32  Grad : tensor([-0.1195,  0.6761])\n",
      "Epoch : 874  Loss : 4.31  Grad : tensor([-0.1192,  0.6750])\n",
      "Epoch : 875  Loss : 4.31  Grad : tensor([-0.1190,  0.6738])\n",
      "Epoch : 876  Loss : 4.30  Grad : tensor([-0.1188,  0.6727])\n",
      "Epoch : 877  Loss : 4.30  Grad : tensor([-0.1186,  0.6715])\n",
      "Epoch : 878  Loss : 4.29  Grad : tensor([-0.1184,  0.6704])\n",
      "Epoch : 879  Loss : 4.29  Grad : tensor([-0.1182,  0.6693])\n",
      "Epoch : 880  Loss : 4.28  Grad : tensor([-0.1180,  0.6681])\n",
      "Epoch : 881  Loss : 4.28  Grad : tensor([-0.1178,  0.6670])\n",
      "Epoch : 882  Loss : 4.27  Grad : tensor([-0.1176,  0.6658])\n",
      "Epoch : 883  Loss : 4.27  Grad : tensor([-0.1174,  0.6647])\n",
      "Epoch : 884  Loss : 4.26  Grad : tensor([-0.1172,  0.6636])\n",
      "Epoch : 885  Loss : 4.26  Grad : tensor([-0.1170,  0.6625])\n",
      "Epoch : 886  Loss : 4.26  Grad : tensor([-0.1168,  0.6613])\n",
      "Epoch : 887  Loss : 4.25  Grad : tensor([-0.1166,  0.6602])\n",
      "Epoch : 888  Loss : 4.25  Grad : tensor([-0.1164,  0.6591])\n",
      "Epoch : 889  Loss : 4.24  Grad : tensor([-0.1162,  0.6580])\n",
      "Epoch : 890  Loss : 4.24  Grad : tensor([-0.1160,  0.6569])\n",
      "Epoch : 891  Loss : 4.23  Grad : tensor([-0.1158,  0.6557])\n",
      "Epoch : 892  Loss : 4.23  Grad : tensor([-0.1157,  0.6546])\n",
      "Epoch : 893  Loss : 4.22  Grad : tensor([-0.1154,  0.6535])\n",
      "Epoch : 894  Loss : 4.22  Grad : tensor([-0.1153,  0.6524])\n",
      "Epoch : 895  Loss : 4.22  Grad : tensor([-0.1151,  0.6513])\n",
      "Epoch : 896  Loss : 4.21  Grad : tensor([-0.1148,  0.6502])\n",
      "Epoch : 897  Loss : 4.21  Grad : tensor([-0.1147,  0.6491])\n",
      "Epoch : 898  Loss : 4.20  Grad : tensor([-0.1145,  0.6480])\n",
      "Epoch : 899  Loss : 4.20  Grad : tensor([-0.1143,  0.6469])\n",
      "Epoch : 900  Loss : 4.19  Grad : tensor([-0.1141,  0.6458])\n",
      "Epoch : 901  Loss : 4.19  Grad : tensor([-0.1139,  0.6447])\n",
      "Epoch : 902  Loss : 4.18  Grad : tensor([-0.1137,  0.6436])\n",
      "Epoch : 903  Loss : 4.18  Grad : tensor([-0.1135,  0.6425])\n",
      "Epoch : 904  Loss : 4.18  Grad : tensor([-0.1133,  0.6414])\n",
      "Epoch : 905  Loss : 4.17  Grad : tensor([-0.1131,  0.6403])\n",
      "Epoch : 906  Loss : 4.17  Grad : tensor([-0.1129,  0.6392])\n",
      "Epoch : 907  Loss : 4.16  Grad : tensor([-0.1127,  0.6381])\n",
      "Epoch : 908  Loss : 4.16  Grad : tensor([-0.1125,  0.6371])\n",
      "Epoch : 909  Loss : 4.16  Grad : tensor([-0.1124,  0.6360])\n",
      "Epoch : 910  Loss : 4.15  Grad : tensor([-0.1122,  0.6349])\n",
      "Epoch : 911  Loss : 4.15  Grad : tensor([-0.1120,  0.6338])\n",
      "Epoch : 912  Loss : 4.14  Grad : tensor([-0.1118,  0.6327])\n",
      "Epoch : 913  Loss : 4.14  Grad : tensor([-0.1116,  0.6317])\n",
      "Epoch : 914  Loss : 4.13  Grad : tensor([-0.1114,  0.6306])\n",
      "Epoch : 915  Loss : 4.13  Grad : tensor([-0.1112,  0.6295])\n",
      "Epoch : 916  Loss : 4.13  Grad : tensor([-0.1110,  0.6284])\n",
      "Epoch : 917  Loss : 4.12  Grad : tensor([-0.1108,  0.6274])\n",
      "Epoch : 918  Loss : 4.12  Grad : tensor([-0.1107,  0.6263])\n",
      "Epoch : 919  Loss : 4.11  Grad : tensor([-0.1104,  0.6253])\n",
      "Epoch : 920  Loss : 4.11  Grad : tensor([-0.1103,  0.6242])\n",
      "Epoch : 921  Loss : 4.11  Grad : tensor([-0.1101,  0.6231])\n",
      "Epoch : 922  Loss : 4.10  Grad : tensor([-0.1099,  0.6221])\n",
      "Epoch : 923  Loss : 4.10  Grad : tensor([-0.1097,  0.6210])\n",
      "Epoch : 924  Loss : 4.09  Grad : tensor([-0.1095,  0.6200])\n",
      "Epoch : 925  Loss : 4.09  Grad : tensor([-0.1093,  0.6189])\n",
      "Epoch : 926  Loss : 4.09  Grad : tensor([-0.1091,  0.6179])\n",
      "Epoch : 927  Loss : 4.08  Grad : tensor([-0.1090,  0.6168])\n",
      "Epoch : 928  Loss : 4.08  Grad : tensor([-0.1088,  0.6158])\n",
      "Epoch : 929  Loss : 4.07  Grad : tensor([-0.1086,  0.6147])\n",
      "Epoch : 930  Loss : 4.07  Grad : tensor([-0.1084,  0.6137])\n",
      "Epoch : 931  Loss : 4.07  Grad : tensor([-0.1082,  0.6126])\n",
      "Epoch : 932  Loss : 4.06  Grad : tensor([-0.1080,  0.6116])\n",
      "Epoch : 933  Loss : 4.06  Grad : tensor([-0.1079,  0.6105])\n",
      "Epoch : 934  Loss : 4.06  Grad : tensor([-0.1077,  0.6095])\n",
      "Epoch : 935  Loss : 4.05  Grad : tensor([-0.1075,  0.6085])\n",
      "Epoch : 936  Loss : 4.05  Grad : tensor([-0.1073,  0.6074])\n",
      "Epoch : 937  Loss : 4.04  Grad : tensor([-0.1071,  0.6064])\n",
      "Epoch : 938  Loss : 4.04  Grad : tensor([-0.1069,  0.6054])\n",
      "Epoch : 939  Loss : 4.04  Grad : tensor([-0.1068,  0.6043])\n",
      "Epoch : 940  Loss : 4.03  Grad : tensor([-0.1066,  0.6033])\n",
      "Epoch : 941  Loss : 4.03  Grad : tensor([-0.1064,  0.6023])\n",
      "Epoch : 942  Loss : 4.02  Grad : tensor([-0.1062,  0.6013])\n",
      "Epoch : 943  Loss : 4.02  Grad : tensor([-0.1060,  0.6003])\n",
      "Epoch : 944  Loss : 4.02  Grad : tensor([-0.1058,  0.5992])\n",
      "Epoch : 945  Loss : 4.01  Grad : tensor([-0.1057,  0.5982])\n",
      "Epoch : 946  Loss : 4.01  Grad : tensor([-0.1055,  0.5972])\n",
      "Epoch : 947  Loss : 4.01  Grad : tensor([-0.1053,  0.5962])\n",
      "Epoch : 948  Loss : 4.00  Grad : tensor([-0.1051,  0.5952])\n",
      "Epoch : 949  Loss : 4.00  Grad : tensor([-0.1050,  0.5942])\n",
      "Epoch : 950  Loss : 4.00  Grad : tensor([-0.1048,  0.5931])\n",
      "Epoch : 951  Loss : 3.99  Grad : tensor([-0.1046,  0.5921])\n",
      "Epoch : 952  Loss : 3.99  Grad : tensor([-0.1044,  0.5911])\n",
      "Epoch : 953  Loss : 3.98  Grad : tensor([-0.1042,  0.5901])\n",
      "Epoch : 954  Loss : 3.98  Grad : tensor([-0.1041,  0.5891])\n",
      "Epoch : 955  Loss : 3.98  Grad : tensor([-0.1039,  0.5881])\n",
      "Epoch : 956  Loss : 3.97  Grad : tensor([-0.1037,  0.5871])\n",
      "Epoch : 957  Loss : 3.97  Grad : tensor([-0.1035,  0.5861])\n",
      "Epoch : 958  Loss : 3.97  Grad : tensor([-0.1034,  0.5851])\n",
      "Epoch : 959  Loss : 3.96  Grad : tensor([-0.1032,  0.5841])\n",
      "Epoch : 960  Loss : 3.96  Grad : tensor([-0.1030,  0.5831])\n",
      "Epoch : 961  Loss : 3.96  Grad : tensor([-0.1028,  0.5822])\n",
      "Epoch : 962  Loss : 3.95  Grad : tensor([-0.1026,  0.5812])\n",
      "Epoch : 963  Loss : 3.95  Grad : tensor([-0.1025,  0.5802])\n",
      "Epoch : 964  Loss : 3.95  Grad : tensor([-0.1023,  0.5792])\n",
      "Epoch : 965  Loss : 3.94  Grad : tensor([-0.1021,  0.5782])\n",
      "Epoch : 966  Loss : 3.94  Grad : tensor([-0.1020,  0.5772])\n",
      "Epoch : 967  Loss : 3.94  Grad : tensor([-0.1018,  0.5762])\n",
      "Epoch : 968  Loss : 3.93  Grad : tensor([-0.1016,  0.5753])\n",
      "Epoch : 969  Loss : 3.93  Grad : tensor([-0.1015,  0.5743])\n",
      "Epoch : 970  Loss : 3.93  Grad : tensor([-0.1013,  0.5733])\n",
      "Epoch : 971  Loss : 3.92  Grad : tensor([-0.1011,  0.5723])\n",
      "Epoch : 972  Loss : 3.92  Grad : tensor([-0.1009,  0.5714])\n",
      "Epoch : 973  Loss : 3.92  Grad : tensor([-0.1008,  0.5704])\n",
      "Epoch : 974  Loss : 3.91  Grad : tensor([-0.1006,  0.5694])\n",
      "Epoch : 975  Loss : 3.91  Grad : tensor([-0.1004,  0.5685])\n",
      "Epoch : 976  Loss : 3.91  Grad : tensor([-0.1003,  0.5675])\n",
      "Epoch : 977  Loss : 3.90  Grad : tensor([-0.1001,  0.5665])\n",
      "Epoch : 978  Loss : 3.90  Grad : tensor([-0.0999,  0.5656])\n",
      "Epoch : 979  Loss : 3.90  Grad : tensor([-0.0997,  0.5646])\n",
      "Epoch : 980  Loss : 3.89  Grad : tensor([-0.0996,  0.5637])\n",
      "Epoch : 981  Loss : 3.89  Grad : tensor([-0.0994,  0.5627])\n",
      "Epoch : 982  Loss : 3.89  Grad : tensor([-0.0992,  0.5617])\n",
      "Epoch : 983  Loss : 3.88  Grad : tensor([-0.0991,  0.5608])\n",
      "Epoch : 984  Loss : 3.88  Grad : tensor([-0.0989,  0.5598])\n",
      "Epoch : 985  Loss : 3.88  Grad : tensor([-0.0987,  0.5589])\n",
      "Epoch : 986  Loss : 3.87  Grad : tensor([-0.0986,  0.5579])\n",
      "Epoch : 987  Loss : 3.87  Grad : tensor([-0.0984,  0.5570])\n",
      "Epoch : 988  Loss : 3.87  Grad : tensor([-0.0982,  0.5560])\n",
      "Epoch : 989  Loss : 3.86  Grad : tensor([-0.0981,  0.5551])\n",
      "Epoch : 990  Loss : 3.86  Grad : tensor([-0.0979,  0.5541])\n",
      "Epoch : 991  Loss : 3.86  Grad : tensor([-0.0978,  0.5532])\n",
      "Epoch : 992  Loss : 3.85  Grad : tensor([-0.0976,  0.5523])\n",
      "Epoch : 993  Loss : 3.85  Grad : tensor([-0.0974,  0.5513])\n",
      "Epoch : 994  Loss : 3.85  Grad : tensor([-0.0973,  0.5504])\n",
      "Epoch : 995  Loss : 3.84  Grad : tensor([-0.0971,  0.5495])\n",
      "Epoch : 996  Loss : 3.84  Grad : tensor([-0.0969,  0.5485])\n",
      "Epoch : 997  Loss : 3.84  Grad : tensor([-0.0967,  0.5476])\n",
      "Epoch : 998  Loss : 3.83  Grad : tensor([-0.0966,  0.5467])\n",
      "Epoch : 999  Loss : 3.83  Grad : tensor([-0.0964,  0.5457])\n",
      "Epoch : 1000  Loss : 3.83  Grad : tensor([-0.0962,  0.5448])\n",
      "Epoch : 1001  Loss : 3.83  Grad : tensor([-0.0961,  0.5439])\n",
      "Epoch : 1002  Loss : 3.82  Grad : tensor([-0.0959,  0.5430])\n",
      "Epoch : 1003  Loss : 3.82  Grad : tensor([-0.0957,  0.5420])\n",
      "Epoch : 1004  Loss : 3.82  Grad : tensor([-0.0956,  0.5411])\n",
      "Epoch : 1005  Loss : 3.81  Grad : tensor([-0.0954,  0.5402])\n",
      "Epoch : 1006  Loss : 3.81  Grad : tensor([-0.0953,  0.5393])\n",
      "Epoch : 1007  Loss : 3.81  Grad : tensor([-0.0951,  0.5384])\n",
      "Epoch : 1008  Loss : 3.80  Grad : tensor([-0.0949,  0.5375])\n",
      "Epoch : 1009  Loss : 3.80  Grad : tensor([-0.0948,  0.5365])\n",
      "Epoch : 1010  Loss : 3.80  Grad : tensor([-0.0946,  0.5356])\n",
      "Epoch : 1011  Loss : 3.80  Grad : tensor([-0.0945,  0.5347])\n",
      "Epoch : 1012  Loss : 3.79  Grad : tensor([-0.0943,  0.5338])\n",
      "Epoch : 1013  Loss : 3.79  Grad : tensor([-0.0942,  0.5329])\n",
      "Epoch : 1014  Loss : 3.79  Grad : tensor([-0.0940,  0.5320])\n",
      "Epoch : 1015  Loss : 3.78  Grad : tensor([-0.0938,  0.5311])\n",
      "Epoch : 1016  Loss : 3.78  Grad : tensor([-0.0937,  0.5302])\n",
      "Epoch : 1017  Loss : 3.78  Grad : tensor([-0.0935,  0.5293])\n",
      "Epoch : 1018  Loss : 3.78  Grad : tensor([-0.0933,  0.5284])\n",
      "Epoch : 1019  Loss : 3.77  Grad : tensor([-0.0932,  0.5275])\n",
      "Epoch : 1020  Loss : 3.77  Grad : tensor([-0.0930,  0.5266])\n",
      "Epoch : 1021  Loss : 3.77  Grad : tensor([-0.0929,  0.5257])\n",
      "Epoch : 1022  Loss : 3.76  Grad : tensor([-0.0927,  0.5248])\n",
      "Epoch : 1023  Loss : 3.76  Grad : tensor([-0.0926,  0.5239])\n",
      "Epoch : 1024  Loss : 3.76  Grad : tensor([-0.0924,  0.5230])\n",
      "Epoch : 1025  Loss : 3.76  Grad : tensor([-0.0922,  0.5221])\n",
      "Epoch : 1026  Loss : 3.75  Grad : tensor([-0.0921,  0.5213])\n",
      "Epoch : 1027  Loss : 3.75  Grad : tensor([-0.0919,  0.5204])\n",
      "Epoch : 1028  Loss : 3.75  Grad : tensor([-0.0918,  0.5195])\n",
      "Epoch : 1029  Loss : 3.74  Grad : tensor([-0.0916,  0.5186])\n",
      "Epoch : 1030  Loss : 3.74  Grad : tensor([-0.0915,  0.5177])\n",
      "Epoch : 1031  Loss : 3.74  Grad : tensor([-0.0913,  0.5168])\n",
      "Epoch : 1032  Loss : 3.74  Grad : tensor([-0.0912,  0.5160])\n",
      "Epoch : 1033  Loss : 3.73  Grad : tensor([-0.0910,  0.5151])\n",
      "Epoch : 1034  Loss : 3.73  Grad : tensor([-0.0908,  0.5142])\n",
      "Epoch : 1035  Loss : 3.73  Grad : tensor([-0.0907,  0.5133])\n",
      "Epoch : 1036  Loss : 3.72  Grad : tensor([-0.0905,  0.5125])\n",
      "Epoch : 1037  Loss : 3.72  Grad : tensor([-0.0904,  0.5116])\n",
      "Epoch : 1038  Loss : 3.72  Grad : tensor([-0.0902,  0.5107])\n",
      "Epoch : 1039  Loss : 3.72  Grad : tensor([-0.0901,  0.5099])\n",
      "Epoch : 1040  Loss : 3.71  Grad : tensor([-0.0899,  0.5090])\n",
      "Epoch : 1041  Loss : 3.71  Grad : tensor([-0.0898,  0.5081])\n",
      "Epoch : 1042  Loss : 3.71  Grad : tensor([-0.0896,  0.5073])\n",
      "Epoch : 1043  Loss : 3.71  Grad : tensor([-0.0895,  0.5064])\n",
      "Epoch : 1044  Loss : 3.70  Grad : tensor([-0.0893,  0.5055])\n",
      "Epoch : 1045  Loss : 3.70  Grad : tensor([-0.0892,  0.5047])\n",
      "Epoch : 1046  Loss : 3.70  Grad : tensor([-0.0890,  0.5038])\n",
      "Epoch : 1047  Loss : 3.70  Grad : tensor([-0.0888,  0.5030])\n",
      "Epoch : 1048  Loss : 3.69  Grad : tensor([-0.0887,  0.5021])\n",
      "Epoch : 1049  Loss : 3.69  Grad : tensor([-0.0886,  0.5013])\n",
      "Epoch : 1050  Loss : 3.69  Grad : tensor([-0.0884,  0.5004])\n",
      "Epoch : 1051  Loss : 3.69  Grad : tensor([-0.0882,  0.4996])\n",
      "Epoch : 1052  Loss : 3.68  Grad : tensor([-0.0881,  0.4987])\n",
      "Epoch : 1053  Loss : 3.68  Grad : tensor([-0.0879,  0.4979])\n",
      "Epoch : 1054  Loss : 3.68  Grad : tensor([-0.0878,  0.4970])\n",
      "Epoch : 1055  Loss : 3.67  Grad : tensor([-0.0877,  0.4962])\n",
      "Epoch : 1056  Loss : 3.67  Grad : tensor([-0.0875,  0.4953])\n",
      "Epoch : 1057  Loss : 3.67  Grad : tensor([-0.0873,  0.4945])\n",
      "Epoch : 1058  Loss : 3.67  Grad : tensor([-0.0872,  0.4936])\n",
      "Epoch : 1059  Loss : 3.66  Grad : tensor([-0.0870,  0.4928])\n",
      "Epoch : 1060  Loss : 3.66  Grad : tensor([-0.0869,  0.4920])\n",
      "Epoch : 1061  Loss : 3.66  Grad : tensor([-0.0868,  0.4911])\n",
      "Epoch : 1062  Loss : 3.66  Grad : tensor([-0.0866,  0.4903])\n",
      "Epoch : 1063  Loss : 3.65  Grad : tensor([-0.0865,  0.4895])\n",
      "Epoch : 1064  Loss : 3.65  Grad : tensor([-0.0863,  0.4886])\n",
      "Epoch : 1065  Loss : 3.65  Grad : tensor([-0.0862,  0.4878])\n",
      "Epoch : 1066  Loss : 3.65  Grad : tensor([-0.0860,  0.4870])\n",
      "Epoch : 1067  Loss : 3.64  Grad : tensor([-0.0859,  0.4862])\n",
      "Epoch : 1068  Loss : 3.64  Grad : tensor([-0.0857,  0.4853])\n",
      "Epoch : 1069  Loss : 3.64  Grad : tensor([-0.0856,  0.4845])\n",
      "Epoch : 1070  Loss : 3.64  Grad : tensor([-0.0854,  0.4837])\n",
      "Epoch : 1071  Loss : 3.64  Grad : tensor([-0.0853,  0.4829])\n",
      "Epoch : 1072  Loss : 3.63  Grad : tensor([-0.0851,  0.4820])\n",
      "Epoch : 1073  Loss : 3.63  Grad : tensor([-0.0850,  0.4812])\n",
      "Epoch : 1074  Loss : 3.63  Grad : tensor([-0.0849,  0.4804])\n",
      "Epoch : 1075  Loss : 3.63  Grad : tensor([-0.0847,  0.4796])\n",
      "Epoch : 1076  Loss : 3.62  Grad : tensor([-0.0846,  0.4788])\n",
      "Epoch : 1077  Loss : 3.62  Grad : tensor([-0.0844,  0.4780])\n",
      "Epoch : 1078  Loss : 3.62  Grad : tensor([-0.0843,  0.4771])\n",
      "Epoch : 1079  Loss : 3.62  Grad : tensor([-0.0841,  0.4763])\n",
      "Epoch : 1080  Loss : 3.61  Grad : tensor([-0.0840,  0.4755])\n",
      "Epoch : 1081  Loss : 3.61  Grad : tensor([-0.0839,  0.4747])\n",
      "Epoch : 1082  Loss : 3.61  Grad : tensor([-0.0837,  0.4739])\n",
      "Epoch : 1083  Loss : 3.61  Grad : tensor([-0.0836,  0.4731])\n",
      "Epoch : 1084  Loss : 3.60  Grad : tensor([-0.0834,  0.4723])\n",
      "Epoch : 1085  Loss : 3.60  Grad : tensor([-0.0833,  0.4715])\n",
      "Epoch : 1086  Loss : 3.60  Grad : tensor([-0.0832,  0.4707])\n",
      "Epoch : 1087  Loss : 3.60  Grad : tensor([-0.0830,  0.4699])\n",
      "Epoch : 1088  Loss : 3.60  Grad : tensor([-0.0829,  0.4691])\n",
      "Epoch : 1089  Loss : 3.59  Grad : tensor([-0.0827,  0.4683])\n",
      "Epoch : 1090  Loss : 3.59  Grad : tensor([-0.0826,  0.4675])\n",
      "Epoch : 1091  Loss : 3.59  Grad : tensor([-0.0824,  0.4667])\n",
      "Epoch : 1092  Loss : 3.59  Grad : tensor([-0.0823,  0.4659])\n",
      "Epoch : 1093  Loss : 3.58  Grad : tensor([-0.0822,  0.4651])\n",
      "Epoch : 1094  Loss : 3.58  Grad : tensor([-0.0820,  0.4643])\n",
      "Epoch : 1095  Loss : 3.58  Grad : tensor([-0.0819,  0.4636])\n",
      "Epoch : 1096  Loss : 3.58  Grad : tensor([-0.0818,  0.4628])\n",
      "Epoch : 1097  Loss : 3.58  Grad : tensor([-0.0816,  0.4620])\n",
      "Epoch : 1098  Loss : 3.57  Grad : tensor([-0.0815,  0.4612])\n",
      "Epoch : 1099  Loss : 3.57  Grad : tensor([-0.0813,  0.4604])\n",
      "Epoch : 1100  Loss : 3.57  Grad : tensor([-0.0812,  0.4596])\n",
      "Epoch : 1101  Loss : 3.57  Grad : tensor([-0.0810,  0.4588])\n",
      "Epoch : 1102  Loss : 3.56  Grad : tensor([-0.0809,  0.4581])\n",
      "Epoch : 1103  Loss : 3.56  Grad : tensor([-0.0808,  0.4573])\n",
      "Epoch : 1104  Loss : 3.56  Grad : tensor([-0.0806,  0.4565])\n",
      "Epoch : 1105  Loss : 3.56  Grad : tensor([-0.0805,  0.4557])\n",
      "Epoch : 1106  Loss : 3.56  Grad : tensor([-0.0804,  0.4550])\n",
      "Epoch : 1107  Loss : 3.55  Grad : tensor([-0.0802,  0.4542])\n",
      "Epoch : 1108  Loss : 3.55  Grad : tensor([-0.0801,  0.4534])\n",
      "Epoch : 1109  Loss : 3.55  Grad : tensor([-0.0799,  0.4527])\n",
      "Epoch : 1110  Loss : 3.55  Grad : tensor([-0.0798,  0.4519])\n",
      "Epoch : 1111  Loss : 3.55  Grad : tensor([-0.0797,  0.4511])\n",
      "Epoch : 1112  Loss : 3.54  Grad : tensor([-0.0796,  0.4503])\n",
      "Epoch : 1113  Loss : 3.54  Grad : tensor([-0.0794,  0.4496])\n",
      "Epoch : 1114  Loss : 3.54  Grad : tensor([-0.0793,  0.4488])\n",
      "Epoch : 1115  Loss : 3.54  Grad : tensor([-0.0791,  0.4481])\n",
      "Epoch : 1116  Loss : 3.53  Grad : tensor([-0.0790,  0.4473])\n",
      "Epoch : 1117  Loss : 3.53  Grad : tensor([-0.0789,  0.4465])\n",
      "Epoch : 1118  Loss : 3.53  Grad : tensor([-0.0787,  0.4458])\n",
      "Epoch : 1119  Loss : 3.53  Grad : tensor([-0.0786,  0.4450])\n",
      "Epoch : 1120  Loss : 3.53  Grad : tensor([-0.0785,  0.4443])\n",
      "Epoch : 1121  Loss : 3.52  Grad : tensor([-0.0784,  0.4435])\n",
      "Epoch : 1122  Loss : 3.52  Grad : tensor([-0.0782,  0.4428])\n",
      "Epoch : 1123  Loss : 3.52  Grad : tensor([-0.0781,  0.4420])\n",
      "Epoch : 1124  Loss : 3.52  Grad : tensor([-0.0779,  0.4413])\n",
      "Epoch : 1125  Loss : 3.52  Grad : tensor([-0.0778,  0.4405])\n",
      "Epoch : 1126  Loss : 3.51  Grad : tensor([-0.0777,  0.4398])\n",
      "Epoch : 1127  Loss : 3.51  Grad : tensor([-0.0775,  0.4390])\n",
      "Epoch : 1128  Loss : 3.51  Grad : tensor([-0.0774,  0.4383])\n",
      "Epoch : 1129  Loss : 3.51  Grad : tensor([-0.0773,  0.4375])\n",
      "Epoch : 1130  Loss : 3.51  Grad : tensor([-0.0772,  0.4368])\n",
      "Epoch : 1131  Loss : 3.50  Grad : tensor([-0.0770,  0.4360])\n",
      "Epoch : 1132  Loss : 3.50  Grad : tensor([-0.0769,  0.4353])\n",
      "Epoch : 1133  Loss : 3.50  Grad : tensor([-0.0767,  0.4346])\n",
      "Epoch : 1134  Loss : 3.50  Grad : tensor([-0.0766,  0.4338])\n",
      "Epoch : 1135  Loss : 3.50  Grad : tensor([-0.0765,  0.4331])\n",
      "Epoch : 1136  Loss : 3.49  Grad : tensor([-0.0764,  0.4323])\n",
      "Epoch : 1137  Loss : 3.49  Grad : tensor([-0.0763,  0.4316])\n",
      "Epoch : 1138  Loss : 3.49  Grad : tensor([-0.0761,  0.4309])\n",
      "Epoch : 1139  Loss : 3.49  Grad : tensor([-0.0760,  0.4301])\n",
      "Epoch : 1140  Loss : 3.49  Grad : tensor([-0.0759,  0.4294])\n",
      "Epoch : 1141  Loss : 3.49  Grad : tensor([-0.0757,  0.4287])\n",
      "Epoch : 1142  Loss : 3.48  Grad : tensor([-0.0756,  0.4280])\n",
      "Epoch : 1143  Loss : 3.48  Grad : tensor([-0.0755,  0.4272])\n",
      "Epoch : 1144  Loss : 3.48  Grad : tensor([-0.0753,  0.4265])\n",
      "Epoch : 1145  Loss : 3.48  Grad : tensor([-0.0752,  0.4258])\n",
      "Epoch : 1146  Loss : 3.48  Grad : tensor([-0.0751,  0.4250])\n",
      "Epoch : 1147  Loss : 3.47  Grad : tensor([-0.0750,  0.4243])\n",
      "Epoch : 1148  Loss : 3.47  Grad : tensor([-0.0748,  0.4236])\n",
      "Epoch : 1149  Loss : 3.47  Grad : tensor([-0.0747,  0.4229])\n",
      "Epoch : 1150  Loss : 3.47  Grad : tensor([-0.0746,  0.4222])\n",
      "Epoch : 1151  Loss : 3.47  Grad : tensor([-0.0745,  0.4215])\n",
      "Epoch : 1152  Loss : 3.46  Grad : tensor([-0.0743,  0.4207])\n",
      "Epoch : 1153  Loss : 3.46  Grad : tensor([-0.0742,  0.4200])\n",
      "Epoch : 1154  Loss : 3.46  Grad : tensor([-0.0741,  0.4193])\n",
      "Epoch : 1155  Loss : 3.46  Grad : tensor([-0.0739,  0.4186])\n",
      "Epoch : 1156  Loss : 3.46  Grad : tensor([-0.0738,  0.4179])\n",
      "Epoch : 1157  Loss : 3.46  Grad : tensor([-0.0737,  0.4172])\n",
      "Epoch : 1158  Loss : 3.45  Grad : tensor([-0.0736,  0.4165])\n",
      "Epoch : 1159  Loss : 3.45  Grad : tensor([-0.0734,  0.4158])\n",
      "Epoch : 1160  Loss : 3.45  Grad : tensor([-0.0733,  0.4151])\n",
      "Epoch : 1161  Loss : 3.45  Grad : tensor([-0.0732,  0.4143])\n",
      "Epoch : 1162  Loss : 3.45  Grad : tensor([-0.0731,  0.4136])\n",
      "Epoch : 1163  Loss : 3.45  Grad : tensor([-0.0730,  0.4129])\n",
      "Epoch : 1164  Loss : 3.44  Grad : tensor([-0.0728,  0.4122])\n",
      "Epoch : 1165  Loss : 3.44  Grad : tensor([-0.0727,  0.4115])\n",
      "Epoch : 1166  Loss : 3.44  Grad : tensor([-0.0726,  0.4108])\n",
      "Epoch : 1167  Loss : 3.44  Grad : tensor([-0.0725,  0.4101])\n",
      "Epoch : 1168  Loss : 3.44  Grad : tensor([-0.0723,  0.4094])\n",
      "Epoch : 1169  Loss : 3.43  Grad : tensor([-0.0722,  0.4087])\n",
      "Epoch : 1170  Loss : 3.43  Grad : tensor([-0.0721,  0.4081])\n",
      "Epoch : 1171  Loss : 3.43  Grad : tensor([-0.0720,  0.4074])\n",
      "Epoch : 1172  Loss : 3.43  Grad : tensor([-0.0719,  0.4067])\n",
      "Epoch : 1173  Loss : 3.43  Grad : tensor([-0.0717,  0.4060])\n",
      "Epoch : 1174  Loss : 3.43  Grad : tensor([-0.0716,  0.4053])\n",
      "Epoch : 1175  Loss : 3.42  Grad : tensor([-0.0715,  0.4046])\n",
      "Epoch : 1176  Loss : 3.42  Grad : tensor([-0.0714,  0.4039])\n",
      "Epoch : 1177  Loss : 3.42  Grad : tensor([-0.0712,  0.4032])\n",
      "Epoch : 1178  Loss : 3.42  Grad : tensor([-0.0711,  0.4025])\n",
      "Epoch : 1179  Loss : 3.42  Grad : tensor([-0.0710,  0.4019])\n",
      "Epoch : 1180  Loss : 3.42  Grad : tensor([-0.0709,  0.4012])\n",
      "Epoch : 1181  Loss : 3.41  Grad : tensor([-0.0708,  0.4005])\n",
      "Epoch : 1182  Loss : 3.41  Grad : tensor([-0.0706,  0.3998])\n",
      "Epoch : 1183  Loss : 3.41  Grad : tensor([-0.0705,  0.3991])\n",
      "Epoch : 1184  Loss : 3.41  Grad : tensor([-0.0704,  0.3985])\n",
      "Epoch : 1185  Loss : 3.41  Grad : tensor([-0.0703,  0.3978])\n",
      "Epoch : 1186  Loss : 3.41  Grad : tensor([-0.0701,  0.3971])\n",
      "Epoch : 1187  Loss : 3.40  Grad : tensor([-0.0700,  0.3964])\n",
      "Epoch : 1188  Loss : 3.40  Grad : tensor([-0.0699,  0.3958])\n",
      "Epoch : 1189  Loss : 3.40  Grad : tensor([-0.0698,  0.3951])\n",
      "Epoch : 1190  Loss : 3.40  Grad : tensor([-0.0697,  0.3944])\n",
      "Epoch : 1191  Loss : 3.40  Grad : tensor([-0.0696,  0.3937])\n",
      "Epoch : 1192  Loss : 3.40  Grad : tensor([-0.0694,  0.3931])\n",
      "Epoch : 1193  Loss : 3.40  Grad : tensor([-0.0693,  0.3924])\n",
      "Epoch : 1194  Loss : 3.39  Grad : tensor([-0.0692,  0.3917])\n",
      "Epoch : 1195  Loss : 3.39  Grad : tensor([-0.0691,  0.3911])\n",
      "Epoch : 1196  Loss : 3.39  Grad : tensor([-0.0690,  0.3904])\n",
      "Epoch : 1197  Loss : 3.39  Grad : tensor([-0.0689,  0.3897])\n",
      "Epoch : 1198  Loss : 3.39  Grad : tensor([-0.0687,  0.3891])\n",
      "Epoch : 1199  Loss : 3.39  Grad : tensor([-0.0686,  0.3884])\n",
      "Epoch : 1200  Loss : 3.38  Grad : tensor([-0.0685,  0.3878])\n",
      "Epoch : 1201  Loss : 3.38  Grad : tensor([-0.0684,  0.3871])\n",
      "Epoch : 1202  Loss : 3.38  Grad : tensor([-0.0683,  0.3864])\n",
      "Epoch : 1203  Loss : 3.38  Grad : tensor([-0.0681,  0.3858])\n",
      "Epoch : 1204  Loss : 3.38  Grad : tensor([-0.0680,  0.3851])\n",
      "Epoch : 1205  Loss : 3.38  Grad : tensor([-0.0679,  0.3845])\n",
      "Epoch : 1206  Loss : 3.37  Grad : tensor([-0.0678,  0.3838])\n",
      "Epoch : 1207  Loss : 3.37  Grad : tensor([-0.0677,  0.3832])\n",
      "Epoch : 1208  Loss : 3.37  Grad : tensor([-0.0676,  0.3825])\n",
      "Epoch : 1209  Loss : 3.37  Grad : tensor([-0.0675,  0.3819])\n",
      "Epoch : 1210  Loss : 3.37  Grad : tensor([-0.0673,  0.3812])\n",
      "Epoch : 1211  Loss : 3.37  Grad : tensor([-0.0672,  0.3806])\n",
      "Epoch : 1212  Loss : 3.37  Grad : tensor([-0.0671,  0.3799])\n",
      "Epoch : 1213  Loss : 3.36  Grad : tensor([-0.0670,  0.3793])\n",
      "Epoch : 1214  Loss : 3.36  Grad : tensor([-0.0669,  0.3786])\n",
      "Epoch : 1215  Loss : 3.36  Grad : tensor([-0.0668,  0.3780])\n",
      "Epoch : 1216  Loss : 3.36  Grad : tensor([-0.0667,  0.3774])\n",
      "Epoch : 1217  Loss : 3.36  Grad : tensor([-0.0665,  0.3767])\n",
      "Epoch : 1218  Loss : 3.36  Grad : tensor([-0.0664,  0.3761])\n",
      "Epoch : 1219  Loss : 3.36  Grad : tensor([-0.0663,  0.3754])\n",
      "Epoch : 1220  Loss : 3.35  Grad : tensor([-0.0662,  0.3748])\n",
      "Epoch : 1221  Loss : 3.35  Grad : tensor([-0.0661,  0.3742])\n",
      "Epoch : 1222  Loss : 3.35  Grad : tensor([-0.0660,  0.3735])\n",
      "Epoch : 1223  Loss : 3.35  Grad : tensor([-0.0659,  0.3729])\n",
      "Epoch : 1224  Loss : 3.35  Grad : tensor([-0.0657,  0.3723])\n",
      "Epoch : 1225  Loss : 3.35  Grad : tensor([-0.0656,  0.3716])\n",
      "Epoch : 1226  Loss : 3.35  Grad : tensor([-0.0655,  0.3710])\n",
      "Epoch : 1227  Loss : 3.34  Grad : tensor([-0.0654,  0.3704])\n",
      "Epoch : 1228  Loss : 3.34  Grad : tensor([-0.0653,  0.3697])\n",
      "Epoch : 1229  Loss : 3.34  Grad : tensor([-0.0652,  0.3691])\n",
      "Epoch : 1230  Loss : 3.34  Grad : tensor([-0.0651,  0.3685])\n",
      "Epoch : 1231  Loss : 3.34  Grad : tensor([-0.0650,  0.3679])\n",
      "Epoch : 1232  Loss : 3.34  Grad : tensor([-0.0649,  0.3672])\n",
      "Epoch : 1233  Loss : 3.34  Grad : tensor([-0.0648,  0.3666])\n",
      "Epoch : 1234  Loss : 3.33  Grad : tensor([-0.0646,  0.3660])\n",
      "Epoch : 1235  Loss : 3.33  Grad : tensor([-0.0645,  0.3654])\n",
      "Epoch : 1236  Loss : 3.33  Grad : tensor([-0.0644,  0.3647])\n",
      "Epoch : 1237  Loss : 3.33  Grad : tensor([-0.0643,  0.3641])\n",
      "Epoch : 1238  Loss : 3.33  Grad : tensor([-0.0642,  0.3635])\n",
      "Epoch : 1239  Loss : 3.33  Grad : tensor([-0.0641,  0.3629])\n",
      "Epoch : 1240  Loss : 3.33  Grad : tensor([-0.0640,  0.3623])\n",
      "Epoch : 1241  Loss : 3.32  Grad : tensor([-0.0639,  0.3617])\n",
      "Epoch : 1242  Loss : 3.32  Grad : tensor([-0.0638,  0.3610])\n",
      "Epoch : 1243  Loss : 3.32  Grad : tensor([-0.0637,  0.3604])\n",
      "Epoch : 1244  Loss : 3.32  Grad : tensor([-0.0636,  0.3598])\n",
      "Epoch : 1245  Loss : 3.32  Grad : tensor([-0.0635,  0.3592])\n",
      "Epoch : 1246  Loss : 3.32  Grad : tensor([-0.0633,  0.3586])\n",
      "Epoch : 1247  Loss : 3.32  Grad : tensor([-0.0633,  0.3580])\n",
      "Epoch : 1248  Loss : 3.32  Grad : tensor([-0.0631,  0.3574])\n",
      "Epoch : 1249  Loss : 3.31  Grad : tensor([-0.0630,  0.3568])\n",
      "Epoch : 1250  Loss : 3.31  Grad : tensor([-0.0629,  0.3562])\n",
      "Epoch : 1251  Loss : 3.31  Grad : tensor([-0.0628,  0.3556])\n",
      "Epoch : 1252  Loss : 3.31  Grad : tensor([-0.0627,  0.3550])\n",
      "Epoch : 1253  Loss : 3.31  Grad : tensor([-0.0626,  0.3543])\n",
      "Epoch : 1254  Loss : 3.31  Grad : tensor([-0.0625,  0.3537])\n",
      "Epoch : 1255  Loss : 3.31  Grad : tensor([-0.0624,  0.3531])\n",
      "Epoch : 1256  Loss : 3.30  Grad : tensor([-0.0623,  0.3525])\n",
      "Epoch : 1257  Loss : 3.30  Grad : tensor([-0.0622,  0.3519])\n",
      "Epoch : 1258  Loss : 3.30  Grad : tensor([-0.0620,  0.3514])\n",
      "Epoch : 1259  Loss : 3.30  Grad : tensor([-0.0620,  0.3508])\n",
      "Epoch : 1260  Loss : 3.30  Grad : tensor([-0.0619,  0.3502])\n",
      "Epoch : 1261  Loss : 3.30  Grad : tensor([-0.0618,  0.3496])\n",
      "Epoch : 1262  Loss : 3.30  Grad : tensor([-0.0616,  0.3490])\n",
      "Epoch : 1263  Loss : 3.30  Grad : tensor([-0.0615,  0.3484])\n",
      "Epoch : 1264  Loss : 3.29  Grad : tensor([-0.0614,  0.3478])\n",
      "Epoch : 1265  Loss : 3.29  Grad : tensor([-0.0613,  0.3472])\n",
      "Epoch : 1266  Loss : 3.29  Grad : tensor([-0.0612,  0.3466])\n",
      "Epoch : 1267  Loss : 3.29  Grad : tensor([-0.0611,  0.3460])\n",
      "Epoch : 1268  Loss : 3.29  Grad : tensor([-0.0610,  0.3454])\n",
      "Epoch : 1269  Loss : 3.29  Grad : tensor([-0.0609,  0.3448])\n",
      "Epoch : 1270  Loss : 3.29  Grad : tensor([-0.0608,  0.3443])\n",
      "Epoch : 1271  Loss : 3.29  Grad : tensor([-0.0607,  0.3437])\n",
      "Epoch : 1272  Loss : 3.28  Grad : tensor([-0.0606,  0.3431])\n",
      "Epoch : 1273  Loss : 3.28  Grad : tensor([-0.0605,  0.3425])\n",
      "Epoch : 1274  Loss : 3.28  Grad : tensor([-0.0604,  0.3419])\n",
      "Epoch : 1275  Loss : 3.28  Grad : tensor([-0.0603,  0.3413])\n",
      "Epoch : 1276  Loss : 3.28  Grad : tensor([-0.0602,  0.3408])\n",
      "Epoch : 1277  Loss : 3.28  Grad : tensor([-0.0601,  0.3402])\n",
      "Epoch : 1278  Loss : 3.28  Grad : tensor([-0.0600,  0.3396])\n",
      "Epoch : 1279  Loss : 3.28  Grad : tensor([-0.0599,  0.3390])\n",
      "Epoch : 1280  Loss : 3.28  Grad : tensor([-0.0598,  0.3384])\n",
      "Epoch : 1281  Loss : 3.27  Grad : tensor([-0.0597,  0.3379])\n",
      "Epoch : 1282  Loss : 3.27  Grad : tensor([-0.0596,  0.3373])\n",
      "Epoch : 1283  Loss : 3.27  Grad : tensor([-0.0595,  0.3367])\n",
      "Epoch : 1284  Loss : 3.27  Grad : tensor([-0.0594,  0.3362])\n",
      "Epoch : 1285  Loss : 3.27  Grad : tensor([-0.0593,  0.3356])\n",
      "Epoch : 1286  Loss : 3.27  Grad : tensor([-0.0592,  0.3350])\n",
      "Epoch : 1287  Loss : 3.27  Grad : tensor([-0.0591,  0.3344])\n",
      "Epoch : 1288  Loss : 3.27  Grad : tensor([-0.0590,  0.3339])\n",
      "Epoch : 1289  Loss : 3.26  Grad : tensor([-0.0589,  0.3333])\n",
      "Epoch : 1290  Loss : 3.26  Grad : tensor([-0.0588,  0.3327])\n",
      "Epoch : 1291  Loss : 3.26  Grad : tensor([-0.0587,  0.3322])\n",
      "Epoch : 1292  Loss : 3.26  Grad : tensor([-0.0586,  0.3316])\n",
      "Epoch : 1293  Loss : 3.26  Grad : tensor([-0.0585,  0.3311])\n",
      "Epoch : 1294  Loss : 3.26  Grad : tensor([-0.0584,  0.3305])\n",
      "Epoch : 1295  Loss : 3.26  Grad : tensor([-0.0583,  0.3299])\n",
      "Epoch : 1296  Loss : 3.26  Grad : tensor([-0.0582,  0.3294])\n",
      "Epoch : 1297  Loss : 3.26  Grad : tensor([-0.0581,  0.3288])\n",
      "Epoch : 1298  Loss : 3.25  Grad : tensor([-0.0580,  0.3282])\n",
      "Epoch : 1299  Loss : 3.25  Grad : tensor([-0.0579,  0.3277])\n",
      "Epoch : 1300  Loss : 3.25  Grad : tensor([-0.0578,  0.3271])\n",
      "Epoch : 1301  Loss : 3.25  Grad : tensor([-0.0577,  0.3266])\n",
      "Epoch : 1302  Loss : 3.25  Grad : tensor([-0.0576,  0.3260])\n",
      "Epoch : 1303  Loss : 3.25  Grad : tensor([-0.0575,  0.3255])\n",
      "Epoch : 1304  Loss : 3.25  Grad : tensor([-0.0574,  0.3249])\n",
      "Epoch : 1305  Loss : 3.25  Grad : tensor([-0.0573,  0.3244])\n",
      "Epoch : 1306  Loss : 3.25  Grad : tensor([-0.0572,  0.3238])\n",
      "Epoch : 1307  Loss : 3.24  Grad : tensor([-0.0571,  0.3233])\n",
      "Epoch : 1308  Loss : 3.24  Grad : tensor([-0.0570,  0.3227])\n",
      "Epoch : 1309  Loss : 3.24  Grad : tensor([-0.0569,  0.3222])\n",
      "Epoch : 1310  Loss : 3.24  Grad : tensor([-0.0568,  0.3216])\n",
      "Epoch : 1311  Loss : 3.24  Grad : tensor([-0.0567,  0.3211])\n",
      "Epoch : 1312  Loss : 3.24  Grad : tensor([-0.0566,  0.3205])\n",
      "Epoch : 1313  Loss : 3.24  Grad : tensor([-0.0565,  0.3200])\n",
      "Epoch : 1314  Loss : 3.24  Grad : tensor([-0.0564,  0.3194])\n",
      "Epoch : 1315  Loss : 3.24  Grad : tensor([-0.0563,  0.3189])\n",
      "Epoch : 1316  Loss : 3.24  Grad : tensor([-0.0562,  0.3184])\n",
      "Epoch : 1317  Loss : 3.23  Grad : tensor([-0.0561,  0.3178])\n",
      "Epoch : 1318  Loss : 3.23  Grad : tensor([-0.0561,  0.3173])\n",
      "Epoch : 1319  Loss : 3.23  Grad : tensor([-0.0560,  0.3167])\n",
      "Epoch : 1320  Loss : 3.23  Grad : tensor([-0.0558,  0.3162])\n",
      "Epoch : 1321  Loss : 3.23  Grad : tensor([-0.0558,  0.3157])\n",
      "Epoch : 1322  Loss : 3.23  Grad : tensor([-0.0557,  0.3151])\n",
      "Epoch : 1323  Loss : 3.23  Grad : tensor([-0.0556,  0.3146])\n",
      "Epoch : 1324  Loss : 3.23  Grad : tensor([-0.0555,  0.3141])\n",
      "Epoch : 1325  Loss : 3.23  Grad : tensor([-0.0554,  0.3135])\n",
      "Epoch : 1326  Loss : 3.22  Grad : tensor([-0.0553,  0.3130])\n",
      "Epoch : 1327  Loss : 3.22  Grad : tensor([-0.0552,  0.3125])\n",
      "Epoch : 1328  Loss : 3.22  Grad : tensor([-0.0551,  0.3119])\n",
      "Epoch : 1329  Loss : 3.22  Grad : tensor([-0.0550,  0.3114])\n",
      "Epoch : 1330  Loss : 3.22  Grad : tensor([-0.0549,  0.3109])\n",
      "Epoch : 1331  Loss : 3.22  Grad : tensor([-0.0548,  0.3103])\n",
      "Epoch : 1332  Loss : 3.22  Grad : tensor([-0.0547,  0.3098])\n",
      "Epoch : 1333  Loss : 3.22  Grad : tensor([-0.0546,  0.3093])\n",
      "Epoch : 1334  Loss : 3.22  Grad : tensor([-0.0545,  0.3088])\n",
      "Epoch : 1335  Loss : 3.22  Grad : tensor([-0.0544,  0.3082])\n",
      "Epoch : 1336  Loss : 3.22  Grad : tensor([-0.0543,  0.3077])\n",
      "Epoch : 1337  Loss : 3.21  Grad : tensor([-0.0543,  0.3072])\n",
      "Epoch : 1338  Loss : 3.21  Grad : tensor([-0.0542,  0.3067])\n",
      "Epoch : 1339  Loss : 3.21  Grad : tensor([-0.0541,  0.3061])\n",
      "Epoch : 1340  Loss : 3.21  Grad : tensor([-0.0540,  0.3056])\n",
      "Epoch : 1341  Loss : 3.21  Grad : tensor([-0.0539,  0.3051])\n",
      "Epoch : 1342  Loss : 3.21  Grad : tensor([-0.0538,  0.3046])\n",
      "Epoch : 1343  Loss : 3.21  Grad : tensor([-0.0537,  0.3041])\n",
      "Epoch : 1344  Loss : 3.21  Grad : tensor([-0.0536,  0.3036])\n",
      "Epoch : 1345  Loss : 3.21  Grad : tensor([-0.0535,  0.3030])\n",
      "Epoch : 1346  Loss : 3.21  Grad : tensor([-0.0534,  0.3025])\n",
      "Epoch : 1347  Loss : 3.20  Grad : tensor([-0.0533,  0.3020])\n",
      "Epoch : 1348  Loss : 3.20  Grad : tensor([-0.0532,  0.3015])\n",
      "Epoch : 1349  Loss : 3.20  Grad : tensor([-0.0532,  0.3010])\n",
      "Epoch : 1350  Loss : 3.20  Grad : tensor([-0.0531,  0.3005])\n",
      "Epoch : 1351  Loss : 3.20  Grad : tensor([-0.0530,  0.3000])\n",
      "Epoch : 1352  Loss : 3.20  Grad : tensor([-0.0529,  0.2995])\n",
      "Epoch : 1353  Loss : 3.20  Grad : tensor([-0.0528,  0.2989])\n",
      "Epoch : 1354  Loss : 3.20  Grad : tensor([-0.0527,  0.2984])\n",
      "Epoch : 1355  Loss : 3.20  Grad : tensor([-0.0526,  0.2979])\n",
      "Epoch : 1356  Loss : 3.20  Grad : tensor([-0.0525,  0.2974])\n",
      "Epoch : 1357  Loss : 3.20  Grad : tensor([-0.0524,  0.2969])\n",
      "Epoch : 1358  Loss : 3.19  Grad : tensor([-0.0524,  0.2964])\n",
      "Epoch : 1359  Loss : 3.19  Grad : tensor([-0.0523,  0.2959])\n",
      "Epoch : 1360  Loss : 3.19  Grad : tensor([-0.0522,  0.2954])\n",
      "Epoch : 1361  Loss : 3.19  Grad : tensor([-0.0521,  0.2949])\n",
      "Epoch : 1362  Loss : 3.19  Grad : tensor([-0.0520,  0.2944])\n",
      "Epoch : 1363  Loss : 3.19  Grad : tensor([-0.0519,  0.2939])\n",
      "Epoch : 1364  Loss : 3.19  Grad : tensor([-0.0518,  0.2934])\n",
      "Epoch : 1365  Loss : 3.19  Grad : tensor([-0.0517,  0.2929])\n",
      "Epoch : 1366  Loss : 3.19  Grad : tensor([-0.0516,  0.2924])\n",
      "Epoch : 1367  Loss : 3.19  Grad : tensor([-0.0516,  0.2919])\n",
      "Epoch : 1368  Loss : 3.19  Grad : tensor([-0.0515,  0.2914])\n",
      "Epoch : 1369  Loss : 3.18  Grad : tensor([-0.0514,  0.2909])\n",
      "Epoch : 1370  Loss : 3.18  Grad : tensor([-0.0513,  0.2904])\n",
      "Epoch : 1371  Loss : 3.18  Grad : tensor([-0.0512,  0.2899])\n",
      "Epoch : 1372  Loss : 3.18  Grad : tensor([-0.0511,  0.2894])\n",
      "Epoch : 1373  Loss : 3.18  Grad : tensor([-0.0510,  0.2890])\n",
      "Epoch : 1374  Loss : 3.18  Grad : tensor([-0.0509,  0.2885])\n",
      "Epoch : 1375  Loss : 3.18  Grad : tensor([-0.0509,  0.2880])\n",
      "Epoch : 1376  Loss : 3.18  Grad : tensor([-0.0508,  0.2875])\n",
      "Epoch : 1377  Loss : 3.18  Grad : tensor([-0.0507,  0.2870])\n",
      "Epoch : 1378  Loss : 3.18  Grad : tensor([-0.0506,  0.2865])\n",
      "Epoch : 1379  Loss : 3.18  Grad : tensor([-0.0505,  0.2860])\n",
      "Epoch : 1380  Loss : 3.18  Grad : tensor([-0.0504,  0.2855])\n",
      "Epoch : 1381  Loss : 3.17  Grad : tensor([-0.0504,  0.2850])\n",
      "Epoch : 1382  Loss : 3.17  Grad : tensor([-0.0503,  0.2846])\n",
      "Epoch : 1383  Loss : 3.17  Grad : tensor([-0.0502,  0.2841])\n",
      "Epoch : 1384  Loss : 3.17  Grad : tensor([-0.0501,  0.2836])\n",
      "Epoch : 1385  Loss : 3.17  Grad : tensor([-0.0500,  0.2831])\n",
      "Epoch : 1386  Loss : 3.17  Grad : tensor([-0.0499,  0.2826])\n",
      "Epoch : 1387  Loss : 3.17  Grad : tensor([-0.0498,  0.2822])\n",
      "Epoch : 1388  Loss : 3.17  Grad : tensor([-0.0498,  0.2817])\n",
      "Epoch : 1389  Loss : 3.17  Grad : tensor([-0.0497,  0.2812])\n",
      "Epoch : 1390  Loss : 3.17  Grad : tensor([-0.0496,  0.2807])\n",
      "Epoch : 1391  Loss : 3.17  Grad : tensor([-0.0495,  0.2802])\n",
      "Epoch : 1392  Loss : 3.17  Grad : tensor([-0.0494,  0.2798])\n",
      "Epoch : 1393  Loss : 3.16  Grad : tensor([-0.0493,  0.2793])\n",
      "Epoch : 1394  Loss : 3.16  Grad : tensor([-0.0492,  0.2788])\n",
      "Epoch : 1395  Loss : 3.16  Grad : tensor([-0.0492,  0.2783])\n",
      "Epoch : 1396  Loss : 3.16  Grad : tensor([-0.0491,  0.2779])\n",
      "Epoch : 1397  Loss : 3.16  Grad : tensor([-0.0490,  0.2774])\n",
      "Epoch : 1398  Loss : 3.16  Grad : tensor([-0.0489,  0.2769])\n",
      "Epoch : 1399  Loss : 3.16  Grad : tensor([-0.0488,  0.2765])\n",
      "Epoch : 1400  Loss : 3.16  Grad : tensor([-0.0488,  0.2760])\n",
      "Epoch : 1401  Loss : 3.16  Grad : tensor([-0.0487,  0.2755])\n",
      "Epoch : 1402  Loss : 3.16  Grad : tensor([-0.0486,  0.2751])\n",
      "Epoch : 1403  Loss : 3.16  Grad : tensor([-0.0485,  0.2746])\n",
      "Epoch : 1404  Loss : 3.16  Grad : tensor([-0.0484,  0.2741])\n",
      "Epoch : 1405  Loss : 3.15  Grad : tensor([-0.0483,  0.2736])\n",
      "Epoch : 1406  Loss : 3.15  Grad : tensor([-0.0483,  0.2732])\n",
      "Epoch : 1407  Loss : 3.15  Grad : tensor([-0.0482,  0.2727])\n",
      "Epoch : 1408  Loss : 3.15  Grad : tensor([-0.0481,  0.2723])\n",
      "Epoch : 1409  Loss : 3.15  Grad : tensor([-0.0480,  0.2718])\n",
      "Epoch : 1410  Loss : 3.15  Grad : tensor([-0.0479,  0.2713])\n",
      "Epoch : 1411  Loss : 3.15  Grad : tensor([-0.0479,  0.2709])\n",
      "Epoch : 1412  Loss : 3.15  Grad : tensor([-0.0478,  0.2704])\n",
      "Epoch : 1413  Loss : 3.15  Grad : tensor([-0.0477,  0.2700])\n",
      "Epoch : 1414  Loss : 3.15  Grad : tensor([-0.0476,  0.2695])\n",
      "Epoch : 1415  Loss : 3.15  Grad : tensor([-0.0475,  0.2690])\n",
      "Epoch : 1416  Loss : 3.15  Grad : tensor([-0.0474,  0.2686])\n",
      "Epoch : 1417  Loss : 3.15  Grad : tensor([-0.0474,  0.2681])\n",
      "Epoch : 1418  Loss : 3.15  Grad : tensor([-0.0473,  0.2677])\n",
      "Epoch : 1419  Loss : 3.14  Grad : tensor([-0.0472,  0.2672])\n",
      "Epoch : 1420  Loss : 3.14  Grad : tensor([-0.0471,  0.2668])\n",
      "Epoch : 1421  Loss : 3.14  Grad : tensor([-0.0470,  0.2663])\n",
      "Epoch : 1422  Loss : 3.14  Grad : tensor([-0.0469,  0.2659])\n",
      "Epoch : 1423  Loss : 3.14  Grad : tensor([-0.0469,  0.2654])\n",
      "Epoch : 1424  Loss : 3.14  Grad : tensor([-0.0468,  0.2649])\n",
      "Epoch : 1425  Loss : 3.14  Grad : tensor([-0.0467,  0.2645])\n",
      "Epoch : 1426  Loss : 3.14  Grad : tensor([-0.0466,  0.2641])\n",
      "Epoch : 1427  Loss : 3.14  Grad : tensor([-0.0466,  0.2636])\n",
      "Epoch : 1428  Loss : 3.14  Grad : tensor([-0.0465,  0.2632])\n",
      "Epoch : 1429  Loss : 3.14  Grad : tensor([-0.0464,  0.2627])\n",
      "Epoch : 1430  Loss : 3.14  Grad : tensor([-0.0463,  0.2623])\n",
      "Epoch : 1431  Loss : 3.14  Grad : tensor([-0.0462,  0.2618])\n",
      "Epoch : 1432  Loss : 3.13  Grad : tensor([-0.0461,  0.2614])\n",
      "Epoch : 1433  Loss : 3.13  Grad : tensor([-0.0461,  0.2609])\n",
      "Epoch : 1434  Loss : 3.13  Grad : tensor([-0.0460,  0.2605])\n",
      "Epoch : 1435  Loss : 3.13  Grad : tensor([-0.0459,  0.2600])\n",
      "Epoch : 1436  Loss : 3.13  Grad : tensor([-0.0459,  0.2596])\n",
      "Epoch : 1437  Loss : 3.13  Grad : tensor([-0.0458,  0.2592])\n",
      "Epoch : 1438  Loss : 3.13  Grad : tensor([-0.0457,  0.2587])\n",
      "Epoch : 1439  Loss : 3.13  Grad : tensor([-0.0456,  0.2583])\n",
      "Epoch : 1440  Loss : 3.13  Grad : tensor([-0.0455,  0.2578])\n",
      "Epoch : 1441  Loss : 3.13  Grad : tensor([-0.0455,  0.2574])\n",
      "Epoch : 1442  Loss : 3.13  Grad : tensor([-0.0454,  0.2570])\n",
      "Epoch : 1443  Loss : 3.13  Grad : tensor([-0.0453,  0.2565])\n",
      "Epoch : 1444  Loss : 3.13  Grad : tensor([-0.0453,  0.2561])\n",
      "Epoch : 1445  Loss : 3.13  Grad : tensor([-0.0452,  0.2557])\n",
      "Epoch : 1446  Loss : 3.13  Grad : tensor([-0.0451,  0.2552])\n",
      "Epoch : 1447  Loss : 3.12  Grad : tensor([-0.0450,  0.2548])\n",
      "Epoch : 1448  Loss : 3.12  Grad : tensor([-0.0449,  0.2544])\n",
      "Epoch : 1449  Loss : 3.12  Grad : tensor([-0.0449,  0.2539])\n",
      "Epoch : 1450  Loss : 3.12  Grad : tensor([-0.0448,  0.2535])\n",
      "Epoch : 1451  Loss : 3.12  Grad : tensor([-0.0447,  0.2531])\n",
      "Epoch : 1452  Loss : 3.12  Grad : tensor([-0.0446,  0.2526])\n",
      "Epoch : 1453  Loss : 3.12  Grad : tensor([-0.0445,  0.2522])\n",
      "Epoch : 1454  Loss : 3.12  Grad : tensor([-0.0445,  0.2518])\n",
      "Epoch : 1455  Loss : 3.12  Grad : tensor([-0.0444,  0.2513])\n",
      "Epoch : 1456  Loss : 3.12  Grad : tensor([-0.0443,  0.2509])\n",
      "Epoch : 1457  Loss : 3.12  Grad : tensor([-0.0442,  0.2505])\n",
      "Epoch : 1458  Loss : 3.12  Grad : tensor([-0.0442,  0.2501])\n",
      "Epoch : 1459  Loss : 3.12  Grad : tensor([-0.0441,  0.2496])\n",
      "Epoch : 1460  Loss : 3.12  Grad : tensor([-0.0440,  0.2492])\n",
      "Epoch : 1461  Loss : 3.12  Grad : tensor([-0.0439,  0.2488])\n",
      "Epoch : 1462  Loss : 3.11  Grad : tensor([-0.0439,  0.2484])\n",
      "Epoch : 1463  Loss : 3.11  Grad : tensor([-0.0438,  0.2480])\n",
      "Epoch : 1464  Loss : 3.11  Grad : tensor([-0.0437,  0.2475])\n",
      "Epoch : 1465  Loss : 3.11  Grad : tensor([-0.0437,  0.2471])\n",
      "Epoch : 1466  Loss : 3.11  Grad : tensor([-0.0436,  0.2467])\n",
      "Epoch : 1467  Loss : 3.11  Grad : tensor([-0.0435,  0.2463])\n",
      "Epoch : 1468  Loss : 3.11  Grad : tensor([-0.0434,  0.2459])\n",
      "Epoch : 1469  Loss : 3.11  Grad : tensor([-0.0433,  0.2454])\n",
      "Epoch : 1470  Loss : 3.11  Grad : tensor([-0.0433,  0.2450])\n",
      "Epoch : 1471  Loss : 3.11  Grad : tensor([-0.0432,  0.2446])\n",
      "Epoch : 1472  Loss : 3.11  Grad : tensor([-0.0431,  0.2442])\n",
      "Epoch : 1473  Loss : 3.11  Grad : tensor([-0.0430,  0.2438])\n",
      "Epoch : 1474  Loss : 3.11  Grad : tensor([-0.0430,  0.2434])\n",
      "Epoch : 1475  Loss : 3.11  Grad : tensor([-0.0429,  0.2429])\n",
      "Epoch : 1476  Loss : 3.11  Grad : tensor([-0.0428,  0.2425])\n",
      "Epoch : 1477  Loss : 3.11  Grad : tensor([-0.0428,  0.2421])\n",
      "Epoch : 1478  Loss : 3.10  Grad : tensor([-0.0427,  0.2417])\n",
      "Epoch : 1479  Loss : 3.10  Grad : tensor([-0.0426,  0.2413])\n",
      "Epoch : 1480  Loss : 3.10  Grad : tensor([-0.0425,  0.2409])\n",
      "Epoch : 1481  Loss : 3.10  Grad : tensor([-0.0425,  0.2405])\n",
      "Epoch : 1482  Loss : 3.10  Grad : tensor([-0.0424,  0.2401])\n",
      "Epoch : 1483  Loss : 3.10  Grad : tensor([-0.0423,  0.2397])\n",
      "Epoch : 1484  Loss : 3.10  Grad : tensor([-0.0423,  0.2393])\n",
      "Epoch : 1485  Loss : 3.10  Grad : tensor([-0.0422,  0.2388])\n",
      "Epoch : 1486  Loss : 3.10  Grad : tensor([-0.0421,  0.2384])\n",
      "Epoch : 1487  Loss : 3.10  Grad : tensor([-0.0421,  0.2380])\n",
      "Epoch : 1488  Loss : 3.10  Grad : tensor([-0.0420,  0.2376])\n",
      "Epoch : 1489  Loss : 3.10  Grad : tensor([-0.0419,  0.2372])\n",
      "Epoch : 1490  Loss : 3.10  Grad : tensor([-0.0418,  0.2368])\n",
      "Epoch : 1491  Loss : 3.10  Grad : tensor([-0.0418,  0.2364])\n",
      "Epoch : 1492  Loss : 3.10  Grad : tensor([-0.0417,  0.2360])\n",
      "Epoch : 1493  Loss : 3.10  Grad : tensor([-0.0416,  0.2356])\n",
      "Epoch : 1494  Loss : 3.10  Grad : tensor([-0.0416,  0.2352])\n",
      "Epoch : 1495  Loss : 3.10  Grad : tensor([-0.0415,  0.2348])\n",
      "Epoch : 1496  Loss : 3.09  Grad : tensor([-0.0414,  0.2344])\n",
      "Epoch : 1497  Loss : 3.09  Grad : tensor([-0.0413,  0.2340])\n",
      "Epoch : 1498  Loss : 3.09  Grad : tensor([-0.0413,  0.2336])\n",
      "Epoch : 1499  Loss : 3.09  Grad : tensor([-0.0412,  0.2332])\n",
      "Epoch : 1500  Loss : 3.09  Grad : tensor([-0.0411,  0.2328])\n",
      "Epoch : 1501  Loss : 3.09  Grad : tensor([-0.0411,  0.2324])\n",
      "Epoch : 1502  Loss : 3.09  Grad : tensor([-0.0410,  0.2320])\n",
      "Epoch : 1503  Loss : 3.09  Grad : tensor([-0.0409,  0.2317])\n",
      "Epoch : 1504  Loss : 3.09  Grad : tensor([-0.0408,  0.2313])\n",
      "Epoch : 1505  Loss : 3.09  Grad : tensor([-0.0408,  0.2309])\n",
      "Epoch : 1506  Loss : 3.09  Grad : tensor([-0.0407,  0.2305])\n",
      "Epoch : 1507  Loss : 3.09  Grad : tensor([-0.0406,  0.2301])\n",
      "Epoch : 1508  Loss : 3.09  Grad : tensor([-0.0406,  0.2297])\n",
      "Epoch : 1509  Loss : 3.09  Grad : tensor([-0.0405,  0.2293])\n",
      "Epoch : 1510  Loss : 3.09  Grad : tensor([-0.0404,  0.2289])\n",
      "Epoch : 1511  Loss : 3.09  Grad : tensor([-0.0404,  0.2285])\n",
      "Epoch : 1512  Loss : 3.09  Grad : tensor([-0.0403,  0.2281])\n",
      "Epoch : 1513  Loss : 3.09  Grad : tensor([-0.0402,  0.2277])\n",
      "Epoch : 1514  Loss : 3.08  Grad : tensor([-0.0402,  0.2274])\n",
      "Epoch : 1515  Loss : 3.08  Grad : tensor([-0.0401,  0.2270])\n",
      "Epoch : 1516  Loss : 3.08  Grad : tensor([-0.0400,  0.2266])\n",
      "Epoch : 1517  Loss : 3.08  Grad : tensor([-0.0400,  0.2262])\n",
      "Epoch : 1518  Loss : 3.08  Grad : tensor([-0.0399,  0.2258])\n",
      "Epoch : 1519  Loss : 3.08  Grad : tensor([-0.0398,  0.2254])\n",
      "Epoch : 1520  Loss : 3.08  Grad : tensor([-0.0398,  0.2250])\n",
      "Epoch : 1521  Loss : 3.08  Grad : tensor([-0.0397,  0.2247])\n",
      "Epoch : 1522  Loss : 3.08  Grad : tensor([-0.0396,  0.2243])\n",
      "Epoch : 1523  Loss : 3.08  Grad : tensor([-0.0396,  0.2239])\n",
      "Epoch : 1524  Loss : 3.08  Grad : tensor([-0.0395,  0.2235])\n",
      "Epoch : 1525  Loss : 3.08  Grad : tensor([-0.0394,  0.2231])\n",
      "Epoch : 1526  Loss : 3.08  Grad : tensor([-0.0394,  0.2228])\n",
      "Epoch : 1527  Loss : 3.08  Grad : tensor([-0.0393,  0.2224])\n",
      "Epoch : 1528  Loss : 3.08  Grad : tensor([-0.0392,  0.2220])\n",
      "Epoch : 1529  Loss : 3.08  Grad : tensor([-0.0391,  0.2216])\n",
      "Epoch : 1530  Loss : 3.08  Grad : tensor([-0.0391,  0.2213])\n",
      "Epoch : 1531  Loss : 3.08  Grad : tensor([-0.0390,  0.2209])\n",
      "Epoch : 1532  Loss : 3.08  Grad : tensor([-0.0390,  0.2205])\n",
      "Epoch : 1533  Loss : 3.07  Grad : tensor([-0.0389,  0.2201])\n",
      "Epoch : 1534  Loss : 3.07  Grad : tensor([-0.0388,  0.2198])\n",
      "Epoch : 1535  Loss : 3.07  Grad : tensor([-0.0387,  0.2194])\n",
      "Epoch : 1536  Loss : 3.07  Grad : tensor([-0.0387,  0.2190])\n",
      "Epoch : 1537  Loss : 3.07  Grad : tensor([-0.0386,  0.2186])\n",
      "Epoch : 1538  Loss : 3.07  Grad : tensor([-0.0385,  0.2183])\n",
      "Epoch : 1539  Loss : 3.07  Grad : tensor([-0.0385,  0.2179])\n",
      "Epoch : 1540  Loss : 3.07  Grad : tensor([-0.0384,  0.2175])\n",
      "Epoch : 1541  Loss : 3.07  Grad : tensor([-0.0383,  0.2172])\n",
      "Epoch : 1542  Loss : 3.07  Grad : tensor([-0.0383,  0.2168])\n",
      "Epoch : 1543  Loss : 3.07  Grad : tensor([-0.0382,  0.2164])\n",
      "Epoch : 1544  Loss : 3.07  Grad : tensor([-0.0382,  0.2161])\n",
      "Epoch : 1545  Loss : 3.07  Grad : tensor([-0.0381,  0.2157])\n",
      "Epoch : 1546  Loss : 3.07  Grad : tensor([-0.0380,  0.2153])\n",
      "Epoch : 1547  Loss : 3.07  Grad : tensor([-0.0380,  0.2150])\n",
      "Epoch : 1548  Loss : 3.07  Grad : tensor([-0.0379,  0.2146])\n",
      "Epoch : 1549  Loss : 3.07  Grad : tensor([-0.0378,  0.2142])\n",
      "Epoch : 1550  Loss : 3.07  Grad : tensor([-0.0378,  0.2139])\n",
      "Epoch : 1551  Loss : 3.07  Grad : tensor([-0.0377,  0.2135])\n",
      "Epoch : 1552  Loss : 3.07  Grad : tensor([-0.0376,  0.2131])\n",
      "Epoch : 1553  Loss : 3.07  Grad : tensor([-0.0376,  0.2128])\n",
      "Epoch : 1554  Loss : 3.06  Grad : tensor([-0.0375,  0.2124])\n",
      "Epoch : 1555  Loss : 3.06  Grad : tensor([-0.0375,  0.2120])\n",
      "Epoch : 1556  Loss : 3.06  Grad : tensor([-0.0374,  0.2117])\n",
      "Epoch : 1557  Loss : 3.06  Grad : tensor([-0.0373,  0.2113])\n",
      "Epoch : 1558  Loss : 3.06  Grad : tensor([-0.0373,  0.2110])\n",
      "Epoch : 1559  Loss : 3.06  Grad : tensor([-0.0372,  0.2106])\n",
      "Epoch : 1560  Loss : 3.06  Grad : tensor([-0.0371,  0.2103])\n",
      "Epoch : 1561  Loss : 3.06  Grad : tensor([-0.0371,  0.2099])\n",
      "Epoch : 1562  Loss : 3.06  Grad : tensor([-0.0370,  0.2095])\n",
      "Epoch : 1563  Loss : 3.06  Grad : tensor([-0.0370,  0.2092])\n",
      "Epoch : 1564  Loss : 3.06  Grad : tensor([-0.0369,  0.2088])\n",
      "Epoch : 1565  Loss : 3.06  Grad : tensor([-0.0368,  0.2085])\n",
      "Epoch : 1566  Loss : 3.06  Grad : tensor([-0.0368,  0.2081])\n",
      "Epoch : 1567  Loss : 3.06  Grad : tensor([-0.0367,  0.2078])\n",
      "Epoch : 1568  Loss : 3.06  Grad : tensor([-0.0366,  0.2074])\n",
      "Epoch : 1569  Loss : 3.06  Grad : tensor([-0.0366,  0.2071])\n",
      "Epoch : 1570  Loss : 3.06  Grad : tensor([-0.0365,  0.2067])\n",
      "Epoch : 1571  Loss : 3.06  Grad : tensor([-0.0364,  0.2064])\n",
      "Epoch : 1572  Loss : 3.06  Grad : tensor([-0.0364,  0.2060])\n",
      "Epoch : 1573  Loss : 3.06  Grad : tensor([-0.0363,  0.2057])\n",
      "Epoch : 1574  Loss : 3.06  Grad : tensor([-0.0363,  0.2053])\n",
      "Epoch : 1575  Loss : 3.06  Grad : tensor([-0.0362,  0.2050])\n",
      "Epoch : 1576  Loss : 3.05  Grad : tensor([-0.0361,  0.2046])\n",
      "Epoch : 1577  Loss : 3.05  Grad : tensor([-0.0361,  0.2043])\n",
      "Epoch : 1578  Loss : 3.05  Grad : tensor([-0.0360,  0.2039])\n",
      "Epoch : 1579  Loss : 3.05  Grad : tensor([-0.0360,  0.2036])\n",
      "Epoch : 1580  Loss : 3.05  Grad : tensor([-0.0359,  0.2032])\n",
      "Epoch : 1581  Loss : 3.05  Grad : tensor([-0.0358,  0.2029])\n",
      "Epoch : 1582  Loss : 3.05  Grad : tensor([-0.0358,  0.2025])\n",
      "Epoch : 1583  Loss : 3.05  Grad : tensor([-0.0357,  0.2022])\n",
      "Epoch : 1584  Loss : 3.05  Grad : tensor([-0.0357,  0.2018])\n",
      "Epoch : 1585  Loss : 3.05  Grad : tensor([-0.0356,  0.2015])\n",
      "Epoch : 1586  Loss : 3.05  Grad : tensor([-0.0355,  0.2012])\n",
      "Epoch : 1587  Loss : 3.05  Grad : tensor([-0.0355,  0.2008])\n",
      "Epoch : 1588  Loss : 3.05  Grad : tensor([-0.0354,  0.2005])\n",
      "Epoch : 1589  Loss : 3.05  Grad : tensor([-0.0354,  0.2001])\n",
      "Epoch : 1590  Loss : 3.05  Grad : tensor([-0.0353,  0.1998])\n",
      "Epoch : 1591  Loss : 3.05  Grad : tensor([-0.0353,  0.1995])\n",
      "Epoch : 1592  Loss : 3.05  Grad : tensor([-0.0352,  0.1991])\n",
      "Epoch : 1593  Loss : 3.05  Grad : tensor([-0.0351,  0.1988])\n",
      "Epoch : 1594  Loss : 3.05  Grad : tensor([-0.0351,  0.1984])\n",
      "Epoch : 1595  Loss : 3.05  Grad : tensor([-0.0350,  0.1981])\n",
      "Epoch : 1596  Loss : 3.05  Grad : tensor([-0.0349,  0.1978])\n",
      "Epoch : 1597  Loss : 3.05  Grad : tensor([-0.0349,  0.1974])\n",
      "Epoch : 1598  Loss : 3.05  Grad : tensor([-0.0348,  0.1971])\n",
      "Epoch : 1599  Loss : 3.05  Grad : tensor([-0.0348,  0.1968])\n",
      "Epoch : 1600  Loss : 3.04  Grad : tensor([-0.0347,  0.1964])\n",
      "Epoch : 1601  Loss : 3.04  Grad : tensor([-0.0346,  0.1961])\n",
      "Epoch : 1602  Loss : 3.04  Grad : tensor([-0.0346,  0.1958])\n",
      "Epoch : 1603  Loss : 3.04  Grad : tensor([-0.0345,  0.1954])\n",
      "Epoch : 1604  Loss : 3.04  Grad : tensor([-0.0345,  0.1951])\n",
      "Epoch : 1605  Loss : 3.04  Grad : tensor([-0.0344,  0.1948])\n",
      "Epoch : 1606  Loss : 3.04  Grad : tensor([-0.0343,  0.1944])\n",
      "Epoch : 1607  Loss : 3.04  Grad : tensor([-0.0343,  0.1941])\n",
      "Epoch : 1608  Loss : 3.04  Grad : tensor([-0.0342,  0.1938])\n",
      "Epoch : 1609  Loss : 3.04  Grad : tensor([-0.0342,  0.1934])\n",
      "Epoch : 1610  Loss : 3.04  Grad : tensor([-0.0341,  0.1931])\n",
      "Epoch : 1611  Loss : 3.04  Grad : tensor([-0.0341,  0.1928])\n",
      "Epoch : 1612  Loss : 3.04  Grad : tensor([-0.0340,  0.1925])\n",
      "Epoch : 1613  Loss : 3.04  Grad : tensor([-0.0339,  0.1921])\n",
      "Epoch : 1614  Loss : 3.04  Grad : tensor([-0.0339,  0.1918])\n",
      "Epoch : 1615  Loss : 3.04  Grad : tensor([-0.0338,  0.1915])\n",
      "Epoch : 1616  Loss : 3.04  Grad : tensor([-0.0338,  0.1912])\n",
      "Epoch : 1617  Loss : 3.04  Grad : tensor([-0.0337,  0.1908])\n",
      "Epoch : 1618  Loss : 3.04  Grad : tensor([-0.0337,  0.1905])\n",
      "Epoch : 1619  Loss : 3.04  Grad : tensor([-0.0336,  0.1902])\n",
      "Epoch : 1620  Loss : 3.04  Grad : tensor([-0.0335,  0.1899])\n",
      "Epoch : 1621  Loss : 3.04  Grad : tensor([-0.0335,  0.1895])\n",
      "Epoch : 1622  Loss : 3.04  Grad : tensor([-0.0334,  0.1892])\n",
      "Epoch : 1623  Loss : 3.04  Grad : tensor([-0.0334,  0.1889])\n",
      "Epoch : 1624  Loss : 3.04  Grad : tensor([-0.0333,  0.1886])\n",
      "Epoch : 1625  Loss : 3.04  Grad : tensor([-0.0333,  0.1883])\n",
      "Epoch : 1626  Loss : 3.03  Grad : tensor([-0.0332,  0.1879])\n",
      "Epoch : 1627  Loss : 3.03  Grad : tensor([-0.0331,  0.1876])\n",
      "Epoch : 1628  Loss : 3.03  Grad : tensor([-0.0331,  0.1873])\n",
      "Epoch : 1629  Loss : 3.03  Grad : tensor([-0.0330,  0.1870])\n",
      "Epoch : 1630  Loss : 3.03  Grad : tensor([-0.0330,  0.1867])\n",
      "Epoch : 1631  Loss : 3.03  Grad : tensor([-0.0329,  0.1863])\n",
      "Epoch : 1632  Loss : 3.03  Grad : tensor([-0.0329,  0.1860])\n",
      "Epoch : 1633  Loss : 3.03  Grad : tensor([-0.0328,  0.1857])\n",
      "Epoch : 1634  Loss : 3.03  Grad : tensor([-0.0327,  0.1854])\n",
      "Epoch : 1635  Loss : 3.03  Grad : tensor([-0.0327,  0.1851])\n",
      "Epoch : 1636  Loss : 3.03  Grad : tensor([-0.0326,  0.1848])\n",
      "Epoch : 1637  Loss : 3.03  Grad : tensor([-0.0326,  0.1845])\n",
      "Epoch : 1638  Loss : 3.03  Grad : tensor([-0.0325,  0.1841])\n",
      "Epoch : 1639  Loss : 3.03  Grad : tensor([-0.0325,  0.1838])\n",
      "Epoch : 1640  Loss : 3.03  Grad : tensor([-0.0324,  0.1835])\n",
      "Epoch : 1641  Loss : 3.03  Grad : tensor([-0.0324,  0.1832])\n",
      "Epoch : 1642  Loss : 3.03  Grad : tensor([-0.0323,  0.1829])\n",
      "Epoch : 1643  Loss : 3.03  Grad : tensor([-0.0323,  0.1826])\n",
      "Epoch : 1644  Loss : 3.03  Grad : tensor([-0.0322,  0.1823])\n",
      "Epoch : 1645  Loss : 3.03  Grad : tensor([-0.0321,  0.1820])\n",
      "Epoch : 1646  Loss : 3.03  Grad : tensor([-0.0321,  0.1817])\n",
      "Epoch : 1647  Loss : 3.03  Grad : tensor([-0.0320,  0.1813])\n",
      "Epoch : 1648  Loss : 3.03  Grad : tensor([-0.0320,  0.1810])\n",
      "Epoch : 1649  Loss : 3.03  Grad : tensor([-0.0319,  0.1807])\n",
      "Epoch : 1650  Loss : 3.03  Grad : tensor([-0.0319,  0.1804])\n",
      "Epoch : 1651  Loss : 3.03  Grad : tensor([-0.0318,  0.1801])\n",
      "Epoch : 1652  Loss : 3.03  Grad : tensor([-0.0318,  0.1798])\n",
      "Epoch : 1653  Loss : 3.03  Grad : tensor([-0.0317,  0.1795])\n",
      "Epoch : 1654  Loss : 3.03  Grad : tensor([-0.0317,  0.1792])\n",
      "Epoch : 1655  Loss : 3.02  Grad : tensor([-0.0316,  0.1789])\n",
      "Epoch : 1656  Loss : 3.02  Grad : tensor([-0.0316,  0.1786])\n",
      "Epoch : 1657  Loss : 3.02  Grad : tensor([-0.0315,  0.1783])\n",
      "Epoch : 1658  Loss : 3.02  Grad : tensor([-0.0315,  0.1780])\n",
      "Epoch : 1659  Loss : 3.02  Grad : tensor([-0.0314,  0.1777])\n",
      "Epoch : 1660  Loss : 3.02  Grad : tensor([-0.0313,  0.1774])\n",
      "Epoch : 1661  Loss : 3.02  Grad : tensor([-0.0313,  0.1771])\n",
      "Epoch : 1662  Loss : 3.02  Grad : tensor([-0.0312,  0.1768])\n",
      "Epoch : 1663  Loss : 3.02  Grad : tensor([-0.0312,  0.1765])\n",
      "Epoch : 1664  Loss : 3.02  Grad : tensor([-0.0311,  0.1762])\n",
      "Epoch : 1665  Loss : 3.02  Grad : tensor([-0.0311,  0.1759])\n",
      "Epoch : 1666  Loss : 3.02  Grad : tensor([-0.0310,  0.1756])\n",
      "Epoch : 1667  Loss : 3.02  Grad : tensor([-0.0310,  0.1753])\n",
      "Epoch : 1668  Loss : 3.02  Grad : tensor([-0.0309,  0.1750])\n",
      "Epoch : 1669  Loss : 3.02  Grad : tensor([-0.0309,  0.1747])\n",
      "Epoch : 1670  Loss : 3.02  Grad : tensor([-0.0308,  0.1744])\n",
      "Epoch : 1671  Loss : 3.02  Grad : tensor([-0.0308,  0.1741])\n",
      "Epoch : 1672  Loss : 3.02  Grad : tensor([-0.0307,  0.1738])\n",
      "Epoch : 1673  Loss : 3.02  Grad : tensor([-0.0307,  0.1735])\n",
      "Epoch : 1674  Loss : 3.02  Grad : tensor([-0.0306,  0.1732])\n",
      "Epoch : 1675  Loss : 3.02  Grad : tensor([-0.0305,  0.1729])\n",
      "Epoch : 1676  Loss : 3.02  Grad : tensor([-0.0305,  0.1726])\n",
      "Epoch : 1677  Loss : 3.02  Grad : tensor([-0.0304,  0.1723])\n",
      "Epoch : 1678  Loss : 3.02  Grad : tensor([-0.0304,  0.1720])\n",
      "Epoch : 1679  Loss : 3.02  Grad : tensor([-0.0303,  0.1717])\n",
      "Epoch : 1680  Loss : 3.02  Grad : tensor([-0.0303,  0.1715])\n",
      "Epoch : 1681  Loss : 3.02  Grad : tensor([-0.0302,  0.1712])\n",
      "Epoch : 1682  Loss : 3.02  Grad : tensor([-0.0302,  0.1709])\n",
      "Epoch : 1683  Loss : 3.02  Grad : tensor([-0.0301,  0.1706])\n",
      "Epoch : 1684  Loss : 3.02  Grad : tensor([-0.0301,  0.1703])\n",
      "Epoch : 1685  Loss : 3.02  Grad : tensor([-0.0300,  0.1700])\n",
      "Epoch : 1686  Loss : 3.02  Grad : tensor([-0.0300,  0.1697])\n",
      "Epoch : 1687  Loss : 3.01  Grad : tensor([-0.0299,  0.1694])\n",
      "Epoch : 1688  Loss : 3.01  Grad : tensor([-0.0299,  0.1691])\n",
      "Epoch : 1689  Loss : 3.01  Grad : tensor([-0.0298,  0.1688])\n",
      "Epoch : 1690  Loss : 3.01  Grad : tensor([-0.0298,  0.1686])\n",
      "Epoch : 1691  Loss : 3.01  Grad : tensor([-0.0297,  0.1683])\n",
      "Epoch : 1692  Loss : 3.01  Grad : tensor([-0.0297,  0.1680])\n",
      "Epoch : 1693  Loss : 3.01  Grad : tensor([-0.0296,  0.1677])\n",
      "Epoch : 1694  Loss : 3.01  Grad : tensor([-0.0296,  0.1674])\n",
      "Epoch : 1695  Loss : 3.01  Grad : tensor([-0.0295,  0.1671])\n",
      "Epoch : 1696  Loss : 3.01  Grad : tensor([-0.0295,  0.1668])\n",
      "Epoch : 1697  Loss : 3.01  Grad : tensor([-0.0294,  0.1666])\n",
      "Epoch : 1698  Loss : 3.01  Grad : tensor([-0.0294,  0.1663])\n",
      "Epoch : 1699  Loss : 3.01  Grad : tensor([-0.0293,  0.1660])\n",
      "Epoch : 1700  Loss : 3.01  Grad : tensor([-0.0293,  0.1657])\n",
      "Epoch : 1701  Loss : 3.01  Grad : tensor([-0.0292,  0.1654])\n",
      "Epoch : 1702  Loss : 3.01  Grad : tensor([-0.0292,  0.1652])\n",
      "Epoch : 1703  Loss : 3.01  Grad : tensor([-0.0291,  0.1649])\n",
      "Epoch : 1704  Loss : 3.01  Grad : tensor([-0.0291,  0.1646])\n",
      "Epoch : 1705  Loss : 3.01  Grad : tensor([-0.0290,  0.1643])\n",
      "Epoch : 1706  Loss : 3.01  Grad : tensor([-0.0290,  0.1640])\n",
      "Epoch : 1707  Loss : 3.01  Grad : tensor([-0.0289,  0.1638])\n",
      "Epoch : 1708  Loss : 3.01  Grad : tensor([-0.0289,  0.1635])\n",
      "Epoch : 1709  Loss : 3.01  Grad : tensor([-0.0288,  0.1632])\n",
      "Epoch : 1710  Loss : 3.01  Grad : tensor([-0.0288,  0.1629])\n",
      "Epoch : 1711  Loss : 3.01  Grad : tensor([-0.0287,  0.1626])\n",
      "Epoch : 1712  Loss : 3.01  Grad : tensor([-0.0287,  0.1624])\n",
      "Epoch : 1713  Loss : 3.01  Grad : tensor([-0.0286,  0.1621])\n",
      "Epoch : 1714  Loss : 3.01  Grad : tensor([-0.0286,  0.1618])\n",
      "Epoch : 1715  Loss : 3.01  Grad : tensor([-0.0285,  0.1615])\n",
      "Epoch : 1716  Loss : 3.01  Grad : tensor([-0.0285,  0.1613])\n",
      "Epoch : 1717  Loss : 3.01  Grad : tensor([-0.0284,  0.1610])\n",
      "Epoch : 1718  Loss : 3.01  Grad : tensor([-0.0284,  0.1607])\n",
      "Epoch : 1719  Loss : 3.01  Grad : tensor([-0.0284,  0.1604])\n",
      "Epoch : 1720  Loss : 3.01  Grad : tensor([-0.0283,  0.1602])\n",
      "Epoch : 1721  Loss : 3.01  Grad : tensor([-0.0283,  0.1599])\n",
      "Epoch : 1722  Loss : 3.00  Grad : tensor([-0.0282,  0.1596])\n",
      "Epoch : 1723  Loss : 3.00  Grad : tensor([-0.0281,  0.1594])\n",
      "Epoch : 1724  Loss : 3.00  Grad : tensor([-0.0281,  0.1591])\n",
      "Epoch : 1725  Loss : 3.00  Grad : tensor([-0.0280,  0.1588])\n",
      "Epoch : 1726  Loss : 3.00  Grad : tensor([-0.0280,  0.1586])\n",
      "Epoch : 1727  Loss : 3.00  Grad : tensor([-0.0280,  0.1583])\n",
      "Epoch : 1728  Loss : 3.00  Grad : tensor([-0.0279,  0.1580])\n",
      "Epoch : 1729  Loss : 3.00  Grad : tensor([-0.0279,  0.1577])\n",
      "Epoch : 1730  Loss : 3.00  Grad : tensor([-0.0278,  0.1575])\n",
      "Epoch : 1731  Loss : 3.00  Grad : tensor([-0.0278,  0.1572])\n",
      "Epoch : 1732  Loss : 3.00  Grad : tensor([-0.0277,  0.1569])\n",
      "Epoch : 1733  Loss : 3.00  Grad : tensor([-0.0277,  0.1567])\n",
      "Epoch : 1734  Loss : 3.00  Grad : tensor([-0.0276,  0.1564])\n",
      "Epoch : 1735  Loss : 3.00  Grad : tensor([-0.0276,  0.1561])\n",
      "Epoch : 1736  Loss : 3.00  Grad : tensor([-0.0275,  0.1559])\n",
      "Epoch : 1737  Loss : 3.00  Grad : tensor([-0.0275,  0.1556])\n",
      "Epoch : 1738  Loss : 3.00  Grad : tensor([-0.0274,  0.1553])\n",
      "Epoch : 1739  Loss : 3.00  Grad : tensor([-0.0274,  0.1551])\n",
      "Epoch : 1740  Loss : 3.00  Grad : tensor([-0.0273,  0.1548])\n",
      "Epoch : 1741  Loss : 3.00  Grad : tensor([-0.0273,  0.1546])\n",
      "Epoch : 1742  Loss : 3.00  Grad : tensor([-0.0273,  0.1543])\n",
      "Epoch : 1743  Loss : 3.00  Grad : tensor([-0.0272,  0.1540])\n",
      "Epoch : 1744  Loss : 3.00  Grad : tensor([-0.0272,  0.1538])\n",
      "Epoch : 1745  Loss : 3.00  Grad : tensor([-0.0271,  0.1535])\n",
      "Epoch : 1746  Loss : 3.00  Grad : tensor([-0.0271,  0.1533])\n",
      "Epoch : 1747  Loss : 3.00  Grad : tensor([-0.0270,  0.1530])\n",
      "Epoch : 1748  Loss : 3.00  Grad : tensor([-0.0270,  0.1527])\n",
      "Epoch : 1749  Loss : 3.00  Grad : tensor([-0.0269,  0.1525])\n",
      "Epoch : 1750  Loss : 3.00  Grad : tensor([-0.0269,  0.1522])\n",
      "Epoch : 1751  Loss : 3.00  Grad : tensor([-0.0268,  0.1520])\n",
      "Epoch : 1752  Loss : 3.00  Grad : tensor([-0.0268,  0.1517])\n",
      "Epoch : 1753  Loss : 3.00  Grad : tensor([-0.0267,  0.1514])\n",
      "Epoch : 1754  Loss : 3.00  Grad : tensor([-0.0267,  0.1512])\n",
      "Epoch : 1755  Loss : 3.00  Grad : tensor([-0.0266,  0.1509])\n",
      "Epoch : 1756  Loss : 3.00  Grad : tensor([-0.0266,  0.1507])\n",
      "Epoch : 1757  Loss : 3.00  Grad : tensor([-0.0266,  0.1504])\n",
      "Epoch : 1758  Loss : 3.00  Grad : tensor([-0.0265,  0.1502])\n",
      "Epoch : 1759  Loss : 3.00  Grad : tensor([-0.0265,  0.1499])\n",
      "Epoch : 1760  Loss : 3.00  Grad : tensor([-0.0264,  0.1496])\n",
      "Epoch : 1761  Loss : 3.00  Grad : tensor([-0.0264,  0.1494])\n",
      "Epoch : 1762  Loss : 3.00  Grad : tensor([-0.0263,  0.1491])\n",
      "Epoch : 1763  Loss : 2.99  Grad : tensor([-0.0263,  0.1489])\n",
      "Epoch : 1764  Loss : 2.99  Grad : tensor([-0.0263,  0.1486])\n",
      "Epoch : 1765  Loss : 2.99  Grad : tensor([-0.0262,  0.1484])\n",
      "Epoch : 1766  Loss : 2.99  Grad : tensor([-0.0262,  0.1481])\n",
      "Epoch : 1767  Loss : 2.99  Grad : tensor([-0.0261,  0.1479])\n",
      "Epoch : 1768  Loss : 2.99  Grad : tensor([-0.0261,  0.1476])\n",
      "Epoch : 1769  Loss : 2.99  Grad : tensor([-0.0260,  0.1474])\n",
      "Epoch : 1770  Loss : 2.99  Grad : tensor([-0.0260,  0.1471])\n",
      "Epoch : 1771  Loss : 2.99  Grad : tensor([-0.0260,  0.1469])\n",
      "Epoch : 1772  Loss : 2.99  Grad : tensor([-0.0259,  0.1466])\n",
      "Epoch : 1773  Loss : 2.99  Grad : tensor([-0.0259,  0.1464])\n",
      "Epoch : 1774  Loss : 2.99  Grad : tensor([-0.0258,  0.1461])\n",
      "Epoch : 1775  Loss : 2.99  Grad : tensor([-0.0258,  0.1459])\n",
      "Epoch : 1776  Loss : 2.99  Grad : tensor([-0.0257,  0.1456])\n",
      "Epoch : 1777  Loss : 2.99  Grad : tensor([-0.0257,  0.1454])\n",
      "Epoch : 1778  Loss : 2.99  Grad : tensor([-0.0256,  0.1451])\n",
      "Epoch : 1779  Loss : 2.99  Grad : tensor([-0.0256,  0.1449])\n",
      "Epoch : 1780  Loss : 2.99  Grad : tensor([-0.0256,  0.1446])\n",
      "Epoch : 1781  Loss : 2.99  Grad : tensor([-0.0255,  0.1444])\n",
      "Epoch : 1782  Loss : 2.99  Grad : tensor([-0.0255,  0.1442])\n",
      "Epoch : 1783  Loss : 2.99  Grad : tensor([-0.0254,  0.1439])\n",
      "Epoch : 1784  Loss : 2.99  Grad : tensor([-0.0254,  0.1437])\n",
      "Epoch : 1785  Loss : 2.99  Grad : tensor([-0.0253,  0.1434])\n",
      "Epoch : 1786  Loss : 2.99  Grad : tensor([-0.0253,  0.1432])\n",
      "Epoch : 1787  Loss : 2.99  Grad : tensor([-0.0252,  0.1429])\n",
      "Epoch : 1788  Loss : 2.99  Grad : tensor([-0.0252,  0.1427])\n",
      "Epoch : 1789  Loss : 2.99  Grad : tensor([-0.0252,  0.1424])\n",
      "Epoch : 1790  Loss : 2.99  Grad : tensor([-0.0251,  0.1422])\n",
      "Epoch : 1791  Loss : 2.99  Grad : tensor([-0.0251,  0.1420])\n",
      "Epoch : 1792  Loss : 2.99  Grad : tensor([-0.0250,  0.1417])\n",
      "Epoch : 1793  Loss : 2.99  Grad : tensor([-0.0250,  0.1415])\n",
      "Epoch : 1794  Loss : 2.99  Grad : tensor([-0.0249,  0.1412])\n",
      "Epoch : 1795  Loss : 2.99  Grad : tensor([-0.0249,  0.1410])\n",
      "Epoch : 1796  Loss : 2.99  Grad : tensor([-0.0249,  0.1408])\n",
      "Epoch : 1797  Loss : 2.99  Grad : tensor([-0.0248,  0.1405])\n",
      "Epoch : 1798  Loss : 2.99  Grad : tensor([-0.0248,  0.1403])\n",
      "Epoch : 1799  Loss : 2.99  Grad : tensor([-0.0247,  0.1400])\n",
      "Epoch : 1800  Loss : 2.99  Grad : tensor([-0.0247,  0.1398])\n",
      "Epoch : 1801  Loss : 2.99  Grad : tensor([-0.0246,  0.1396])\n",
      "Epoch : 1802  Loss : 2.99  Grad : tensor([-0.0246,  0.1393])\n",
      "Epoch : 1803  Loss : 2.99  Grad : tensor([-0.0246,  0.1391])\n",
      "Epoch : 1804  Loss : 2.99  Grad : tensor([-0.0245,  0.1389])\n",
      "Epoch : 1805  Loss : 2.99  Grad : tensor([-0.0245,  0.1386])\n",
      "Epoch : 1806  Loss : 2.99  Grad : tensor([-0.0245,  0.1384])\n",
      "Epoch : 1807  Loss : 2.99  Grad : tensor([-0.0244,  0.1382])\n",
      "Epoch : 1808  Loss : 2.99  Grad : tensor([-0.0244,  0.1379])\n",
      "Epoch : 1809  Loss : 2.99  Grad : tensor([-0.0243,  0.1377])\n",
      "Epoch : 1810  Loss : 2.98  Grad : tensor([-0.0243,  0.1374])\n",
      "Epoch : 1811  Loss : 2.98  Grad : tensor([-0.0243,  0.1372])\n",
      "Epoch : 1812  Loss : 2.98  Grad : tensor([-0.0242,  0.1370])\n",
      "Epoch : 1813  Loss : 2.98  Grad : tensor([-0.0242,  0.1368])\n",
      "Epoch : 1814  Loss : 2.98  Grad : tensor([-0.0241,  0.1365])\n",
      "Epoch : 1815  Loss : 2.98  Grad : tensor([-0.0241,  0.1363])\n",
      "Epoch : 1816  Loss : 2.98  Grad : tensor([-0.0240,  0.1361])\n",
      "Epoch : 1817  Loss : 2.98  Grad : tensor([-0.0240,  0.1358])\n",
      "Epoch : 1818  Loss : 2.98  Grad : tensor([-0.0239,  0.1356])\n",
      "Epoch : 1819  Loss : 2.98  Grad : tensor([-0.0239,  0.1354])\n",
      "Epoch : 1820  Loss : 2.98  Grad : tensor([-0.0239,  0.1351])\n",
      "Epoch : 1821  Loss : 2.98  Grad : tensor([-0.0238,  0.1349])\n",
      "Epoch : 1822  Loss : 2.98  Grad : tensor([-0.0238,  0.1347])\n",
      "Epoch : 1823  Loss : 2.98  Grad : tensor([-0.0237,  0.1344])\n",
      "Epoch : 1824  Loss : 2.98  Grad : tensor([-0.0237,  0.1342])\n",
      "Epoch : 1825  Loss : 2.98  Grad : tensor([-0.0237,  0.1340])\n",
      "Epoch : 1826  Loss : 2.98  Grad : tensor([-0.0236,  0.1338])\n",
      "Epoch : 1827  Loss : 2.98  Grad : tensor([-0.0236,  0.1335])\n",
      "Epoch : 1828  Loss : 2.98  Grad : tensor([-0.0236,  0.1333])\n",
      "Epoch : 1829  Loss : 2.98  Grad : tensor([-0.0235,  0.1331])\n",
      "Epoch : 1830  Loss : 2.98  Grad : tensor([-0.0235,  0.1329])\n",
      "Epoch : 1831  Loss : 2.98  Grad : tensor([-0.0235,  0.1326])\n",
      "Epoch : 1832  Loss : 2.98  Grad : tensor([-0.0234,  0.1324])\n",
      "Epoch : 1833  Loss : 2.98  Grad : tensor([-0.0234,  0.1322])\n",
      "Epoch : 1834  Loss : 2.98  Grad : tensor([-0.0233,  0.1320])\n",
      "Epoch : 1835  Loss : 2.98  Grad : tensor([-0.0233,  0.1317])\n",
      "Epoch : 1836  Loss : 2.98  Grad : tensor([-0.0232,  0.1315])\n",
      "Epoch : 1837  Loss : 2.98  Grad : tensor([-0.0232,  0.1313])\n",
      "Epoch : 1838  Loss : 2.98  Grad : tensor([-0.0232,  0.1311])\n",
      "Epoch : 1839  Loss : 2.98  Grad : tensor([-0.0231,  0.1308])\n",
      "Epoch : 1840  Loss : 2.98  Grad : tensor([-0.0231,  0.1306])\n",
      "Epoch : 1841  Loss : 2.98  Grad : tensor([-0.0230,  0.1304])\n",
      "Epoch : 1842  Loss : 2.98  Grad : tensor([-0.0230,  0.1302])\n",
      "Epoch : 1843  Loss : 2.98  Grad : tensor([-0.0229,  0.1300])\n",
      "Epoch : 1844  Loss : 2.98  Grad : tensor([-0.0229,  0.1297])\n",
      "Epoch : 1845  Loss : 2.98  Grad : tensor([-0.0229,  0.1295])\n",
      "Epoch : 1846  Loss : 2.98  Grad : tensor([-0.0228,  0.1293])\n",
      "Epoch : 1847  Loss : 2.98  Grad : tensor([-0.0228,  0.1291])\n",
      "Epoch : 1848  Loss : 2.98  Grad : tensor([-0.0228,  0.1288])\n",
      "Epoch : 1849  Loss : 2.98  Grad : tensor([-0.0227,  0.1286])\n",
      "Epoch : 1850  Loss : 2.98  Grad : tensor([-0.0227,  0.1284])\n",
      "Epoch : 1851  Loss : 2.98  Grad : tensor([-0.0227,  0.1282])\n",
      "Epoch : 1852  Loss : 2.98  Grad : tensor([-0.0226,  0.1280])\n",
      "Epoch : 1853  Loss : 2.98  Grad : tensor([-0.0226,  0.1278])\n",
      "Epoch : 1854  Loss : 2.98  Grad : tensor([-0.0225,  0.1275])\n",
      "Epoch : 1855  Loss : 2.98  Grad : tensor([-0.0225,  0.1273])\n",
      "Epoch : 1856  Loss : 2.98  Grad : tensor([-0.0225,  0.1271])\n",
      "Epoch : 1857  Loss : 2.98  Grad : tensor([-0.0224,  0.1269])\n",
      "Epoch : 1858  Loss : 2.98  Grad : tensor([-0.0224,  0.1267])\n",
      "Epoch : 1859  Loss : 2.98  Grad : tensor([-0.0223,  0.1265])\n",
      "Epoch : 1860  Loss : 2.98  Grad : tensor([-0.0223,  0.1263])\n",
      "Epoch : 1861  Loss : 2.98  Grad : tensor([-0.0223,  0.1260])\n",
      "Epoch : 1862  Loss : 2.98  Grad : tensor([-0.0222,  0.1258])\n",
      "Epoch : 1863  Loss : 2.98  Grad : tensor([-0.0222,  0.1256])\n",
      "Epoch : 1864  Loss : 2.98  Grad : tensor([-0.0222,  0.1254])\n",
      "Epoch : 1865  Loss : 2.98  Grad : tensor([-0.0221,  0.1252])\n",
      "Epoch : 1866  Loss : 2.98  Grad : tensor([-0.0221,  0.1250])\n",
      "Epoch : 1867  Loss : 2.97  Grad : tensor([-0.0220,  0.1248])\n",
      "Epoch : 1868  Loss : 2.97  Grad : tensor([-0.0220,  0.1245])\n",
      "Epoch : 1869  Loss : 2.97  Grad : tensor([-0.0220,  0.1243])\n",
      "Epoch : 1870  Loss : 2.97  Grad : tensor([-0.0219,  0.1241])\n",
      "Epoch : 1871  Loss : 2.97  Grad : tensor([-0.0219,  0.1239])\n",
      "Epoch : 1872  Loss : 2.97  Grad : tensor([-0.0219,  0.1237])\n",
      "Epoch : 1873  Loss : 2.97  Grad : tensor([-0.0218,  0.1235])\n",
      "Epoch : 1874  Loss : 2.97  Grad : tensor([-0.0218,  0.1233])\n",
      "Epoch : 1875  Loss : 2.97  Grad : tensor([-0.0217,  0.1231])\n",
      "Epoch : 1876  Loss : 2.97  Grad : tensor([-0.0217,  0.1229])\n",
      "Epoch : 1877  Loss : 2.97  Grad : tensor([-0.0217,  0.1227])\n",
      "Epoch : 1878  Loss : 2.97  Grad : tensor([-0.0216,  0.1224])\n",
      "Epoch : 1879  Loss : 2.97  Grad : tensor([-0.0216,  0.1222])\n",
      "Epoch : 1880  Loss : 2.97  Grad : tensor([-0.0215,  0.1220])\n",
      "Epoch : 1881  Loss : 2.97  Grad : tensor([-0.0215,  0.1218])\n",
      "Epoch : 1882  Loss : 2.97  Grad : tensor([-0.0215,  0.1216])\n",
      "Epoch : 1883  Loss : 2.97  Grad : tensor([-0.0214,  0.1214])\n",
      "Epoch : 1884  Loss : 2.97  Grad : tensor([-0.0214,  0.1212])\n",
      "Epoch : 1885  Loss : 2.97  Grad : tensor([-0.0214,  0.1210])\n",
      "Epoch : 1886  Loss : 2.97  Grad : tensor([-0.0213,  0.1208])\n",
      "Epoch : 1887  Loss : 2.97  Grad : tensor([-0.0213,  0.1206])\n",
      "Epoch : 1888  Loss : 2.97  Grad : tensor([-0.0213,  0.1204])\n",
      "Epoch : 1889  Loss : 2.97  Grad : tensor([-0.0212,  0.1202])\n",
      "Epoch : 1890  Loss : 2.97  Grad : tensor([-0.0212,  0.1200])\n",
      "Epoch : 1891  Loss : 2.97  Grad : tensor([-0.0212,  0.1198])\n",
      "Epoch : 1892  Loss : 2.97  Grad : tensor([-0.0211,  0.1196])\n",
      "Epoch : 1893  Loss : 2.97  Grad : tensor([-0.0211,  0.1194])\n",
      "Epoch : 1894  Loss : 2.97  Grad : tensor([-0.0211,  0.1192])\n",
      "Epoch : 1895  Loss : 2.97  Grad : tensor([-0.0210,  0.1190])\n",
      "Epoch : 1896  Loss : 2.97  Grad : tensor([-0.0210,  0.1188])\n",
      "Epoch : 1897  Loss : 2.97  Grad : tensor([-0.0209,  0.1186])\n",
      "Epoch : 1898  Loss : 2.97  Grad : tensor([-0.0209,  0.1183])\n",
      "Epoch : 1899  Loss : 2.97  Grad : tensor([-0.0209,  0.1182])\n",
      "Epoch : 1900  Loss : 2.97  Grad : tensor([-0.0208,  0.1180])\n",
      "Epoch : 1901  Loss : 2.97  Grad : tensor([-0.0208,  0.1178])\n",
      "Epoch : 1902  Loss : 2.97  Grad : tensor([-0.0208,  0.1175])\n",
      "Epoch : 1903  Loss : 2.97  Grad : tensor([-0.0207,  0.1173])\n",
      "Epoch : 1904  Loss : 2.97  Grad : tensor([-0.0207,  0.1172])\n",
      "Epoch : 1905  Loss : 2.97  Grad : tensor([-0.0206,  0.1170])\n",
      "Epoch : 1906  Loss : 2.97  Grad : tensor([-0.0206,  0.1168])\n",
      "Epoch : 1907  Loss : 2.97  Grad : tensor([-0.0206,  0.1166])\n",
      "Epoch : 1908  Loss : 2.97  Grad : tensor([-0.0205,  0.1164])\n",
      "Epoch : 1909  Loss : 2.97  Grad : tensor([-0.0205,  0.1162])\n",
      "Epoch : 1910  Loss : 2.97  Grad : tensor([-0.0205,  0.1160])\n",
      "Epoch : 1911  Loss : 2.97  Grad : tensor([-0.0204,  0.1158])\n",
      "Epoch : 1912  Loss : 2.97  Grad : tensor([-0.0204,  0.1156])\n",
      "Epoch : 1913  Loss : 2.97  Grad : tensor([-0.0204,  0.1154])\n",
      "Epoch : 1914  Loss : 2.97  Grad : tensor([-0.0204,  0.1152])\n",
      "Epoch : 1915  Loss : 2.97  Grad : tensor([-0.0203,  0.1150])\n",
      "Epoch : 1916  Loss : 2.97  Grad : tensor([-0.0203,  0.1148])\n",
      "Epoch : 1917  Loss : 2.97  Grad : tensor([-0.0202,  0.1146])\n",
      "Epoch : 1918  Loss : 2.97  Grad : tensor([-0.0202,  0.1144])\n",
      "Epoch : 1919  Loss : 2.97  Grad : tensor([-0.0202,  0.1142])\n",
      "Epoch : 1920  Loss : 2.97  Grad : tensor([-0.0202,  0.1140])\n",
      "Epoch : 1921  Loss : 2.97  Grad : tensor([-0.0201,  0.1138])\n",
      "Epoch : 1922  Loss : 2.97  Grad : tensor([-0.0201,  0.1136])\n",
      "Epoch : 1923  Loss : 2.97  Grad : tensor([-0.0200,  0.1134])\n",
      "Epoch : 1924  Loss : 2.97  Grad : tensor([-0.0200,  0.1132])\n",
      "Epoch : 1925  Loss : 2.97  Grad : tensor([-0.0200,  0.1130])\n",
      "Epoch : 1926  Loss : 2.97  Grad : tensor([-0.0199,  0.1128])\n",
      "Epoch : 1927  Loss : 2.97  Grad : tensor([-0.0199,  0.1127])\n",
      "Epoch : 1928  Loss : 2.97  Grad : tensor([-0.0199,  0.1125])\n",
      "Epoch : 1929  Loss : 2.97  Grad : tensor([-0.0198,  0.1123])\n",
      "Epoch : 1930  Loss : 2.97  Grad : tensor([-0.0198,  0.1121])\n",
      "Epoch : 1931  Loss : 2.97  Grad : tensor([-0.0198,  0.1119])\n",
      "Epoch : 1932  Loss : 2.97  Grad : tensor([-0.0197,  0.1117])\n",
      "Epoch : 1933  Loss : 2.97  Grad : tensor([-0.0197,  0.1115])\n",
      "Epoch : 1934  Loss : 2.97  Grad : tensor([-0.0197,  0.1113])\n",
      "Epoch : 1935  Loss : 2.97  Grad : tensor([-0.0196,  0.1111])\n",
      "Epoch : 1936  Loss : 2.97  Grad : tensor([-0.0196,  0.1109])\n",
      "Epoch : 1937  Loss : 2.96  Grad : tensor([-0.0196,  0.1108])\n",
      "Epoch : 1938  Loss : 2.96  Grad : tensor([-0.0195,  0.1106])\n",
      "Epoch : 1939  Loss : 2.96  Grad : tensor([-0.0195,  0.1104])\n",
      "Epoch : 1940  Loss : 2.96  Grad : tensor([-0.0195,  0.1102])\n",
      "Epoch : 1941  Loss : 2.96  Grad : tensor([-0.0195,  0.1100])\n",
      "Epoch : 1942  Loss : 2.96  Grad : tensor([-0.0194,  0.1098])\n",
      "Epoch : 1943  Loss : 2.96  Grad : tensor([-0.0194,  0.1096])\n",
      "Epoch : 1944  Loss : 2.96  Grad : tensor([-0.0194,  0.1094])\n",
      "Epoch : 1945  Loss : 2.96  Grad : tensor([-0.0193,  0.1093])\n",
      "Epoch : 1946  Loss : 2.96  Grad : tensor([-0.0193,  0.1091])\n",
      "Epoch : 1947  Loss : 2.96  Grad : tensor([-0.0192,  0.1089])\n",
      "Epoch : 1948  Loss : 2.96  Grad : tensor([-0.0192,  0.1087])\n",
      "Epoch : 1949  Loss : 2.96  Grad : tensor([-0.0192,  0.1085])\n",
      "Epoch : 1950  Loss : 2.96  Grad : tensor([-0.0191,  0.1083])\n",
      "Epoch : 1951  Loss : 2.96  Grad : tensor([-0.0191,  0.1081])\n",
      "Epoch : 1952  Loss : 2.96  Grad : tensor([-0.0191,  0.1080])\n",
      "Epoch : 1953  Loss : 2.96  Grad : tensor([-0.0190,  0.1078])\n",
      "Epoch : 1954  Loss : 2.96  Grad : tensor([-0.0190,  0.1076])\n",
      "Epoch : 1955  Loss : 2.96  Grad : tensor([-0.0190,  0.1074])\n",
      "Epoch : 1956  Loss : 2.96  Grad : tensor([-0.0189,  0.1072])\n",
      "Epoch : 1957  Loss : 2.96  Grad : tensor([-0.0189,  0.1071])\n",
      "Epoch : 1958  Loss : 2.96  Grad : tensor([-0.0189,  0.1069])\n",
      "Epoch : 1959  Loss : 2.96  Grad : tensor([-0.0188,  0.1067])\n",
      "Epoch : 1960  Loss : 2.96  Grad : tensor([-0.0188,  0.1065])\n",
      "Epoch : 1961  Loss : 2.96  Grad : tensor([-0.0188,  0.1063])\n",
      "Epoch : 1962  Loss : 2.96  Grad : tensor([-0.0187,  0.1062])\n",
      "Epoch : 1963  Loss : 2.96  Grad : tensor([-0.0187,  0.1060])\n",
      "Epoch : 1964  Loss : 2.96  Grad : tensor([-0.0187,  0.1058])\n",
      "Epoch : 1965  Loss : 2.96  Grad : tensor([-0.0187,  0.1056])\n",
      "Epoch : 1966  Loss : 2.96  Grad : tensor([-0.0186,  0.1054])\n",
      "Epoch : 1967  Loss : 2.96  Grad : tensor([-0.0186,  0.1052])\n",
      "Epoch : 1968  Loss : 2.96  Grad : tensor([-0.0186,  0.1051])\n",
      "Epoch : 1969  Loss : 2.96  Grad : tensor([-0.0185,  0.1049])\n",
      "Epoch : 1970  Loss : 2.96  Grad : tensor([-0.0185,  0.1047])\n",
      "Epoch : 1971  Loss : 2.96  Grad : tensor([-0.0185,  0.1045])\n",
      "Epoch : 1972  Loss : 2.96  Grad : tensor([-0.0184,  0.1044])\n",
      "Epoch : 1973  Loss : 2.96  Grad : tensor([-0.0184,  0.1042])\n",
      "Epoch : 1974  Loss : 2.96  Grad : tensor([-0.0184,  0.1040])\n",
      "Epoch : 1975  Loss : 2.96  Grad : tensor([-0.0183,  0.1038])\n",
      "Epoch : 1976  Loss : 2.96  Grad : tensor([-0.0183,  0.1037])\n",
      "Epoch : 1977  Loss : 2.96  Grad : tensor([-0.0183,  0.1035])\n",
      "Epoch : 1978  Loss : 2.96  Grad : tensor([-0.0182,  0.1033])\n",
      "Epoch : 1979  Loss : 2.96  Grad : tensor([-0.0182,  0.1031])\n",
      "Epoch : 1980  Loss : 2.96  Grad : tensor([-0.0182,  0.1029])\n",
      "Epoch : 1981  Loss : 2.96  Grad : tensor([-0.0182,  0.1028])\n",
      "Epoch : 1982  Loss : 2.96  Grad : tensor([-0.0181,  0.1026])\n",
      "Epoch : 1983  Loss : 2.96  Grad : tensor([-0.0181,  0.1024])\n",
      "Epoch : 1984  Loss : 2.96  Grad : tensor([-0.0181,  0.1022])\n",
      "Epoch : 1985  Loss : 2.96  Grad : tensor([-0.0180,  0.1021])\n",
      "Epoch : 1986  Loss : 2.96  Grad : tensor([-0.0180,  0.1019])\n",
      "Epoch : 1987  Loss : 2.96  Grad : tensor([-0.0180,  0.1017])\n",
      "Epoch : 1988  Loss : 2.96  Grad : tensor([-0.0179,  0.1016])\n",
      "Epoch : 1989  Loss : 2.96  Grad : tensor([-0.0179,  0.1014])\n",
      "Epoch : 1990  Loss : 2.96  Grad : tensor([-0.0179,  0.1012])\n",
      "Epoch : 1991  Loss : 2.96  Grad : tensor([-0.0179,  0.1010])\n",
      "Epoch : 1992  Loss : 2.96  Grad : tensor([-0.0178,  0.1009])\n",
      "Epoch : 1993  Loss : 2.96  Grad : tensor([-0.0178,  0.1007])\n",
      "Epoch : 1994  Loss : 2.96  Grad : tensor([-0.0178,  0.1005])\n",
      "Epoch : 1995  Loss : 2.96  Grad : tensor([-0.0177,  0.1004])\n",
      "Epoch : 1996  Loss : 2.96  Grad : tensor([-0.0177,  0.1002])\n",
      "Epoch : 1997  Loss : 2.96  Grad : tensor([-0.0176,  0.1000])\n",
      "Epoch : 1998  Loss : 2.96  Grad : tensor([-0.0176,  0.0998])\n",
      "Epoch : 1999  Loss : 2.96  Grad : tensor([-0.0176,  0.0997])\n",
      "Epoch : 2000  Loss : 2.96  Grad : tensor([-0.0176,  0.0995])\n",
      "Epoch : 2001  Loss : 2.96  Grad : tensor([-0.0176,  0.0993])\n",
      "Epoch : 2002  Loss : 2.96  Grad : tensor([-0.0175,  0.0992])\n",
      "Epoch : 2003  Loss : 2.96  Grad : tensor([-0.0175,  0.0990])\n",
      "Epoch : 2004  Loss : 2.96  Grad : tensor([-0.0174,  0.0988])\n",
      "Epoch : 2005  Loss : 2.96  Grad : tensor([-0.0174,  0.0987])\n",
      "Epoch : 2006  Loss : 2.96  Grad : tensor([-0.0174,  0.0985])\n",
      "Epoch : 2007  Loss : 2.96  Grad : tensor([-0.0174,  0.0983])\n",
      "Epoch : 2008  Loss : 2.96  Grad : tensor([-0.0173,  0.0982])\n",
      "Epoch : 2009  Loss : 2.96  Grad : tensor([-0.0173,  0.0980])\n",
      "Epoch : 2010  Loss : 2.96  Grad : tensor([-0.0173,  0.0978])\n",
      "Epoch : 2011  Loss : 2.96  Grad : tensor([-0.0172,  0.0977])\n",
      "Epoch : 2012  Loss : 2.96  Grad : tensor([-0.0172,  0.0975])\n",
      "Epoch : 2013  Loss : 2.96  Grad : tensor([-0.0172,  0.0973])\n",
      "Epoch : 2014  Loss : 2.96  Grad : tensor([-0.0172,  0.0972])\n",
      "Epoch : 2015  Loss : 2.96  Grad : tensor([-0.0171,  0.0970])\n",
      "Epoch : 2016  Loss : 2.96  Grad : tensor([-0.0171,  0.0968])\n",
      "Epoch : 2017  Loss : 2.96  Grad : tensor([-0.0171,  0.0967])\n",
      "Epoch : 2018  Loss : 2.96  Grad : tensor([-0.0171,  0.0965])\n",
      "Epoch : 2019  Loss : 2.96  Grad : tensor([-0.0170,  0.0963])\n",
      "Epoch : 2020  Loss : 2.96  Grad : tensor([-0.0170,  0.0962])\n",
      "Epoch : 2021  Loss : 2.96  Grad : tensor([-0.0170,  0.0960])\n",
      "Epoch : 2022  Loss : 2.96  Grad : tensor([-0.0169,  0.0959])\n",
      "Epoch : 2023  Loss : 2.96  Grad : tensor([-0.0169,  0.0957])\n",
      "Epoch : 2024  Loss : 2.96  Grad : tensor([-0.0169,  0.0955])\n",
      "Epoch : 2025  Loss : 2.96  Grad : tensor([-0.0169,  0.0954])\n",
      "Epoch : 2026  Loss : 2.96  Grad : tensor([-0.0168,  0.0952])\n",
      "Epoch : 2027  Loss : 2.96  Grad : tensor([-0.0168,  0.0950])\n",
      "Epoch : 2028  Loss : 2.95  Grad : tensor([-0.0168,  0.0949])\n",
      "Epoch : 2029  Loss : 2.95  Grad : tensor([-0.0167,  0.0947])\n",
      "Epoch : 2030  Loss : 2.95  Grad : tensor([-0.0167,  0.0946])\n",
      "Epoch : 2031  Loss : 2.95  Grad : tensor([-0.0167,  0.0944])\n",
      "Epoch : 2032  Loss : 2.95  Grad : tensor([-0.0167,  0.0942])\n",
      "Epoch : 2033  Loss : 2.95  Grad : tensor([-0.0166,  0.0941])\n",
      "Epoch : 2034  Loss : 2.95  Grad : tensor([-0.0166,  0.0939])\n",
      "Epoch : 2035  Loss : 2.95  Grad : tensor([-0.0165,  0.0938])\n",
      "Epoch : 2036  Loss : 2.95  Grad : tensor([-0.0165,  0.0936])\n",
      "Epoch : 2037  Loss : 2.95  Grad : tensor([-0.0165,  0.0934])\n",
      "Epoch : 2038  Loss : 2.95  Grad : tensor([-0.0165,  0.0933])\n",
      "Epoch : 2039  Loss : 2.95  Grad : tensor([-0.0164,  0.0931])\n",
      "Epoch : 2040  Loss : 2.95  Grad : tensor([-0.0164,  0.0930])\n",
      "Epoch : 2041  Loss : 2.95  Grad : tensor([-0.0164,  0.0928])\n",
      "Epoch : 2042  Loss : 2.95  Grad : tensor([-0.0164,  0.0926])\n",
      "Epoch : 2043  Loss : 2.95  Grad : tensor([-0.0163,  0.0925])\n",
      "Epoch : 2044  Loss : 2.95  Grad : tensor([-0.0163,  0.0923])\n",
      "Epoch : 2045  Loss : 2.95  Grad : tensor([-0.0163,  0.0922])\n",
      "Epoch : 2046  Loss : 2.95  Grad : tensor([-0.0163,  0.0920])\n",
      "Epoch : 2047  Loss : 2.95  Grad : tensor([-0.0162,  0.0919])\n",
      "Epoch : 2048  Loss : 2.95  Grad : tensor([-0.0162,  0.0917])\n",
      "Epoch : 2049  Loss : 2.95  Grad : tensor([-0.0162,  0.0915])\n",
      "Epoch : 2050  Loss : 2.95  Grad : tensor([-0.0162,  0.0914])\n",
      "Epoch : 2051  Loss : 2.95  Grad : tensor([-0.0161,  0.0912])\n",
      "Epoch : 2052  Loss : 2.95  Grad : tensor([-0.0161,  0.0911])\n",
      "Epoch : 2053  Loss : 2.95  Grad : tensor([-0.0161,  0.0909])\n",
      "Epoch : 2054  Loss : 2.95  Grad : tensor([-0.0160,  0.0908])\n",
      "Epoch : 2055  Loss : 2.95  Grad : tensor([-0.0160,  0.0906])\n",
      "Epoch : 2056  Loss : 2.95  Grad : tensor([-0.0160,  0.0905])\n",
      "Epoch : 2057  Loss : 2.95  Grad : tensor([-0.0160,  0.0903])\n",
      "Epoch : 2058  Loss : 2.95  Grad : tensor([-0.0159,  0.0902])\n",
      "Epoch : 2059  Loss : 2.95  Grad : tensor([-0.0159,  0.0900])\n",
      "Epoch : 2060  Loss : 2.95  Grad : tensor([-0.0159,  0.0899])\n",
      "Epoch : 2061  Loss : 2.95  Grad : tensor([-0.0158,  0.0897])\n",
      "Epoch : 2062  Loss : 2.95  Grad : tensor([-0.0158,  0.0895])\n",
      "Epoch : 2063  Loss : 2.95  Grad : tensor([-0.0158,  0.0894])\n",
      "Epoch : 2064  Loss : 2.95  Grad : tensor([-0.0158,  0.0892])\n",
      "Epoch : 2065  Loss : 2.95  Grad : tensor([-0.0157,  0.0891])\n",
      "Epoch : 2066  Loss : 2.95  Grad : tensor([-0.0157,  0.0889])\n",
      "Epoch : 2067  Loss : 2.95  Grad : tensor([-0.0157,  0.0888])\n",
      "Epoch : 2068  Loss : 2.95  Grad : tensor([-0.0157,  0.0886])\n",
      "Epoch : 2069  Loss : 2.95  Grad : tensor([-0.0157,  0.0885])\n",
      "Epoch : 2070  Loss : 2.95  Grad : tensor([-0.0156,  0.0883])\n",
      "Epoch : 2071  Loss : 2.95  Grad : tensor([-0.0156,  0.0882])\n",
      "Epoch : 2072  Loss : 2.95  Grad : tensor([-0.0155,  0.0880])\n",
      "Epoch : 2073  Loss : 2.95  Grad : tensor([-0.0155,  0.0879])\n",
      "Epoch : 2074  Loss : 2.95  Grad : tensor([-0.0155,  0.0877])\n",
      "Epoch : 2075  Loss : 2.95  Grad : tensor([-0.0155,  0.0876])\n",
      "Epoch : 2076  Loss : 2.95  Grad : tensor([-0.0154,  0.0874])\n",
      "Epoch : 2077  Loss : 2.95  Grad : tensor([-0.0154,  0.0873])\n",
      "Epoch : 2078  Loss : 2.95  Grad : tensor([-0.0154,  0.0871])\n",
      "Epoch : 2079  Loss : 2.95  Grad : tensor([-0.0154,  0.0870])\n",
      "Epoch : 2080  Loss : 2.95  Grad : tensor([-0.0154,  0.0868])\n",
      "Epoch : 2081  Loss : 2.95  Grad : tensor([-0.0153,  0.0867])\n",
      "Epoch : 2082  Loss : 2.95  Grad : tensor([-0.0153,  0.0866])\n",
      "Epoch : 2083  Loss : 2.95  Grad : tensor([-0.0153,  0.0864])\n",
      "Epoch : 2084  Loss : 2.95  Grad : tensor([-0.0152,  0.0863])\n",
      "Epoch : 2085  Loss : 2.95  Grad : tensor([-0.0152,  0.0861])\n",
      "Epoch : 2086  Loss : 2.95  Grad : tensor([-0.0152,  0.0860])\n",
      "Epoch : 2087  Loss : 2.95  Grad : tensor([-0.0152,  0.0858])\n",
      "Epoch : 2088  Loss : 2.95  Grad : tensor([-0.0152,  0.0857])\n",
      "Epoch : 2089  Loss : 2.95  Grad : tensor([-0.0151,  0.0855])\n",
      "Epoch : 2090  Loss : 2.95  Grad : tensor([-0.0151,  0.0854])\n",
      "Epoch : 2091  Loss : 2.95  Grad : tensor([-0.0151,  0.0852])\n",
      "Epoch : 2092  Loss : 2.95  Grad : tensor([-0.0150,  0.0851])\n",
      "Epoch : 2093  Loss : 2.95  Grad : tensor([-0.0150,  0.0850])\n",
      "Epoch : 2094  Loss : 2.95  Grad : tensor([-0.0150,  0.0848])\n",
      "Epoch : 2095  Loss : 2.95  Grad : tensor([-0.0149,  0.0847])\n",
      "Epoch : 2096  Loss : 2.95  Grad : tensor([-0.0150,  0.0845])\n",
      "Epoch : 2097  Loss : 2.95  Grad : tensor([-0.0149,  0.0844])\n",
      "Epoch : 2098  Loss : 2.95  Grad : tensor([-0.0149,  0.0842])\n",
      "Epoch : 2099  Loss : 2.95  Grad : tensor([-0.0149,  0.0841])\n",
      "Epoch : 2100  Loss : 2.95  Grad : tensor([-0.0148,  0.0839])\n",
      "Epoch : 2101  Loss : 2.95  Grad : tensor([-0.0148,  0.0838])\n",
      "Epoch : 2102  Loss : 2.95  Grad : tensor([-0.0148,  0.0837])\n",
      "Epoch : 2103  Loss : 2.95  Grad : tensor([-0.0148,  0.0835])\n",
      "Epoch : 2104  Loss : 2.95  Grad : tensor([-0.0148,  0.0834])\n",
      "Epoch : 2105  Loss : 2.95  Grad : tensor([-0.0147,  0.0832])\n",
      "Epoch : 2106  Loss : 2.95  Grad : tensor([-0.0147,  0.0831])\n",
      "Epoch : 2107  Loss : 2.95  Grad : tensor([-0.0146,  0.0830])\n",
      "Epoch : 2108  Loss : 2.95  Grad : tensor([-0.0146,  0.0828])\n",
      "Epoch : 2109  Loss : 2.95  Grad : tensor([-0.0146,  0.0827])\n",
      "Epoch : 2110  Loss : 2.95  Grad : tensor([-0.0146,  0.0825])\n",
      "Epoch : 2111  Loss : 2.95  Grad : tensor([-0.0145,  0.0824])\n",
      "Epoch : 2112  Loss : 2.95  Grad : tensor([-0.0145,  0.0823])\n",
      "Epoch : 2113  Loss : 2.95  Grad : tensor([-0.0145,  0.0821])\n",
      "Epoch : 2114  Loss : 2.95  Grad : tensor([-0.0145,  0.0820])\n",
      "Epoch : 2115  Loss : 2.95  Grad : tensor([-0.0144,  0.0818])\n",
      "Epoch : 2116  Loss : 2.95  Grad : tensor([-0.0144,  0.0817])\n",
      "Epoch : 2117  Loss : 2.95  Grad : tensor([-0.0144,  0.0816])\n",
      "Epoch : 2118  Loss : 2.95  Grad : tensor([-0.0144,  0.0814])\n",
      "Epoch : 2119  Loss : 2.95  Grad : tensor([-0.0144,  0.0813])\n",
      "Epoch : 2120  Loss : 2.95  Grad : tensor([-0.0143,  0.0811])\n",
      "Epoch : 2121  Loss : 2.95  Grad : tensor([-0.0143,  0.0810])\n",
      "Epoch : 2122  Loss : 2.95  Grad : tensor([-0.0143,  0.0809])\n",
      "Epoch : 2123  Loss : 2.95  Grad : tensor([-0.0143,  0.0807])\n",
      "Epoch : 2124  Loss : 2.95  Grad : tensor([-0.0142,  0.0806])\n",
      "Epoch : 2125  Loss : 2.95  Grad : tensor([-0.0142,  0.0805])\n",
      "Epoch : 2126  Loss : 2.95  Grad : tensor([-0.0142,  0.0803])\n",
      "Epoch : 2127  Loss : 2.95  Grad : tensor([-0.0142,  0.0802])\n",
      "Epoch : 2128  Loss : 2.95  Grad : tensor([-0.0141,  0.0800])\n",
      "Epoch : 2129  Loss : 2.95  Grad : tensor([-0.0141,  0.0799])\n",
      "Epoch : 2130  Loss : 2.95  Grad : tensor([-0.0141,  0.0798])\n",
      "Epoch : 2131  Loss : 2.95  Grad : tensor([-0.0141,  0.0796])\n",
      "Epoch : 2132  Loss : 2.95  Grad : tensor([-0.0141,  0.0795])\n",
      "Epoch : 2133  Loss : 2.95  Grad : tensor([-0.0140,  0.0794])\n",
      "Epoch : 2134  Loss : 2.95  Grad : tensor([-0.0140,  0.0792])\n",
      "Epoch : 2135  Loss : 2.95  Grad : tensor([-0.0140,  0.0791])\n",
      "Epoch : 2136  Loss : 2.95  Grad : tensor([-0.0139,  0.0790])\n",
      "Epoch : 2137  Loss : 2.95  Grad : tensor([-0.0139,  0.0788])\n",
      "Epoch : 2138  Loss : 2.95  Grad : tensor([-0.0139,  0.0787])\n",
      "Epoch : 2139  Loss : 2.95  Grad : tensor([-0.0139,  0.0786])\n",
      "Epoch : 2140  Loss : 2.95  Grad : tensor([-0.0138,  0.0784])\n",
      "Epoch : 2141  Loss : 2.95  Grad : tensor([-0.0138,  0.0783])\n",
      "Epoch : 2142  Loss : 2.95  Grad : tensor([-0.0138,  0.0782])\n",
      "Epoch : 2143  Loss : 2.95  Grad : tensor([-0.0138,  0.0780])\n",
      "Epoch : 2144  Loss : 2.95  Grad : tensor([-0.0138,  0.0779])\n",
      "Epoch : 2145  Loss : 2.95  Grad : tensor([-0.0137,  0.0778])\n",
      "Epoch : 2146  Loss : 2.95  Grad : tensor([-0.0137,  0.0776])\n",
      "Epoch : 2147  Loss : 2.95  Grad : tensor([-0.0137,  0.0775])\n",
      "Epoch : 2148  Loss : 2.95  Grad : tensor([-0.0137,  0.0774])\n",
      "Epoch : 2149  Loss : 2.95  Grad : tensor([-0.0136,  0.0772])\n",
      "Epoch : 2150  Loss : 2.95  Grad : tensor([-0.0136,  0.0771])\n",
      "Epoch : 2151  Loss : 2.95  Grad : tensor([-0.0136,  0.0770])\n",
      "Epoch : 2152  Loss : 2.95  Grad : tensor([-0.0136,  0.0768])\n",
      "Epoch : 2153  Loss : 2.95  Grad : tensor([-0.0135,  0.0767])\n",
      "Epoch : 2154  Loss : 2.95  Grad : tensor([-0.0135,  0.0766])\n",
      "Epoch : 2155  Loss : 2.95  Grad : tensor([-0.0135,  0.0765])\n",
      "Epoch : 2156  Loss : 2.95  Grad : tensor([-0.0135,  0.0763])\n",
      "Epoch : 2157  Loss : 2.95  Grad : tensor([-0.0135,  0.0762])\n",
      "Epoch : 2158  Loss : 2.95  Grad : tensor([-0.0134,  0.0761])\n",
      "Epoch : 2159  Loss : 2.95  Grad : tensor([-0.0134,  0.0759])\n",
      "Epoch : 2160  Loss : 2.95  Grad : tensor([-0.0134,  0.0758])\n",
      "Epoch : 2161  Loss : 2.95  Grad : tensor([-0.0134,  0.0757])\n",
      "Epoch : 2162  Loss : 2.94  Grad : tensor([-0.0133,  0.0755])\n",
      "Epoch : 2163  Loss : 2.94  Grad : tensor([-0.0133,  0.0754])\n",
      "Epoch : 2164  Loss : 2.94  Grad : tensor([-0.0133,  0.0753])\n",
      "Epoch : 2165  Loss : 2.94  Grad : tensor([-0.0133,  0.0752])\n",
      "Epoch : 2166  Loss : 2.94  Grad : tensor([-0.0133,  0.0750])\n",
      "Epoch : 2167  Loss : 2.94  Grad : tensor([-0.0132,  0.0749])\n",
      "Epoch : 2168  Loss : 2.94  Grad : tensor([-0.0132,  0.0748])\n",
      "Epoch : 2169  Loss : 2.94  Grad : tensor([-0.0132,  0.0747])\n",
      "Epoch : 2170  Loss : 2.94  Grad : tensor([-0.0132,  0.0745])\n",
      "Epoch : 2171  Loss : 2.94  Grad : tensor([-0.0132,  0.0744])\n",
      "Epoch : 2172  Loss : 2.94  Grad : tensor([-0.0131,  0.0743])\n",
      "Epoch : 2173  Loss : 2.94  Grad : tensor([-0.0131,  0.0742])\n",
      "Epoch : 2174  Loss : 2.94  Grad : tensor([-0.0131,  0.0740])\n",
      "Epoch : 2175  Loss : 2.94  Grad : tensor([-0.0131,  0.0739])\n",
      "Epoch : 2176  Loss : 2.94  Grad : tensor([-0.0130,  0.0738])\n",
      "Epoch : 2177  Loss : 2.94  Grad : tensor([-0.0130,  0.0736])\n",
      "Epoch : 2178  Loss : 2.94  Grad : tensor([-0.0130,  0.0735])\n",
      "Epoch : 2179  Loss : 2.94  Grad : tensor([-0.0130,  0.0734])\n",
      "Epoch : 2180  Loss : 2.94  Grad : tensor([-0.0129,  0.0733])\n",
      "Epoch : 2181  Loss : 2.94  Grad : tensor([-0.0129,  0.0731])\n",
      "Epoch : 2182  Loss : 2.94  Grad : tensor([-0.0129,  0.0730])\n",
      "Epoch : 2183  Loss : 2.94  Grad : tensor([-0.0129,  0.0729])\n",
      "Epoch : 2184  Loss : 2.94  Grad : tensor([-0.0129,  0.0728])\n",
      "Epoch : 2185  Loss : 2.94  Grad : tensor([-0.0128,  0.0727])\n",
      "Epoch : 2186  Loss : 2.94  Grad : tensor([-0.0128,  0.0725])\n",
      "Epoch : 2187  Loss : 2.94  Grad : tensor([-0.0128,  0.0724])\n",
      "Epoch : 2188  Loss : 2.94  Grad : tensor([-0.0128,  0.0723])\n",
      "Epoch : 2189  Loss : 2.94  Grad : tensor([-0.0127,  0.0722])\n",
      "Epoch : 2190  Loss : 2.94  Grad : tensor([-0.0127,  0.0720])\n",
      "Epoch : 2191  Loss : 2.94  Grad : tensor([-0.0127,  0.0719])\n",
      "Epoch : 2192  Loss : 2.94  Grad : tensor([-0.0127,  0.0718])\n",
      "Epoch : 2193  Loss : 2.94  Grad : tensor([-0.0127,  0.0717])\n",
      "Epoch : 2194  Loss : 2.94  Grad : tensor([-0.0126,  0.0715])\n",
      "Epoch : 2195  Loss : 2.94  Grad : tensor([-0.0126,  0.0714])\n",
      "Epoch : 2196  Loss : 2.94  Grad : tensor([-0.0126,  0.0713])\n",
      "Epoch : 2197  Loss : 2.94  Grad : tensor([-0.0126,  0.0712])\n",
      "Epoch : 2198  Loss : 2.94  Grad : tensor([-0.0126,  0.0711])\n",
      "Epoch : 2199  Loss : 2.94  Grad : tensor([-0.0125,  0.0709])\n",
      "Epoch : 2200  Loss : 2.94  Grad : tensor([-0.0125,  0.0708])\n",
      "Epoch : 2201  Loss : 2.94  Grad : tensor([-0.0125,  0.0707])\n",
      "Epoch : 2202  Loss : 2.94  Grad : tensor([-0.0125,  0.0706])\n",
      "Epoch : 2203  Loss : 2.94  Grad : tensor([-0.0124,  0.0705])\n",
      "Epoch : 2204  Loss : 2.94  Grad : tensor([-0.0124,  0.0703])\n",
      "Epoch : 2205  Loss : 2.94  Grad : tensor([-0.0124,  0.0702])\n",
      "Epoch : 2206  Loss : 2.94  Grad : tensor([-0.0124,  0.0701])\n",
      "Epoch : 2207  Loss : 2.94  Grad : tensor([-0.0124,  0.0700])\n",
      "Epoch : 2208  Loss : 2.94  Grad : tensor([-0.0123,  0.0699])\n",
      "Epoch : 2209  Loss : 2.94  Grad : tensor([-0.0123,  0.0697])\n",
      "Epoch : 2210  Loss : 2.94  Grad : tensor([-0.0123,  0.0696])\n",
      "Epoch : 2211  Loss : 2.94  Grad : tensor([-0.0123,  0.0695])\n",
      "Epoch : 2212  Loss : 2.94  Grad : tensor([-0.0122,  0.0694])\n",
      "Epoch : 2213  Loss : 2.94  Grad : tensor([-0.0122,  0.0693])\n",
      "Epoch : 2214  Loss : 2.94  Grad : tensor([-0.0122,  0.0692])\n",
      "Epoch : 2215  Loss : 2.94  Grad : tensor([-0.0122,  0.0690])\n",
      "Epoch : 2216  Loss : 2.94  Grad : tensor([-0.0122,  0.0689])\n",
      "Epoch : 2217  Loss : 2.94  Grad : tensor([-0.0122,  0.0688])\n",
      "Epoch : 2218  Loss : 2.94  Grad : tensor([-0.0121,  0.0687])\n",
      "Epoch : 2219  Loss : 2.94  Grad : tensor([-0.0121,  0.0686])\n",
      "Epoch : 2220  Loss : 2.94  Grad : tensor([-0.0121,  0.0685])\n",
      "Epoch : 2221  Loss : 2.94  Grad : tensor([-0.0121,  0.0683])\n",
      "Epoch : 2222  Loss : 2.94  Grad : tensor([-0.0120,  0.0682])\n",
      "Epoch : 2223  Loss : 2.94  Grad : tensor([-0.0120,  0.0681])\n",
      "Epoch : 2224  Loss : 2.94  Grad : tensor([-0.0120,  0.0680])\n",
      "Epoch : 2225  Loss : 2.94  Grad : tensor([-0.0120,  0.0679])\n",
      "Epoch : 2226  Loss : 2.94  Grad : tensor([-0.0120,  0.0678])\n",
      "Epoch : 2227  Loss : 2.94  Grad : tensor([-0.0119,  0.0676])\n",
      "Epoch : 2228  Loss : 2.94  Grad : tensor([-0.0119,  0.0675])\n",
      "Epoch : 2229  Loss : 2.94  Grad : tensor([-0.0119,  0.0674])\n",
      "Epoch : 2230  Loss : 2.94  Grad : tensor([-0.0119,  0.0673])\n",
      "Epoch : 2231  Loss : 2.94  Grad : tensor([-0.0119,  0.0672])\n",
      "Epoch : 2232  Loss : 2.94  Grad : tensor([-0.0118,  0.0671])\n",
      "Epoch : 2233  Loss : 2.94  Grad : tensor([-0.0118,  0.0670])\n",
      "Epoch : 2234  Loss : 2.94  Grad : tensor([-0.0118,  0.0668])\n",
      "Epoch : 2235  Loss : 2.94  Grad : tensor([-0.0118,  0.0667])\n",
      "Epoch : 2236  Loss : 2.94  Grad : tensor([-0.0118,  0.0666])\n",
      "Epoch : 2237  Loss : 2.94  Grad : tensor([-0.0117,  0.0665])\n",
      "Epoch : 2238  Loss : 2.94  Grad : tensor([-0.0117,  0.0664])\n",
      "Epoch : 2239  Loss : 2.94  Grad : tensor([-0.0117,  0.0663])\n",
      "Epoch : 2240  Loss : 2.94  Grad : tensor([-0.0117,  0.0662])\n",
      "Epoch : 2241  Loss : 2.94  Grad : tensor([-0.0117,  0.0661])\n",
      "Epoch : 2242  Loss : 2.94  Grad : tensor([-0.0117,  0.0659])\n",
      "Epoch : 2243  Loss : 2.94  Grad : tensor([-0.0116,  0.0658])\n",
      "Epoch : 2244  Loss : 2.94  Grad : tensor([-0.0116,  0.0657])\n",
      "Epoch : 2245  Loss : 2.94  Grad : tensor([-0.0116,  0.0656])\n",
      "Epoch : 2246  Loss : 2.94  Grad : tensor([-0.0116,  0.0655])\n",
      "Epoch : 2247  Loss : 2.94  Grad : tensor([-0.0115,  0.0654])\n",
      "Epoch : 2248  Loss : 2.94  Grad : tensor([-0.0115,  0.0653])\n",
      "Epoch : 2249  Loss : 2.94  Grad : tensor([-0.0115,  0.0652])\n",
      "Epoch : 2250  Loss : 2.94  Grad : tensor([-0.0115,  0.0650])\n",
      "Epoch : 2251  Loss : 2.94  Grad : tensor([-0.0115,  0.0649])\n",
      "Epoch : 2252  Loss : 2.94  Grad : tensor([-0.0114,  0.0648])\n",
      "Epoch : 2253  Loss : 2.94  Grad : tensor([-0.0114,  0.0647])\n",
      "Epoch : 2254  Loss : 2.94  Grad : tensor([-0.0114,  0.0646])\n",
      "Epoch : 2255  Loss : 2.94  Grad : tensor([-0.0114,  0.0645])\n",
      "Epoch : 2256  Loss : 2.94  Grad : tensor([-0.0114,  0.0644])\n",
      "Epoch : 2257  Loss : 2.94  Grad : tensor([-0.0114,  0.0643])\n",
      "Epoch : 2258  Loss : 2.94  Grad : tensor([-0.0114,  0.0642])\n",
      "Epoch : 2259  Loss : 2.94  Grad : tensor([-0.0113,  0.0641])\n",
      "Epoch : 2260  Loss : 2.94  Grad : tensor([-0.0113,  0.0640])\n",
      "Epoch : 2261  Loss : 2.94  Grad : tensor([-0.0113,  0.0638])\n",
      "Epoch : 2262  Loss : 2.94  Grad : tensor([-0.0113,  0.0637])\n",
      "Epoch : 2263  Loss : 2.94  Grad : tensor([-0.0112,  0.0636])\n",
      "Epoch : 2264  Loss : 2.94  Grad : tensor([-0.0112,  0.0635])\n",
      "Epoch : 2265  Loss : 2.94  Grad : tensor([-0.0112,  0.0634])\n",
      "Epoch : 2266  Loss : 2.94  Grad : tensor([-0.0112,  0.0633])\n",
      "Epoch : 2267  Loss : 2.94  Grad : tensor([-0.0112,  0.0632])\n",
      "Epoch : 2268  Loss : 2.94  Grad : tensor([-0.0111,  0.0631])\n",
      "Epoch : 2269  Loss : 2.94  Grad : tensor([-0.0111,  0.0630])\n",
      "Epoch : 2270  Loss : 2.94  Grad : tensor([-0.0111,  0.0629])\n",
      "Epoch : 2271  Loss : 2.94  Grad : tensor([-0.0111,  0.0628])\n",
      "Epoch : 2272  Loss : 2.94  Grad : tensor([-0.0111,  0.0627])\n",
      "Epoch : 2273  Loss : 2.94  Grad : tensor([-0.0111,  0.0626])\n",
      "Epoch : 2274  Loss : 2.94  Grad : tensor([-0.0110,  0.0624])\n",
      "Epoch : 2275  Loss : 2.94  Grad : tensor([-0.0110,  0.0623])\n",
      "Epoch : 2276  Loss : 2.94  Grad : tensor([-0.0110,  0.0622])\n",
      "Epoch : 2277  Loss : 2.94  Grad : tensor([-0.0110,  0.0621])\n",
      "Epoch : 2278  Loss : 2.94  Grad : tensor([-0.0110,  0.0620])\n",
      "Epoch : 2279  Loss : 2.94  Grad : tensor([-0.0109,  0.0619])\n",
      "Epoch : 2280  Loss : 2.94  Grad : tensor([-0.0109,  0.0618])\n",
      "Epoch : 2281  Loss : 2.94  Grad : tensor([-0.0109,  0.0617])\n",
      "Epoch : 2282  Loss : 2.94  Grad : tensor([-0.0109,  0.0616])\n",
      "Epoch : 2283  Loss : 2.94  Grad : tensor([-0.0109,  0.0615])\n",
      "Epoch : 2284  Loss : 2.94  Grad : tensor([-0.0108,  0.0614])\n",
      "Epoch : 2285  Loss : 2.94  Grad : tensor([-0.0108,  0.0613])\n",
      "Epoch : 2286  Loss : 2.94  Grad : tensor([-0.0108,  0.0612])\n",
      "Epoch : 2287  Loss : 2.94  Grad : tensor([-0.0108,  0.0611])\n",
      "Epoch : 2288  Loss : 2.94  Grad : tensor([-0.0108,  0.0610])\n",
      "Epoch : 2289  Loss : 2.94  Grad : tensor([-0.0108,  0.0609])\n",
      "Epoch : 2290  Loss : 2.94  Grad : tensor([-0.0107,  0.0608])\n",
      "Epoch : 2291  Loss : 2.94  Grad : tensor([-0.0107,  0.0607])\n",
      "Epoch : 2292  Loss : 2.94  Grad : tensor([-0.0107,  0.0606])\n",
      "Epoch : 2293  Loss : 2.94  Grad : tensor([-0.0107,  0.0605])\n",
      "Epoch : 2294  Loss : 2.94  Grad : tensor([-0.0107,  0.0604])\n",
      "Epoch : 2295  Loss : 2.94  Grad : tensor([-0.0106,  0.0603])\n",
      "Epoch : 2296  Loss : 2.94  Grad : tensor([-0.0106,  0.0602])\n",
      "Epoch : 2297  Loss : 2.94  Grad : tensor([-0.0106,  0.0601])\n",
      "Epoch : 2298  Loss : 2.94  Grad : tensor([-0.0106,  0.0600])\n",
      "Epoch : 2299  Loss : 2.94  Grad : tensor([-0.0106,  0.0598])\n",
      "Epoch : 2300  Loss : 2.94  Grad : tensor([-0.0106,  0.0597])\n",
      "Epoch : 2301  Loss : 2.94  Grad : tensor([-0.0105,  0.0596])\n",
      "Epoch : 2302  Loss : 2.94  Grad : tensor([-0.0105,  0.0595])\n",
      "Epoch : 2303  Loss : 2.94  Grad : tensor([-0.0105,  0.0594])\n",
      "Epoch : 2304  Loss : 2.94  Grad : tensor([-0.0105,  0.0593])\n",
      "Epoch : 2305  Loss : 2.94  Grad : tensor([-0.0105,  0.0592])\n",
      "Epoch : 2306  Loss : 2.94  Grad : tensor([-0.0105,  0.0591])\n",
      "Epoch : 2307  Loss : 2.94  Grad : tensor([-0.0104,  0.0590])\n",
      "Epoch : 2308  Loss : 2.94  Grad : tensor([-0.0104,  0.0589])\n",
      "Epoch : 2309  Loss : 2.94  Grad : tensor([-0.0104,  0.0588])\n",
      "Epoch : 2310  Loss : 2.94  Grad : tensor([-0.0104,  0.0587])\n",
      "Epoch : 2311  Loss : 2.94  Grad : tensor([-0.0104,  0.0586])\n",
      "Epoch : 2312  Loss : 2.94  Grad : tensor([-0.0103,  0.0585])\n",
      "Epoch : 2313  Loss : 2.94  Grad : tensor([-0.0103,  0.0584])\n",
      "Epoch : 2314  Loss : 2.94  Grad : tensor([-0.0103,  0.0583])\n",
      "Epoch : 2315  Loss : 2.94  Grad : tensor([-0.0103,  0.0582])\n",
      "Epoch : 2316  Loss : 2.94  Grad : tensor([-0.0103,  0.0581])\n",
      "Epoch : 2317  Loss : 2.94  Grad : tensor([-0.0103,  0.0580])\n",
      "Epoch : 2318  Loss : 2.94  Grad : tensor([-0.0102,  0.0580])\n",
      "Epoch : 2319  Loss : 2.94  Grad : tensor([-0.0102,  0.0578])\n",
      "Epoch : 2320  Loss : 2.94  Grad : tensor([-0.0102,  0.0578])\n",
      "Epoch : 2321  Loss : 2.94  Grad : tensor([-0.0102,  0.0577])\n",
      "Epoch : 2322  Loss : 2.94  Grad : tensor([-0.0102,  0.0576])\n",
      "Epoch : 2323  Loss : 2.94  Grad : tensor([-0.0102,  0.0575])\n",
      "Epoch : 2324  Loss : 2.94  Grad : tensor([-0.0101,  0.0574])\n",
      "Epoch : 2325  Loss : 2.94  Grad : tensor([-0.0101,  0.0573])\n",
      "Epoch : 2326  Loss : 2.94  Grad : tensor([-0.0101,  0.0572])\n",
      "Epoch : 2327  Loss : 2.94  Grad : tensor([-0.0101,  0.0571])\n",
      "Epoch : 2328  Loss : 2.94  Grad : tensor([-0.0101,  0.0570])\n",
      "Epoch : 2329  Loss : 2.94  Grad : tensor([-0.0101,  0.0569])\n",
      "Epoch : 2330  Loss : 2.94  Grad : tensor([-0.0100,  0.0568])\n",
      "Epoch : 2331  Loss : 2.94  Grad : tensor([-0.0100,  0.0567])\n",
      "Epoch : 2332  Loss : 2.94  Grad : tensor([-0.0100,  0.0566])\n",
      "Epoch : 2333  Loss : 2.94  Grad : tensor([-0.0100,  0.0565])\n",
      "Epoch : 2334  Loss : 2.94  Grad : tensor([-0.0100,  0.0564])\n",
      "Epoch : 2335  Loss : 2.94  Grad : tensor([-0.0100,  0.0563])\n",
      "Epoch : 2336  Loss : 2.94  Grad : tensor([-0.0099,  0.0562])\n",
      "Epoch : 2337  Loss : 2.94  Grad : tensor([-0.0099,  0.0561])\n",
      "Epoch : 2338  Loss : 2.94  Grad : tensor([-0.0099,  0.0560])\n",
      "Epoch : 2339  Loss : 2.94  Grad : tensor([-0.0099,  0.0559])\n",
      "Epoch : 2340  Loss : 2.94  Grad : tensor([-0.0099,  0.0558])\n",
      "Epoch : 2341  Loss : 2.94  Grad : tensor([-0.0098,  0.0557])\n",
      "Epoch : 2342  Loss : 2.94  Grad : tensor([-0.0098,  0.0556])\n",
      "Epoch : 2343  Loss : 2.94  Grad : tensor([-0.0098,  0.0555])\n",
      "Epoch : 2344  Loss : 2.94  Grad : tensor([-0.0098,  0.0554])\n",
      "Epoch : 2345  Loss : 2.94  Grad : tensor([-0.0098,  0.0553])\n",
      "Epoch : 2346  Loss : 2.94  Grad : tensor([-0.0098,  0.0553])\n",
      "Epoch : 2347  Loss : 2.94  Grad : tensor([-0.0097,  0.0552])\n",
      "Epoch : 2348  Loss : 2.94  Grad : tensor([-0.0097,  0.0551])\n",
      "Epoch : 2349  Loss : 2.94  Grad : tensor([-0.0097,  0.0550])\n",
      "Epoch : 2350  Loss : 2.94  Grad : tensor([-0.0097,  0.0549])\n",
      "Epoch : 2351  Loss : 2.94  Grad : tensor([-0.0097,  0.0548])\n",
      "Epoch : 2352  Loss : 2.94  Grad : tensor([-0.0097,  0.0547])\n",
      "Epoch : 2353  Loss : 2.94  Grad : tensor([-0.0096,  0.0546])\n",
      "Epoch : 2354  Loss : 2.94  Grad : tensor([-0.0096,  0.0545])\n",
      "Epoch : 2355  Loss : 2.94  Grad : tensor([-0.0096,  0.0544])\n",
      "Epoch : 2356  Loss : 2.94  Grad : tensor([-0.0096,  0.0543])\n",
      "Epoch : 2357  Loss : 2.94  Grad : tensor([-0.0096,  0.0542])\n",
      "Epoch : 2358  Loss : 2.94  Grad : tensor([-0.0095,  0.0541])\n",
      "Epoch : 2359  Loss : 2.94  Grad : tensor([-0.0096,  0.0540])\n",
      "Epoch : 2360  Loss : 2.94  Grad : tensor([-0.0095,  0.0540])\n",
      "Epoch : 2361  Loss : 2.94  Grad : tensor([-0.0095,  0.0539])\n",
      "Epoch : 2362  Loss : 2.94  Grad : tensor([-0.0095,  0.0538])\n",
      "Epoch : 2363  Loss : 2.94  Grad : tensor([-0.0095,  0.0537])\n",
      "Epoch : 2364  Loss : 2.94  Grad : tensor([-0.0094,  0.0536])\n",
      "Epoch : 2365  Loss : 2.94  Grad : tensor([-0.0094,  0.0535])\n",
      "Epoch : 2366  Loss : 2.94  Grad : tensor([-0.0094,  0.0534])\n",
      "Epoch : 2367  Loss : 2.94  Grad : tensor([-0.0094,  0.0533])\n",
      "Epoch : 2368  Loss : 2.94  Grad : tensor([-0.0094,  0.0532])\n",
      "Epoch : 2369  Loss : 2.94  Grad : tensor([-0.0094,  0.0531])\n",
      "Epoch : 2370  Loss : 2.94  Grad : tensor([-0.0094,  0.0530])\n",
      "Epoch : 2371  Loss : 2.94  Grad : tensor([-0.0094,  0.0530])\n",
      "Epoch : 2372  Loss : 2.94  Grad : tensor([-0.0093,  0.0529])\n",
      "Epoch : 2373  Loss : 2.94  Grad : tensor([-0.0093,  0.0528])\n",
      "Epoch : 2374  Loss : 2.94  Grad : tensor([-0.0093,  0.0527])\n",
      "Epoch : 2375  Loss : 2.94  Grad : tensor([-0.0093,  0.0526])\n",
      "Epoch : 2376  Loss : 2.94  Grad : tensor([-0.0093,  0.0525])\n",
      "Epoch : 2377  Loss : 2.94  Grad : tensor([-0.0093,  0.0524])\n",
      "Epoch : 2378  Loss : 2.94  Grad : tensor([-0.0093,  0.0523])\n",
      "Epoch : 2379  Loss : 2.94  Grad : tensor([-0.0092,  0.0522])\n",
      "Epoch : 2380  Loss : 2.94  Grad : tensor([-0.0092,  0.0522])\n",
      "Epoch : 2381  Loss : 2.94  Grad : tensor([-0.0092,  0.0521])\n",
      "Epoch : 2382  Loss : 2.94  Grad : tensor([-0.0092,  0.0520])\n",
      "Epoch : 2383  Loss : 2.94  Grad : tensor([-0.0092,  0.0519])\n",
      "Epoch : 2384  Loss : 2.94  Grad : tensor([-0.0092,  0.0518])\n",
      "Epoch : 2385  Loss : 2.94  Grad : tensor([-0.0092,  0.0517])\n",
      "Epoch : 2386  Loss : 2.94  Grad : tensor([-0.0091,  0.0516])\n",
      "Epoch : 2387  Loss : 2.94  Grad : tensor([-0.0091,  0.0515])\n",
      "Epoch : 2388  Loss : 2.94  Grad : tensor([-0.0091,  0.0514])\n",
      "Epoch : 2389  Loss : 2.94  Grad : tensor([-0.0091,  0.0514])\n",
      "Epoch : 2390  Loss : 2.94  Grad : tensor([-0.0090,  0.0513])\n",
      "Epoch : 2391  Loss : 2.94  Grad : tensor([-0.0090,  0.0512])\n",
      "Epoch : 2392  Loss : 2.94  Grad : tensor([-0.0090,  0.0511])\n",
      "Epoch : 2393  Loss : 2.94  Grad : tensor([-0.0090,  0.0510])\n",
      "Epoch : 2394  Loss : 2.94  Grad : tensor([-0.0090,  0.0509])\n",
      "Epoch : 2395  Loss : 2.94  Grad : tensor([-0.0090,  0.0508])\n",
      "Epoch : 2396  Loss : 2.94  Grad : tensor([-0.0090,  0.0507])\n",
      "Epoch : 2397  Loss : 2.94  Grad : tensor([-0.0090,  0.0507])\n",
      "Epoch : 2398  Loss : 2.94  Grad : tensor([-0.0089,  0.0506])\n",
      "Epoch : 2399  Loss : 2.94  Grad : tensor([-0.0089,  0.0505])\n",
      "Epoch : 2400  Loss : 2.94  Grad : tensor([-0.0089,  0.0504])\n",
      "Epoch : 2401  Loss : 2.94  Grad : tensor([-0.0089,  0.0503])\n",
      "Epoch : 2402  Loss : 2.94  Grad : tensor([-0.0089,  0.0502])\n",
      "Epoch : 2403  Loss : 2.94  Grad : tensor([-0.0088,  0.0502])\n",
      "Epoch : 2404  Loss : 2.94  Grad : tensor([-0.0088,  0.0501])\n",
      "Epoch : 2405  Loss : 2.94  Grad : tensor([-0.0088,  0.0500])\n",
      "Epoch : 2406  Loss : 2.94  Grad : tensor([-0.0088,  0.0499])\n",
      "Epoch : 2407  Loss : 2.94  Grad : tensor([-0.0088,  0.0498])\n",
      "Epoch : 2408  Loss : 2.94  Grad : tensor([-0.0088,  0.0497])\n",
      "Epoch : 2409  Loss : 2.94  Grad : tensor([-0.0088,  0.0496])\n",
      "Epoch : 2410  Loss : 2.94  Grad : tensor([-0.0088,  0.0496])\n",
      "Epoch : 2411  Loss : 2.94  Grad : tensor([-0.0087,  0.0495])\n",
      "Epoch : 2412  Loss : 2.94  Grad : tensor([-0.0087,  0.0494])\n",
      "Epoch : 2413  Loss : 2.94  Grad : tensor([-0.0087,  0.0493])\n",
      "Epoch : 2414  Loss : 2.94  Grad : tensor([-0.0087,  0.0492])\n",
      "Epoch : 2415  Loss : 2.93  Grad : tensor([-0.0087,  0.0491])\n",
      "Epoch : 2416  Loss : 2.93  Grad : tensor([-0.0087,  0.0491])\n",
      "Epoch : 2417  Loss : 2.93  Grad : tensor([-0.0086,  0.0490])\n",
      "Epoch : 2418  Loss : 2.93  Grad : tensor([-0.0086,  0.0489])\n",
      "Epoch : 2419  Loss : 2.93  Grad : tensor([-0.0086,  0.0488])\n",
      "Epoch : 2420  Loss : 2.93  Grad : tensor([-0.0086,  0.0487])\n",
      "Epoch : 2421  Loss : 2.93  Grad : tensor([-0.0086,  0.0486])\n",
      "Epoch : 2422  Loss : 2.93  Grad : tensor([-0.0086,  0.0486])\n",
      "Epoch : 2423  Loss : 2.93  Grad : tensor([-0.0086,  0.0485])\n",
      "Epoch : 2424  Loss : 2.93  Grad : tensor([-0.0086,  0.0484])\n",
      "Epoch : 2425  Loss : 2.93  Grad : tensor([-0.0086,  0.0483])\n",
      "Epoch : 2426  Loss : 2.93  Grad : tensor([-0.0085,  0.0482])\n",
      "Epoch : 2427  Loss : 2.93  Grad : tensor([-0.0085,  0.0481])\n",
      "Epoch : 2428  Loss : 2.93  Grad : tensor([-0.0085,  0.0481])\n",
      "Epoch : 2429  Loss : 2.93  Grad : tensor([-0.0085,  0.0480])\n",
      "Epoch : 2430  Loss : 2.93  Grad : tensor([-0.0085,  0.0479])\n",
      "Epoch : 2431  Loss : 2.93  Grad : tensor([-0.0084,  0.0478])\n",
      "Epoch : 2432  Loss : 2.93  Grad : tensor([-0.0084,  0.0477])\n",
      "Epoch : 2433  Loss : 2.93  Grad : tensor([-0.0084,  0.0477])\n",
      "Epoch : 2434  Loss : 2.93  Grad : tensor([-0.0084,  0.0476])\n",
      "Epoch : 2435  Loss : 2.93  Grad : tensor([-0.0084,  0.0475])\n",
      "Epoch : 2436  Loss : 2.93  Grad : tensor([-0.0084,  0.0474])\n",
      "Epoch : 2437  Loss : 2.93  Grad : tensor([-0.0084,  0.0473])\n",
      "Epoch : 2438  Loss : 2.93  Grad : tensor([-0.0083,  0.0473])\n",
      "Epoch : 2439  Loss : 2.93  Grad : tensor([-0.0083,  0.0472])\n",
      "Epoch : 2440  Loss : 2.93  Grad : tensor([-0.0083,  0.0471])\n",
      "Epoch : 2441  Loss : 2.93  Grad : tensor([-0.0083,  0.0470])\n",
      "Epoch : 2442  Loss : 2.93  Grad : tensor([-0.0083,  0.0469])\n",
      "Epoch : 2443  Loss : 2.93  Grad : tensor([-0.0083,  0.0469])\n",
      "Epoch : 2444  Loss : 2.93  Grad : tensor([-0.0083,  0.0468])\n",
      "Epoch : 2445  Loss : 2.93  Grad : tensor([-0.0083,  0.0467])\n",
      "Epoch : 2446  Loss : 2.93  Grad : tensor([-0.0083,  0.0466])\n",
      "Epoch : 2447  Loss : 2.93  Grad : tensor([-0.0082,  0.0465])\n",
      "Epoch : 2448  Loss : 2.93  Grad : tensor([-0.0082,  0.0465])\n",
      "Epoch : 2449  Loss : 2.93  Grad : tensor([-0.0082,  0.0464])\n",
      "Epoch : 2450  Loss : 2.93  Grad : tensor([-0.0082,  0.0463])\n",
      "Epoch : 2451  Loss : 2.93  Grad : tensor([-0.0082,  0.0462])\n",
      "Epoch : 2452  Loss : 2.93  Grad : tensor([-0.0082,  0.0461])\n",
      "Epoch : 2453  Loss : 2.93  Grad : tensor([-0.0081,  0.0461])\n",
      "Epoch : 2454  Loss : 2.93  Grad : tensor([-0.0081,  0.0460])\n",
      "Epoch : 2455  Loss : 2.93  Grad : tensor([-0.0081,  0.0459])\n",
      "Epoch : 2456  Loss : 2.93  Grad : tensor([-0.0081,  0.0458])\n",
      "Epoch : 2457  Loss : 2.93  Grad : tensor([-0.0081,  0.0457])\n",
      "Epoch : 2458  Loss : 2.93  Grad : tensor([-0.0081,  0.0457])\n",
      "Epoch : 2459  Loss : 2.93  Grad : tensor([-0.0080,  0.0456])\n",
      "Epoch : 2460  Loss : 2.93  Grad : tensor([-0.0080,  0.0455])\n",
      "Epoch : 2461  Loss : 2.93  Grad : tensor([-0.0080,  0.0454])\n",
      "Epoch : 2462  Loss : 2.93  Grad : tensor([-0.0080,  0.0454])\n",
      "Epoch : 2463  Loss : 2.93  Grad : tensor([-0.0080,  0.0453])\n",
      "Epoch : 2464  Loss : 2.93  Grad : tensor([-0.0080,  0.0452])\n",
      "Epoch : 2465  Loss : 2.93  Grad : tensor([-0.0080,  0.0451])\n",
      "Epoch : 2466  Loss : 2.93  Grad : tensor([-0.0080,  0.0451])\n",
      "Epoch : 2467  Loss : 2.93  Grad : tensor([-0.0079,  0.0450])\n",
      "Epoch : 2468  Loss : 2.93  Grad : tensor([-0.0079,  0.0449])\n",
      "Epoch : 2469  Loss : 2.93  Grad : tensor([-0.0079,  0.0448])\n",
      "Epoch : 2470  Loss : 2.93  Grad : tensor([-0.0079,  0.0448])\n",
      "Epoch : 2471  Loss : 2.93  Grad : tensor([-0.0079,  0.0447])\n",
      "Epoch : 2472  Loss : 2.93  Grad : tensor([-0.0079,  0.0446])\n",
      "Epoch : 2473  Loss : 2.93  Grad : tensor([-0.0079,  0.0445])\n",
      "Epoch : 2474  Loss : 2.93  Grad : tensor([-0.0079,  0.0444])\n",
      "Epoch : 2475  Loss : 2.93  Grad : tensor([-0.0078,  0.0444])\n",
      "Epoch : 2476  Loss : 2.93  Grad : tensor([-0.0078,  0.0443])\n",
      "Epoch : 2477  Loss : 2.93  Grad : tensor([-0.0078,  0.0442])\n",
      "Epoch : 2478  Loss : 2.93  Grad : tensor([-0.0078,  0.0441])\n",
      "Epoch : 2479  Loss : 2.93  Grad : tensor([-0.0078,  0.0441])\n",
      "Epoch : 2480  Loss : 2.93  Grad : tensor([-0.0078,  0.0440])\n",
      "Epoch : 2481  Loss : 2.93  Grad : tensor([-0.0078,  0.0439])\n",
      "Epoch : 2482  Loss : 2.93  Grad : tensor([-0.0077,  0.0438])\n",
      "Epoch : 2483  Loss : 2.93  Grad : tensor([-0.0077,  0.0438])\n",
      "Epoch : 2484  Loss : 2.93  Grad : tensor([-0.0077,  0.0437])\n",
      "Epoch : 2485  Loss : 2.93  Grad : tensor([-0.0077,  0.0436])\n",
      "Epoch : 2486  Loss : 2.93  Grad : tensor([-0.0077,  0.0436])\n",
      "Epoch : 2487  Loss : 2.93  Grad : tensor([-0.0077,  0.0435])\n",
      "Epoch : 2488  Loss : 2.93  Grad : tensor([-0.0077,  0.0434])\n",
      "Epoch : 2489  Loss : 2.93  Grad : tensor([-0.0077,  0.0433])\n",
      "Epoch : 2490  Loss : 2.93  Grad : tensor([-0.0076,  0.0433])\n",
      "Epoch : 2491  Loss : 2.93  Grad : tensor([-0.0076,  0.0432])\n",
      "Epoch : 2492  Loss : 2.93  Grad : tensor([-0.0076,  0.0431])\n",
      "Epoch : 2493  Loss : 2.93  Grad : tensor([-0.0076,  0.0430])\n",
      "Epoch : 2494  Loss : 2.93  Grad : tensor([-0.0076,  0.0430])\n",
      "Epoch : 2495  Loss : 2.93  Grad : tensor([-0.0076,  0.0429])\n",
      "Epoch : 2496  Loss : 2.93  Grad : tensor([-0.0076,  0.0428])\n",
      "Epoch : 2497  Loss : 2.93  Grad : tensor([-0.0075,  0.0427])\n",
      "Epoch : 2498  Loss : 2.93  Grad : tensor([-0.0075,  0.0427])\n",
      "Epoch : 2499  Loss : 2.93  Grad : tensor([-0.0075,  0.0426])\n",
      "Epoch : 2500  Loss : 2.93  Grad : tensor([-0.0075,  0.0425])\n",
      "Epoch : 2501  Loss : 2.93  Grad : tensor([-0.0075,  0.0425])\n",
      "Epoch : 2502  Loss : 2.93  Grad : tensor([-0.0075,  0.0424])\n",
      "Epoch : 2503  Loss : 2.93  Grad : tensor([-0.0075,  0.0423])\n",
      "Epoch : 2504  Loss : 2.93  Grad : tensor([-0.0075,  0.0422])\n",
      "Epoch : 2505  Loss : 2.93  Grad : tensor([-0.0074,  0.0422])\n",
      "Epoch : 2506  Loss : 2.93  Grad : tensor([-0.0074,  0.0421])\n",
      "Epoch : 2507  Loss : 2.93  Grad : tensor([-0.0074,  0.0420])\n",
      "Epoch : 2508  Loss : 2.93  Grad : tensor([-0.0074,  0.0420])\n",
      "Epoch : 2509  Loss : 2.93  Grad : tensor([-0.0074,  0.0419])\n",
      "Epoch : 2510  Loss : 2.93  Grad : tensor([-0.0074,  0.0418])\n",
      "Epoch : 2511  Loss : 2.93  Grad : tensor([-0.0074,  0.0417])\n",
      "Epoch : 2512  Loss : 2.93  Grad : tensor([-0.0073,  0.0417])\n",
      "Epoch : 2513  Loss : 2.93  Grad : tensor([-0.0073,  0.0416])\n",
      "Epoch : 2514  Loss : 2.93  Grad : tensor([-0.0073,  0.0415])\n",
      "Epoch : 2515  Loss : 2.93  Grad : tensor([-0.0073,  0.0415])\n",
      "Epoch : 2516  Loss : 2.93  Grad : tensor([-0.0073,  0.0414])\n",
      "Epoch : 2517  Loss : 2.93  Grad : tensor([-0.0073,  0.0413])\n",
      "Epoch : 2518  Loss : 2.93  Grad : tensor([-0.0073,  0.0412])\n",
      "Epoch : 2519  Loss : 2.93  Grad : tensor([-0.0073,  0.0412])\n",
      "Epoch : 2520  Loss : 2.93  Grad : tensor([-0.0073,  0.0411])\n",
      "Epoch : 2521  Loss : 2.93  Grad : tensor([-0.0073,  0.0410])\n",
      "Epoch : 2522  Loss : 2.93  Grad : tensor([-0.0073,  0.0410])\n",
      "Epoch : 2523  Loss : 2.93  Grad : tensor([-0.0072,  0.0409])\n",
      "Epoch : 2524  Loss : 2.93  Grad : tensor([-0.0072,  0.0408])\n",
      "Epoch : 2525  Loss : 2.93  Grad : tensor([-0.0072,  0.0408])\n",
      "Epoch : 2526  Loss : 2.93  Grad : tensor([-0.0072,  0.0407])\n",
      "Epoch : 2527  Loss : 2.93  Grad : tensor([-0.0072,  0.0406])\n",
      "Epoch : 2528  Loss : 2.93  Grad : tensor([-0.0072,  0.0405])\n",
      "Epoch : 2529  Loss : 2.93  Grad : tensor([-0.0072,  0.0405])\n",
      "Epoch : 2530  Loss : 2.93  Grad : tensor([-0.0071,  0.0404])\n",
      "Epoch : 2531  Loss : 2.93  Grad : tensor([-0.0071,  0.0403])\n",
      "Epoch : 2532  Loss : 2.93  Grad : tensor([-0.0071,  0.0403])\n",
      "Epoch : 2533  Loss : 2.93  Grad : tensor([-0.0071,  0.0402])\n",
      "Epoch : 2534  Loss : 2.93  Grad : tensor([-0.0071,  0.0401])\n",
      "Epoch : 2535  Loss : 2.93  Grad : tensor([-0.0071,  0.0401])\n",
      "Epoch : 2536  Loss : 2.93  Grad : tensor([-0.0071,  0.0400])\n",
      "Epoch : 2537  Loss : 2.93  Grad : tensor([-0.0071,  0.0399])\n",
      "Epoch : 2538  Loss : 2.93  Grad : tensor([-0.0070,  0.0399])\n",
      "Epoch : 2539  Loss : 2.93  Grad : tensor([-0.0070,  0.0398])\n",
      "Epoch : 2540  Loss : 2.93  Grad : tensor([-0.0070,  0.0397])\n",
      "Epoch : 2541  Loss : 2.93  Grad : tensor([-0.0070,  0.0397])\n",
      "Epoch : 2542  Loss : 2.93  Grad : tensor([-0.0070,  0.0396])\n",
      "Epoch : 2543  Loss : 2.93  Grad : tensor([-0.0070,  0.0395])\n",
      "Epoch : 2544  Loss : 2.93  Grad : tensor([-0.0070,  0.0395])\n",
      "Epoch : 2545  Loss : 2.93  Grad : tensor([-0.0070,  0.0394])\n",
      "Epoch : 2546  Loss : 2.93  Grad : tensor([-0.0069,  0.0393])\n",
      "Epoch : 2547  Loss : 2.93  Grad : tensor([-0.0069,  0.0393])\n",
      "Epoch : 2548  Loss : 2.93  Grad : tensor([-0.0069,  0.0392])\n",
      "Epoch : 2549  Loss : 2.93  Grad : tensor([-0.0069,  0.0391])\n",
      "Epoch : 2550  Loss : 2.93  Grad : tensor([-0.0069,  0.0391])\n",
      "Epoch : 2551  Loss : 2.93  Grad : tensor([-0.0069,  0.0390])\n",
      "Epoch : 2552  Loss : 2.93  Grad : tensor([-0.0069,  0.0389])\n",
      "Epoch : 2553  Loss : 2.93  Grad : tensor([-0.0069,  0.0389])\n",
      "Epoch : 2554  Loss : 2.93  Grad : tensor([-0.0069,  0.0388])\n",
      "Epoch : 2555  Loss : 2.93  Grad : tensor([-0.0068,  0.0387])\n",
      "Epoch : 2556  Loss : 2.93  Grad : tensor([-0.0068,  0.0387])\n",
      "Epoch : 2557  Loss : 2.93  Grad : tensor([-0.0068,  0.0386])\n",
      "Epoch : 2558  Loss : 2.93  Grad : tensor([-0.0068,  0.0385])\n",
      "Epoch : 2559  Loss : 2.93  Grad : tensor([-0.0068,  0.0385])\n",
      "Epoch : 2560  Loss : 2.93  Grad : tensor([-0.0068,  0.0384])\n",
      "Epoch : 2561  Loss : 2.93  Grad : tensor([-0.0068,  0.0383])\n",
      "Epoch : 2562  Loss : 2.93  Grad : tensor([-0.0068,  0.0383])\n",
      "Epoch : 2563  Loss : 2.93  Grad : tensor([-0.0067,  0.0382])\n",
      "Epoch : 2564  Loss : 2.93  Grad : tensor([-0.0067,  0.0381])\n",
      "Epoch : 2565  Loss : 2.93  Grad : tensor([-0.0067,  0.0381])\n",
      "Epoch : 2566  Loss : 2.93  Grad : tensor([-0.0067,  0.0380])\n",
      "Epoch : 2567  Loss : 2.93  Grad : tensor([-0.0067,  0.0379])\n",
      "Epoch : 2568  Loss : 2.93  Grad : tensor([-0.0067,  0.0379])\n",
      "Epoch : 2569  Loss : 2.93  Grad : tensor([-0.0067,  0.0378])\n",
      "Epoch : 2570  Loss : 2.93  Grad : tensor([-0.0067,  0.0378])\n",
      "Epoch : 2571  Loss : 2.93  Grad : tensor([-0.0067,  0.0377])\n",
      "Epoch : 2572  Loss : 2.93  Grad : tensor([-0.0067,  0.0376])\n",
      "Epoch : 2573  Loss : 2.93  Grad : tensor([-0.0066,  0.0376])\n",
      "Epoch : 2574  Loss : 2.93  Grad : tensor([-0.0066,  0.0375])\n",
      "Epoch : 2575  Loss : 2.93  Grad : tensor([-0.0066,  0.0374])\n",
      "Epoch : 2576  Loss : 2.93  Grad : tensor([-0.0066,  0.0374])\n",
      "Epoch : 2577  Loss : 2.93  Grad : tensor([-0.0066,  0.0373])\n",
      "Epoch : 2578  Loss : 2.93  Grad : tensor([-0.0066,  0.0372])\n",
      "Epoch : 2579  Loss : 2.93  Grad : tensor([-0.0066,  0.0372])\n",
      "Epoch : 2580  Loss : 2.93  Grad : tensor([-0.0066,  0.0371])\n",
      "Epoch : 2581  Loss : 2.93  Grad : tensor([-0.0065,  0.0371])\n",
      "Epoch : 2582  Loss : 2.93  Grad : tensor([-0.0065,  0.0370])\n",
      "Epoch : 2583  Loss : 2.93  Grad : tensor([-0.0065,  0.0369])\n",
      "Epoch : 2584  Loss : 2.93  Grad : tensor([-0.0065,  0.0369])\n",
      "Epoch : 2585  Loss : 2.93  Grad : tensor([-0.0065,  0.0368])\n",
      "Epoch : 2586  Loss : 2.93  Grad : tensor([-0.0065,  0.0367])\n",
      "Epoch : 2587  Loss : 2.93  Grad : tensor([-0.0065,  0.0367])\n",
      "Epoch : 2588  Loss : 2.93  Grad : tensor([-0.0065,  0.0366])\n",
      "Epoch : 2589  Loss : 2.93  Grad : tensor([-0.0065,  0.0366])\n",
      "Epoch : 2590  Loss : 2.93  Grad : tensor([-0.0065,  0.0365])\n",
      "Epoch : 2591  Loss : 2.93  Grad : tensor([-0.0064,  0.0364])\n",
      "Epoch : 2592  Loss : 2.93  Grad : tensor([-0.0064,  0.0364])\n",
      "Epoch : 2593  Loss : 2.93  Grad : tensor([-0.0064,  0.0363])\n",
      "Epoch : 2594  Loss : 2.93  Grad : tensor([-0.0064,  0.0362])\n",
      "Epoch : 2595  Loss : 2.93  Grad : tensor([-0.0064,  0.0362])\n",
      "Epoch : 2596  Loss : 2.93  Grad : tensor([-0.0064,  0.0361])\n",
      "Epoch : 2597  Loss : 2.93  Grad : tensor([-0.0064,  0.0361])\n",
      "Epoch : 2598  Loss : 2.93  Grad : tensor([-0.0064,  0.0360])\n",
      "Epoch : 2599  Loss : 2.93  Grad : tensor([-0.0064,  0.0359])\n",
      "Epoch : 2600  Loss : 2.93  Grad : tensor([-0.0064,  0.0359])\n",
      "Epoch : 2601  Loss : 2.93  Grad : tensor([-0.0063,  0.0358])\n",
      "Epoch : 2602  Loss : 2.93  Grad : tensor([-0.0063,  0.0358])\n",
      "Epoch : 2603  Loss : 2.93  Grad : tensor([-0.0063,  0.0357])\n",
      "Epoch : 2604  Loss : 2.93  Grad : tensor([-0.0063,  0.0356])\n",
      "Epoch : 2605  Loss : 2.93  Grad : tensor([-0.0063,  0.0356])\n",
      "Epoch : 2606  Loss : 2.93  Grad : tensor([-0.0063,  0.0355])\n",
      "Epoch : 2607  Loss : 2.93  Grad : tensor([-0.0062,  0.0355])\n",
      "Epoch : 2608  Loss : 2.93  Grad : tensor([-0.0062,  0.0354])\n",
      "Epoch : 2609  Loss : 2.93  Grad : tensor([-0.0062,  0.0353])\n",
      "Epoch : 2610  Loss : 2.93  Grad : tensor([-0.0062,  0.0353])\n",
      "Epoch : 2611  Loss : 2.93  Grad : tensor([-0.0062,  0.0352])\n",
      "Epoch : 2612  Loss : 2.93  Grad : tensor([-0.0062,  0.0352])\n",
      "Epoch : 2613  Loss : 2.93  Grad : tensor([-0.0062,  0.0351])\n",
      "Epoch : 2614  Loss : 2.93  Grad : tensor([-0.0062,  0.0350])\n",
      "Epoch : 2615  Loss : 2.93  Grad : tensor([-0.0062,  0.0350])\n",
      "Epoch : 2616  Loss : 2.93  Grad : tensor([-0.0062,  0.0349])\n",
      "Epoch : 2617  Loss : 2.93  Grad : tensor([-0.0062,  0.0349])\n",
      "Epoch : 2618  Loss : 2.93  Grad : tensor([-0.0062,  0.0348])\n",
      "Epoch : 2619  Loss : 2.93  Grad : tensor([-0.0061,  0.0347])\n",
      "Epoch : 2620  Loss : 2.93  Grad : tensor([-0.0061,  0.0347])\n",
      "Epoch : 2621  Loss : 2.93  Grad : tensor([-0.0061,  0.0346])\n",
      "Epoch : 2622  Loss : 2.93  Grad : tensor([-0.0061,  0.0346])\n",
      "Epoch : 2623  Loss : 2.93  Grad : tensor([-0.0061,  0.0345])\n",
      "Epoch : 2624  Loss : 2.93  Grad : tensor([-0.0061,  0.0344])\n",
      "Epoch : 2625  Loss : 2.93  Grad : tensor([-0.0061,  0.0344])\n",
      "Epoch : 2626  Loss : 2.93  Grad : tensor([-0.0061,  0.0343])\n",
      "Epoch : 2627  Loss : 2.93  Grad : tensor([-0.0060,  0.0343])\n",
      "Epoch : 2628  Loss : 2.93  Grad : tensor([-0.0060,  0.0342])\n",
      "Epoch : 2629  Loss : 2.93  Grad : tensor([-0.0060,  0.0342])\n",
      "Epoch : 2630  Loss : 2.93  Grad : tensor([-0.0060,  0.0341])\n",
      "Epoch : 2631  Loss : 2.93  Grad : tensor([-0.0060,  0.0340])\n",
      "Epoch : 2632  Loss : 2.93  Grad : tensor([-0.0060,  0.0340])\n",
      "Epoch : 2633  Loss : 2.93  Grad : tensor([-0.0060,  0.0339])\n",
      "Epoch : 2634  Loss : 2.93  Grad : tensor([-0.0060,  0.0339])\n",
      "Epoch : 2635  Loss : 2.93  Grad : tensor([-0.0060,  0.0338])\n",
      "Epoch : 2636  Loss : 2.93  Grad : tensor([-0.0060,  0.0337])\n",
      "Epoch : 2637  Loss : 2.93  Grad : tensor([-0.0059,  0.0337])\n",
      "Epoch : 2638  Loss : 2.93  Grad : tensor([-0.0059,  0.0336])\n",
      "Epoch : 2639  Loss : 2.93  Grad : tensor([-0.0059,  0.0336])\n",
      "Epoch : 2640  Loss : 2.93  Grad : tensor([-0.0059,  0.0335])\n",
      "Epoch : 2641  Loss : 2.93  Grad : tensor([-0.0059,  0.0335])\n",
      "Epoch : 2642  Loss : 2.93  Grad : tensor([-0.0059,  0.0334])\n",
      "Epoch : 2643  Loss : 2.93  Grad : tensor([-0.0059,  0.0333])\n",
      "Epoch : 2644  Loss : 2.93  Grad : tensor([-0.0059,  0.0333])\n",
      "Epoch : 2645  Loss : 2.93  Grad : tensor([-0.0059,  0.0332])\n",
      "Epoch : 2646  Loss : 2.93  Grad : tensor([-0.0059,  0.0332])\n",
      "Epoch : 2647  Loss : 2.93  Grad : tensor([-0.0059,  0.0331])\n",
      "Epoch : 2648  Loss : 2.93  Grad : tensor([-0.0059,  0.0331])\n",
      "Epoch : 2649  Loss : 2.93  Grad : tensor([-0.0058,  0.0330])\n",
      "Epoch : 2650  Loss : 2.93  Grad : tensor([-0.0058,  0.0330])\n",
      "Epoch : 2651  Loss : 2.93  Grad : tensor([-0.0058,  0.0329])\n",
      "Epoch : 2652  Loss : 2.93  Grad : tensor([-0.0058,  0.0328])\n",
      "Epoch : 2653  Loss : 2.93  Grad : tensor([-0.0058,  0.0328])\n",
      "Epoch : 2654  Loss : 2.93  Grad : tensor([-0.0058,  0.0327])\n",
      "Epoch : 2655  Loss : 2.93  Grad : tensor([-0.0058,  0.0327])\n",
      "Epoch : 2656  Loss : 2.93  Grad : tensor([-0.0058,  0.0326])\n",
      "Epoch : 2657  Loss : 2.93  Grad : tensor([-0.0057,  0.0326])\n",
      "Epoch : 2658  Loss : 2.93  Grad : tensor([-0.0057,  0.0325])\n",
      "Epoch : 2659  Loss : 2.93  Grad : tensor([-0.0057,  0.0325])\n",
      "Epoch : 2660  Loss : 2.93  Grad : tensor([-0.0057,  0.0324])\n",
      "Epoch : 2661  Loss : 2.93  Grad : tensor([-0.0057,  0.0323])\n",
      "Epoch : 2662  Loss : 2.93  Grad : tensor([-0.0057,  0.0323])\n",
      "Epoch : 2663  Loss : 2.93  Grad : tensor([-0.0057,  0.0322])\n",
      "Epoch : 2664  Loss : 2.93  Grad : tensor([-0.0057,  0.0322])\n",
      "Epoch : 2665  Loss : 2.93  Grad : tensor([-0.0057,  0.0321])\n",
      "Epoch : 2666  Loss : 2.93  Grad : tensor([-0.0057,  0.0321])\n",
      "Epoch : 2667  Loss : 2.93  Grad : tensor([-0.0057,  0.0320])\n",
      "Epoch : 2668  Loss : 2.93  Grad : tensor([-0.0056,  0.0320])\n",
      "Epoch : 2669  Loss : 2.93  Grad : tensor([-0.0056,  0.0319])\n",
      "Epoch : 2670  Loss : 2.93  Grad : tensor([-0.0056,  0.0319])\n",
      "Epoch : 2671  Loss : 2.93  Grad : tensor([-0.0056,  0.0318])\n",
      "Epoch : 2672  Loss : 2.93  Grad : tensor([-0.0056,  0.0317])\n",
      "Epoch : 2673  Loss : 2.93  Grad : tensor([-0.0056,  0.0317])\n",
      "Epoch : 2674  Loss : 2.93  Grad : tensor([-0.0056,  0.0316])\n",
      "Epoch : 2675  Loss : 2.93  Grad : tensor([-0.0056,  0.0316])\n",
      "Epoch : 2676  Loss : 2.93  Grad : tensor([-0.0056,  0.0315])\n",
      "Epoch : 2677  Loss : 2.93  Grad : tensor([-0.0056,  0.0315])\n",
      "Epoch : 2678  Loss : 2.93  Grad : tensor([-0.0055,  0.0314])\n",
      "Epoch : 2679  Loss : 2.93  Grad : tensor([-0.0055,  0.0314])\n",
      "Epoch : 2680  Loss : 2.93  Grad : tensor([-0.0055,  0.0313])\n",
      "Epoch : 2681  Loss : 2.93  Grad : tensor([-0.0055,  0.0313])\n",
      "Epoch : 2682  Loss : 2.93  Grad : tensor([-0.0055,  0.0312])\n",
      "Epoch : 2683  Loss : 2.93  Grad : tensor([-0.0055,  0.0312])\n",
      "Epoch : 2684  Loss : 2.93  Grad : tensor([-0.0055,  0.0311])\n",
      "Epoch : 2685  Loss : 2.93  Grad : tensor([-0.0055,  0.0310])\n",
      "Epoch : 2686  Loss : 2.93  Grad : tensor([-0.0055,  0.0310])\n",
      "Epoch : 2687  Loss : 2.93  Grad : tensor([-0.0055,  0.0309])\n",
      "Epoch : 2688  Loss : 2.93  Grad : tensor([-0.0055,  0.0309])\n",
      "Epoch : 2689  Loss : 2.93  Grad : tensor([-0.0055,  0.0308])\n",
      "Epoch : 2690  Loss : 2.93  Grad : tensor([-0.0054,  0.0308])\n",
      "Epoch : 2691  Loss : 2.93  Grad : tensor([-0.0054,  0.0307])\n",
      "Epoch : 2692  Loss : 2.93  Grad : tensor([-0.0054,  0.0307])\n",
      "Epoch : 2693  Loss : 2.93  Grad : tensor([-0.0054,  0.0306])\n",
      "Epoch : 2694  Loss : 2.93  Grad : tensor([-0.0054,  0.0306])\n",
      "Epoch : 2695  Loss : 2.93  Grad : tensor([-0.0054,  0.0305])\n",
      "Epoch : 2696  Loss : 2.93  Grad : tensor([-0.0054,  0.0305])\n",
      "Epoch : 2697  Loss : 2.93  Grad : tensor([-0.0054,  0.0304])\n",
      "Epoch : 2698  Loss : 2.93  Grad : tensor([-0.0054,  0.0304])\n",
      "Epoch : 2699  Loss : 2.93  Grad : tensor([-0.0054,  0.0303])\n",
      "Epoch : 2700  Loss : 2.93  Grad : tensor([-0.0054,  0.0303])\n",
      "Epoch : 2701  Loss : 2.93  Grad : tensor([-0.0054,  0.0302])\n",
      "Epoch : 2702  Loss : 2.93  Grad : tensor([-0.0053,  0.0302])\n",
      "Epoch : 2703  Loss : 2.93  Grad : tensor([-0.0053,  0.0301])\n",
      "Epoch : 2704  Loss : 2.93  Grad : tensor([-0.0053,  0.0301])\n",
      "Epoch : 2705  Loss : 2.93  Grad : tensor([-0.0053,  0.0300])\n",
      "Epoch : 2706  Loss : 2.93  Grad : tensor([-0.0053,  0.0300])\n",
      "Epoch : 2707  Loss : 2.93  Grad : tensor([-0.0053,  0.0299])\n",
      "Epoch : 2708  Loss : 2.93  Grad : tensor([-0.0053,  0.0299])\n",
      "Epoch : 2709  Loss : 2.93  Grad : tensor([-0.0053,  0.0298])\n",
      "Epoch : 2710  Loss : 2.93  Grad : tensor([-0.0053,  0.0298])\n",
      "Epoch : 2711  Loss : 2.93  Grad : tensor([-0.0053,  0.0297])\n",
      "Epoch : 2712  Loss : 2.93  Grad : tensor([-0.0053,  0.0297])\n",
      "Epoch : 2713  Loss : 2.93  Grad : tensor([-0.0052,  0.0296])\n",
      "Epoch : 2714  Loss : 2.93  Grad : tensor([-0.0052,  0.0296])\n",
      "Epoch : 2715  Loss : 2.93  Grad : tensor([-0.0052,  0.0295])\n",
      "Epoch : 2716  Loss : 2.93  Grad : tensor([-0.0052,  0.0295])\n",
      "Epoch : 2717  Loss : 2.93  Grad : tensor([-0.0052,  0.0294])\n",
      "Epoch : 2718  Loss : 2.93  Grad : tensor([-0.0052,  0.0294])\n",
      "Epoch : 2719  Loss : 2.93  Grad : tensor([-0.0052,  0.0293])\n",
      "Epoch : 2720  Loss : 2.93  Grad : tensor([-0.0052,  0.0293])\n",
      "Epoch : 2721  Loss : 2.93  Grad : tensor([-0.0052,  0.0292])\n",
      "Epoch : 2722  Loss : 2.93  Grad : tensor([-0.0052,  0.0292])\n",
      "Epoch : 2723  Loss : 2.93  Grad : tensor([-0.0051,  0.0291])\n",
      "Epoch : 2724  Loss : 2.93  Grad : tensor([-0.0051,  0.0291])\n",
      "Epoch : 2725  Loss : 2.93  Grad : tensor([-0.0051,  0.0290])\n",
      "Epoch : 2726  Loss : 2.93  Grad : tensor([-0.0051,  0.0290])\n",
      "Epoch : 2727  Loss : 2.93  Grad : tensor([-0.0051,  0.0289])\n",
      "Epoch : 2728  Loss : 2.93  Grad : tensor([-0.0051,  0.0289])\n",
      "Epoch : 2729  Loss : 2.93  Grad : tensor([-0.0051,  0.0288])\n",
      "Epoch : 2730  Loss : 2.93  Grad : tensor([-0.0051,  0.0288])\n",
      "Epoch : 2731  Loss : 2.93  Grad : tensor([-0.0051,  0.0287])\n",
      "Epoch : 2732  Loss : 2.93  Grad : tensor([-0.0051,  0.0287])\n",
      "Epoch : 2733  Loss : 2.93  Grad : tensor([-0.0050,  0.0286])\n",
      "Epoch : 2734  Loss : 2.93  Grad : tensor([-0.0050,  0.0286])\n",
      "Epoch : 2735  Loss : 2.93  Grad : tensor([-0.0050,  0.0285])\n",
      "Epoch : 2736  Loss : 2.93  Grad : tensor([-0.0051,  0.0285])\n",
      "Epoch : 2737  Loss : 2.93  Grad : tensor([-0.0050,  0.0284])\n",
      "Epoch : 2738  Loss : 2.93  Grad : tensor([-0.0050,  0.0284])\n",
      "Epoch : 2739  Loss : 2.93  Grad : tensor([-0.0050,  0.0283])\n",
      "Epoch : 2740  Loss : 2.93  Grad : tensor([-0.0050,  0.0283])\n",
      "Epoch : 2741  Loss : 2.93  Grad : tensor([-0.0050,  0.0282])\n",
      "Epoch : 2742  Loss : 2.93  Grad : tensor([-0.0050,  0.0282])\n",
      "Epoch : 2743  Loss : 2.93  Grad : tensor([-0.0050,  0.0281])\n",
      "Epoch : 2744  Loss : 2.93  Grad : tensor([-0.0050,  0.0281])\n",
      "Epoch : 2745  Loss : 2.93  Grad : tensor([-0.0050,  0.0280])\n",
      "Epoch : 2746  Loss : 2.93  Grad : tensor([-0.0050,  0.0280])\n",
      "Epoch : 2747  Loss : 2.93  Grad : tensor([-0.0049,  0.0279])\n",
      "Epoch : 2748  Loss : 2.93  Grad : tensor([-0.0049,  0.0279])\n",
      "Epoch : 2749  Loss : 2.93  Grad : tensor([-0.0049,  0.0279])\n",
      "Epoch : 2750  Loss : 2.93  Grad : tensor([-0.0049,  0.0278])\n",
      "Epoch : 2751  Loss : 2.93  Grad : tensor([-0.0049,  0.0278])\n",
      "Epoch : 2752  Loss : 2.93  Grad : tensor([-0.0049,  0.0277])\n",
      "Epoch : 2753  Loss : 2.93  Grad : tensor([-0.0049,  0.0277])\n",
      "Epoch : 2754  Loss : 2.93  Grad : tensor([-0.0049,  0.0276])\n",
      "Epoch : 2755  Loss : 2.93  Grad : tensor([-0.0049,  0.0276])\n",
      "Epoch : 2756  Loss : 2.93  Grad : tensor([-0.0049,  0.0275])\n",
      "Epoch : 2757  Loss : 2.93  Grad : tensor([-0.0049,  0.0275])\n",
      "Epoch : 2758  Loss : 2.93  Grad : tensor([-0.0049,  0.0274])\n",
      "Epoch : 2759  Loss : 2.93  Grad : tensor([-0.0048,  0.0274])\n",
      "Epoch : 2760  Loss : 2.93  Grad : tensor([-0.0049,  0.0273])\n",
      "Epoch : 2761  Loss : 2.93  Grad : tensor([-0.0048,  0.0273])\n",
      "Epoch : 2762  Loss : 2.93  Grad : tensor([-0.0048,  0.0272])\n",
      "Epoch : 2763  Loss : 2.93  Grad : tensor([-0.0048,  0.0272])\n",
      "Epoch : 2764  Loss : 2.93  Grad : tensor([-0.0048,  0.0271])\n",
      "Epoch : 2765  Loss : 2.93  Grad : tensor([-0.0048,  0.0271])\n",
      "Epoch : 2766  Loss : 2.93  Grad : tensor([-0.0048,  0.0271])\n",
      "Epoch : 2767  Loss : 2.93  Grad : tensor([-0.0048,  0.0270])\n",
      "Epoch : 2768  Loss : 2.93  Grad : tensor([-0.0048,  0.0270])\n",
      "Epoch : 2769  Loss : 2.93  Grad : tensor([-0.0048,  0.0269])\n",
      "Epoch : 2770  Loss : 2.93  Grad : tensor([-0.0047,  0.0269])\n",
      "Epoch : 2771  Loss : 2.93  Grad : tensor([-0.0047,  0.0268])\n",
      "Epoch : 2772  Loss : 2.93  Grad : tensor([-0.0047,  0.0268])\n",
      "Epoch : 2773  Loss : 2.93  Grad : tensor([-0.0047,  0.0267])\n",
      "Epoch : 2774  Loss : 2.93  Grad : tensor([-0.0047,  0.0267])\n",
      "Epoch : 2775  Loss : 2.93  Grad : tensor([-0.0047,  0.0266])\n",
      "Epoch : 2776  Loss : 2.93  Grad : tensor([-0.0047,  0.0266])\n",
      "Epoch : 2777  Loss : 2.93  Grad : tensor([-0.0047,  0.0266])\n",
      "Epoch : 2778  Loss : 2.93  Grad : tensor([-0.0047,  0.0265])\n",
      "Epoch : 2779  Loss : 2.93  Grad : tensor([-0.0047,  0.0265])\n",
      "Epoch : 2780  Loss : 2.93  Grad : tensor([-0.0047,  0.0264])\n",
      "Epoch : 2781  Loss : 2.93  Grad : tensor([-0.0047,  0.0264])\n",
      "Epoch : 2782  Loss : 2.93  Grad : tensor([-0.0046,  0.0263])\n",
      "Epoch : 2783  Loss : 2.93  Grad : tensor([-0.0046,  0.0263])\n",
      "Epoch : 2784  Loss : 2.93  Grad : tensor([-0.0046,  0.0262])\n",
      "Epoch : 2785  Loss : 2.93  Grad : tensor([-0.0047,  0.0262])\n",
      "Epoch : 2786  Loss : 2.93  Grad : tensor([-0.0046,  0.0262])\n",
      "Epoch : 2787  Loss : 2.93  Grad : tensor([-0.0046,  0.0261])\n",
      "Epoch : 2788  Loss : 2.93  Grad : tensor([-0.0046,  0.0261])\n",
      "Epoch : 2789  Loss : 2.93  Grad : tensor([-0.0046,  0.0260])\n",
      "Epoch : 2790  Loss : 2.93  Grad : tensor([-0.0046,  0.0260])\n",
      "Epoch : 2791  Loss : 2.93  Grad : tensor([-0.0046,  0.0259])\n",
      "Epoch : 2792  Loss : 2.93  Grad : tensor([-0.0046,  0.0259])\n",
      "Epoch : 2793  Loss : 2.93  Grad : tensor([-0.0046,  0.0258])\n",
      "Epoch : 2794  Loss : 2.93  Grad : tensor([-0.0046,  0.0258])\n",
      "Epoch : 2795  Loss : 2.93  Grad : tensor([-0.0045,  0.0258])\n",
      "Epoch : 2796  Loss : 2.93  Grad : tensor([-0.0045,  0.0257])\n",
      "Epoch : 2797  Loss : 2.93  Grad : tensor([-0.0045,  0.0257])\n",
      "Epoch : 2798  Loss : 2.93  Grad : tensor([-0.0045,  0.0256])\n",
      "Epoch : 2799  Loss : 2.93  Grad : tensor([-0.0045,  0.0256])\n",
      "Epoch : 2800  Loss : 2.93  Grad : tensor([-0.0045,  0.0255])\n",
      "Epoch : 2801  Loss : 2.93  Grad : tensor([-0.0045,  0.0255])\n",
      "Epoch : 2802  Loss : 2.93  Grad : tensor([-0.0045,  0.0254])\n",
      "Epoch : 2803  Loss : 2.93  Grad : tensor([-0.0045,  0.0254])\n",
      "Epoch : 2804  Loss : 2.93  Grad : tensor([-0.0045,  0.0254])\n",
      "Epoch : 2805  Loss : 2.93  Grad : tensor([-0.0045,  0.0253])\n",
      "Epoch : 2806  Loss : 2.93  Grad : tensor([-0.0045,  0.0253])\n",
      "Epoch : 2807  Loss : 2.93  Grad : tensor([-0.0045,  0.0252])\n",
      "Epoch : 2808  Loss : 2.93  Grad : tensor([-0.0044,  0.0252])\n",
      "Epoch : 2809  Loss : 2.93  Grad : tensor([-0.0044,  0.0251])\n",
      "Epoch : 2810  Loss : 2.93  Grad : tensor([-0.0044,  0.0251])\n",
      "Epoch : 2811  Loss : 2.93  Grad : tensor([-0.0044,  0.0251])\n",
      "Epoch : 2812  Loss : 2.93  Grad : tensor([-0.0044,  0.0250])\n",
      "Epoch : 2813  Loss : 2.93  Grad : tensor([-0.0044,  0.0250])\n",
      "Epoch : 2814  Loss : 2.93  Grad : tensor([-0.0044,  0.0249])\n",
      "Epoch : 2815  Loss : 2.93  Grad : tensor([-0.0044,  0.0249])\n",
      "Epoch : 2816  Loss : 2.93  Grad : tensor([-0.0044,  0.0249])\n",
      "Epoch : 2817  Loss : 2.93  Grad : tensor([-0.0044,  0.0248])\n",
      "Epoch : 2818  Loss : 2.93  Grad : tensor([-0.0043,  0.0248])\n",
      "Epoch : 2819  Loss : 2.93  Grad : tensor([-0.0044,  0.0247])\n",
      "Epoch : 2820  Loss : 2.93  Grad : tensor([-0.0044,  0.0247])\n",
      "Epoch : 2821  Loss : 2.93  Grad : tensor([-0.0044,  0.0246])\n",
      "Epoch : 2822  Loss : 2.93  Grad : tensor([-0.0043,  0.0246])\n",
      "Epoch : 2823  Loss : 2.93  Grad : tensor([-0.0043,  0.0246])\n",
      "Epoch : 2824  Loss : 2.93  Grad : tensor([-0.0043,  0.0245])\n",
      "Epoch : 2825  Loss : 2.93  Grad : tensor([-0.0043,  0.0245])\n",
      "Epoch : 2826  Loss : 2.93  Grad : tensor([-0.0043,  0.0244])\n",
      "Epoch : 2827  Loss : 2.93  Grad : tensor([-0.0043,  0.0244])\n",
      "Epoch : 2828  Loss : 2.93  Grad : tensor([-0.0043,  0.0243])\n",
      "Epoch : 2829  Loss : 2.93  Grad : tensor([-0.0043,  0.0243])\n",
      "Epoch : 2830  Loss : 2.93  Grad : tensor([-0.0043,  0.0243])\n",
      "Epoch : 2831  Loss : 2.93  Grad : tensor([-0.0043,  0.0242])\n",
      "Epoch : 2832  Loss : 2.93  Grad : tensor([-0.0043,  0.0242])\n",
      "Epoch : 2833  Loss : 2.93  Grad : tensor([-0.0043,  0.0241])\n",
      "Epoch : 2834  Loss : 2.93  Grad : tensor([-0.0043,  0.0241])\n",
      "Epoch : 2835  Loss : 2.93  Grad : tensor([-0.0043,  0.0241])\n",
      "Epoch : 2836  Loss : 2.93  Grad : tensor([-0.0042,  0.0240])\n",
      "Epoch : 2837  Loss : 2.93  Grad : tensor([-0.0042,  0.0240])\n",
      "Epoch : 2838  Loss : 2.93  Grad : tensor([-0.0042,  0.0239])\n",
      "Epoch : 2839  Loss : 2.93  Grad : tensor([-0.0042,  0.0239])\n",
      "Epoch : 2840  Loss : 2.93  Grad : tensor([-0.0042,  0.0239])\n",
      "Epoch : 2841  Loss : 2.93  Grad : tensor([-0.0042,  0.0238])\n",
      "Epoch : 2842  Loss : 2.93  Grad : tensor([-0.0042,  0.0238])\n",
      "Epoch : 2843  Loss : 2.93  Grad : tensor([-0.0042,  0.0237])\n",
      "Epoch : 2844  Loss : 2.93  Grad : tensor([-0.0042,  0.0237])\n",
      "Epoch : 2845  Loss : 2.93  Grad : tensor([-0.0042,  0.0237])\n",
      "Epoch : 2846  Loss : 2.93  Grad : tensor([-0.0042,  0.0236])\n",
      "Epoch : 2847  Loss : 2.93  Grad : tensor([-0.0042,  0.0236])\n",
      "Epoch : 2848  Loss : 2.93  Grad : tensor([-0.0042,  0.0235])\n",
      "Epoch : 2849  Loss : 2.93  Grad : tensor([-0.0041,  0.0235])\n",
      "Epoch : 2850  Loss : 2.93  Grad : tensor([-0.0041,  0.0235])\n",
      "Epoch : 2851  Loss : 2.93  Grad : tensor([-0.0041,  0.0234])\n",
      "Epoch : 2852  Loss : 2.93  Grad : tensor([-0.0041,  0.0234])\n",
      "Epoch : 2853  Loss : 2.93  Grad : tensor([-0.0041,  0.0233])\n",
      "Epoch : 2854  Loss : 2.93  Grad : tensor([-0.0041,  0.0233])\n",
      "Epoch : 2855  Loss : 2.93  Grad : tensor([-0.0041,  0.0233])\n",
      "Epoch : 2856  Loss : 2.93  Grad : tensor([-0.0041,  0.0232])\n",
      "Epoch : 2857  Loss : 2.93  Grad : tensor([-0.0041,  0.0232])\n",
      "Epoch : 2858  Loss : 2.93  Grad : tensor([-0.0041,  0.0231])\n",
      "Epoch : 2859  Loss : 2.93  Grad : tensor([-0.0041,  0.0231])\n",
      "Epoch : 2860  Loss : 2.93  Grad : tensor([-0.0041,  0.0231])\n",
      "Epoch : 2861  Loss : 2.93  Grad : tensor([-0.0041,  0.0230])\n",
      "Epoch : 2862  Loss : 2.93  Grad : tensor([-0.0041,  0.0230])\n",
      "Epoch : 2863  Loss : 2.93  Grad : tensor([-0.0040,  0.0229])\n",
      "Epoch : 2864  Loss : 2.93  Grad : tensor([-0.0040,  0.0229])\n",
      "Epoch : 2865  Loss : 2.93  Grad : tensor([-0.0040,  0.0229])\n",
      "Epoch : 2866  Loss : 2.93  Grad : tensor([-0.0040,  0.0228])\n",
      "Epoch : 2867  Loss : 2.93  Grad : tensor([-0.0040,  0.0228])\n",
      "Epoch : 2868  Loss : 2.93  Grad : tensor([-0.0040,  0.0227])\n",
      "Epoch : 2869  Loss : 2.93  Grad : tensor([-0.0040,  0.0227])\n",
      "Epoch : 2870  Loss : 2.93  Grad : tensor([-0.0040,  0.0227])\n",
      "Epoch : 2871  Loss : 2.93  Grad : tensor([-0.0040,  0.0226])\n",
      "Epoch : 2872  Loss : 2.93  Grad : tensor([-0.0040,  0.0226])\n",
      "Epoch : 2873  Loss : 2.93  Grad : tensor([-0.0040,  0.0226])\n",
      "Epoch : 2874  Loss : 2.93  Grad : tensor([-0.0040,  0.0225])\n",
      "Epoch : 2875  Loss : 2.93  Grad : tensor([-0.0040,  0.0225])\n",
      "Epoch : 2876  Loss : 2.93  Grad : tensor([-0.0040,  0.0224])\n",
      "Epoch : 2877  Loss : 2.93  Grad : tensor([-0.0040,  0.0224])\n",
      "Epoch : 2878  Loss : 2.93  Grad : tensor([-0.0040,  0.0224])\n",
      "Epoch : 2879  Loss : 2.93  Grad : tensor([-0.0039,  0.0223])\n",
      "Epoch : 2880  Loss : 2.93  Grad : tensor([-0.0039,  0.0223])\n",
      "Epoch : 2881  Loss : 2.93  Grad : tensor([-0.0039,  0.0223])\n",
      "Epoch : 2882  Loss : 2.93  Grad : tensor([-0.0039,  0.0222])\n",
      "Epoch : 2883  Loss : 2.93  Grad : tensor([-0.0039,  0.0222])\n",
      "Epoch : 2884  Loss : 2.93  Grad : tensor([-0.0039,  0.0221])\n",
      "Epoch : 2885  Loss : 2.93  Grad : tensor([-0.0039,  0.0221])\n",
      "Epoch : 2886  Loss : 2.93  Grad : tensor([-0.0039,  0.0221])\n",
      "Epoch : 2887  Loss : 2.93  Grad : tensor([-0.0039,  0.0220])\n",
      "Epoch : 2888  Loss : 2.93  Grad : tensor([-0.0039,  0.0220])\n",
      "Epoch : 2889  Loss : 2.93  Grad : tensor([-0.0039,  0.0220])\n",
      "Epoch : 2890  Loss : 2.93  Grad : tensor([-0.0039,  0.0219])\n",
      "Epoch : 2891  Loss : 2.93  Grad : tensor([-0.0039,  0.0219])\n",
      "Epoch : 2892  Loss : 2.93  Grad : tensor([-0.0039,  0.0218])\n",
      "Epoch : 2893  Loss : 2.93  Grad : tensor([-0.0039,  0.0218])\n",
      "Epoch : 2894  Loss : 2.93  Grad : tensor([-0.0038,  0.0218])\n",
      "Epoch : 2895  Loss : 2.93  Grad : tensor([-0.0038,  0.0217])\n",
      "Epoch : 2896  Loss : 2.93  Grad : tensor([-0.0038,  0.0217])\n",
      "Epoch : 2897  Loss : 2.93  Grad : tensor([-0.0038,  0.0217])\n",
      "Epoch : 2898  Loss : 2.93  Grad : tensor([-0.0038,  0.0216])\n",
      "Epoch : 2899  Loss : 2.93  Grad : tensor([-0.0038,  0.0216])\n",
      "Epoch : 2900  Loss : 2.93  Grad : tensor([-0.0038,  0.0215])\n",
      "Epoch : 2901  Loss : 2.93  Grad : tensor([-0.0038,  0.0215])\n",
      "Epoch : 2902  Loss : 2.93  Grad : tensor([-0.0038,  0.0215])\n",
      "Epoch : 2903  Loss : 2.93  Grad : tensor([-0.0038,  0.0214])\n",
      "Epoch : 2904  Loss : 2.93  Grad : tensor([-0.0038,  0.0214])\n",
      "Epoch : 2905  Loss : 2.93  Grad : tensor([-0.0038,  0.0214])\n",
      "Epoch : 2906  Loss : 2.93  Grad : tensor([-0.0038,  0.0213])\n",
      "Epoch : 2907  Loss : 2.93  Grad : tensor([-0.0038,  0.0213])\n",
      "Epoch : 2908  Loss : 2.93  Grad : tensor([-0.0037,  0.0213])\n",
      "Epoch : 2909  Loss : 2.93  Grad : tensor([-0.0037,  0.0212])\n",
      "Epoch : 2910  Loss : 2.93  Grad : tensor([-0.0037,  0.0212])\n",
      "Epoch : 2911  Loss : 2.93  Grad : tensor([-0.0037,  0.0211])\n",
      "Epoch : 2912  Loss : 2.93  Grad : tensor([-0.0037,  0.0211])\n",
      "Epoch : 2913  Loss : 2.93  Grad : tensor([-0.0037,  0.0211])\n",
      "Epoch : 2914  Loss : 2.93  Grad : tensor([-0.0037,  0.0210])\n",
      "Epoch : 2915  Loss : 2.93  Grad : tensor([-0.0037,  0.0210])\n",
      "Epoch : 2916  Loss : 2.93  Grad : tensor([-0.0037,  0.0210])\n",
      "Epoch : 2917  Loss : 2.93  Grad : tensor([-0.0037,  0.0209])\n",
      "Epoch : 2918  Loss : 2.93  Grad : tensor([-0.0037,  0.0209])\n",
      "Epoch : 2919  Loss : 2.93  Grad : tensor([-0.0037,  0.0209])\n",
      "Epoch : 2920  Loss : 2.93  Grad : tensor([-0.0037,  0.0208])\n",
      "Epoch : 2921  Loss : 2.93  Grad : tensor([-0.0037,  0.0208])\n",
      "Epoch : 2922  Loss : 2.93  Grad : tensor([-0.0037,  0.0208])\n",
      "Epoch : 2923  Loss : 2.93  Grad : tensor([-0.0036,  0.0207])\n",
      "Epoch : 2924  Loss : 2.93  Grad : tensor([-0.0037,  0.0207])\n",
      "Epoch : 2925  Loss : 2.93  Grad : tensor([-0.0036,  0.0206])\n",
      "Epoch : 2926  Loss : 2.93  Grad : tensor([-0.0036,  0.0206])\n",
      "Epoch : 2927  Loss : 2.93  Grad : tensor([-0.0036,  0.0206])\n",
      "Epoch : 2928  Loss : 2.93  Grad : tensor([-0.0036,  0.0205])\n",
      "Epoch : 2929  Loss : 2.93  Grad : tensor([-0.0036,  0.0205])\n",
      "Epoch : 2930  Loss : 2.93  Grad : tensor([-0.0036,  0.0205])\n",
      "Epoch : 2931  Loss : 2.93  Grad : tensor([-0.0036,  0.0204])\n",
      "Epoch : 2932  Loss : 2.93  Grad : tensor([-0.0036,  0.0204])\n",
      "Epoch : 2933  Loss : 2.93  Grad : tensor([-0.0036,  0.0204])\n",
      "Epoch : 2934  Loss : 2.93  Grad : tensor([-0.0036,  0.0203])\n",
      "Epoch : 2935  Loss : 2.93  Grad : tensor([-0.0036,  0.0203])\n",
      "Epoch : 2936  Loss : 2.93  Grad : tensor([-0.0036,  0.0203])\n",
      "Epoch : 2937  Loss : 2.93  Grad : tensor([-0.0036,  0.0202])\n",
      "Epoch : 2938  Loss : 2.93  Grad : tensor([-0.0035,  0.0202])\n",
      "Epoch : 2939  Loss : 2.93  Grad : tensor([-0.0036,  0.0202])\n",
      "Epoch : 2940  Loss : 2.93  Grad : tensor([-0.0036,  0.0201])\n",
      "Epoch : 2941  Loss : 2.93  Grad : tensor([-0.0035,  0.0201])\n",
      "Epoch : 2942  Loss : 2.93  Grad : tensor([-0.0035,  0.0201])\n",
      "Epoch : 2943  Loss : 2.93  Grad : tensor([-0.0035,  0.0200])\n",
      "Epoch : 2944  Loss : 2.93  Grad : tensor([-0.0035,  0.0200])\n",
      "Epoch : 2945  Loss : 2.93  Grad : tensor([-0.0035,  0.0200])\n",
      "Epoch : 2946  Loss : 2.93  Grad : tensor([-0.0035,  0.0199])\n",
      "Epoch : 2947  Loss : 2.93  Grad : tensor([-0.0035,  0.0199])\n",
      "Epoch : 2948  Loss : 2.93  Grad : tensor([-0.0035,  0.0199])\n",
      "Epoch : 2949  Loss : 2.93  Grad : tensor([-0.0035,  0.0198])\n",
      "Epoch : 2950  Loss : 2.93  Grad : tensor([-0.0035,  0.0198])\n",
      "Epoch : 2951  Loss : 2.93  Grad : tensor([-0.0035,  0.0198])\n",
      "Epoch : 2952  Loss : 2.93  Grad : tensor([-0.0035,  0.0197])\n",
      "Epoch : 2953  Loss : 2.93  Grad : tensor([-0.0035,  0.0197])\n",
      "Epoch : 2954  Loss : 2.93  Grad : tensor([-0.0035,  0.0197])\n",
      "Epoch : 2955  Loss : 2.93  Grad : tensor([-0.0035,  0.0196])\n",
      "Epoch : 2956  Loss : 2.93  Grad : tensor([-0.0035,  0.0196])\n",
      "Epoch : 2957  Loss : 2.93  Grad : tensor([-0.0034,  0.0196])\n",
      "Epoch : 2958  Loss : 2.93  Grad : tensor([-0.0035,  0.0195])\n",
      "Epoch : 2959  Loss : 2.93  Grad : tensor([-0.0034,  0.0195])\n",
      "Epoch : 2960  Loss : 2.93  Grad : tensor([-0.0034,  0.0195])\n",
      "Epoch : 2961  Loss : 2.93  Grad : tensor([-0.0034,  0.0194])\n",
      "Epoch : 2962  Loss : 2.93  Grad : tensor([-0.0034,  0.0194])\n",
      "Epoch : 2963  Loss : 2.93  Grad : tensor([-0.0034,  0.0194])\n",
      "Epoch : 2964  Loss : 2.93  Grad : tensor([-0.0034,  0.0193])\n",
      "Epoch : 2965  Loss : 2.93  Grad : tensor([-0.0034,  0.0193])\n",
      "Epoch : 2966  Loss : 2.93  Grad : tensor([-0.0034,  0.0193])\n",
      "Epoch : 2967  Loss : 2.93  Grad : tensor([-0.0034,  0.0192])\n",
      "Epoch : 2968  Loss : 2.93  Grad : tensor([-0.0034,  0.0192])\n",
      "Epoch : 2969  Loss : 2.93  Grad : tensor([-0.0034,  0.0192])\n",
      "Epoch : 2970  Loss : 2.93  Grad : tensor([-0.0034,  0.0191])\n",
      "Epoch : 2971  Loss : 2.93  Grad : tensor([-0.0034,  0.0191])\n",
      "Epoch : 2972  Loss : 2.93  Grad : tensor([-0.0034,  0.0191])\n",
      "Epoch : 2973  Loss : 2.93  Grad : tensor([-0.0034,  0.0190])\n",
      "Epoch : 2974  Loss : 2.93  Grad : tensor([-0.0034,  0.0190])\n",
      "Epoch : 2975  Loss : 2.93  Grad : tensor([-0.0034,  0.0190])\n",
      "Epoch : 2976  Loss : 2.93  Grad : tensor([-0.0033,  0.0189])\n",
      "Epoch : 2977  Loss : 2.93  Grad : tensor([-0.0033,  0.0189])\n",
      "Epoch : 2978  Loss : 2.93  Grad : tensor([-0.0033,  0.0189])\n",
      "Epoch : 2979  Loss : 2.93  Grad : tensor([-0.0033,  0.0188])\n",
      "Epoch : 2980  Loss : 2.93  Grad : tensor([-0.0033,  0.0188])\n",
      "Epoch : 2981  Loss : 2.93  Grad : tensor([-0.0033,  0.0188])\n",
      "Epoch : 2982  Loss : 2.93  Grad : tensor([-0.0033,  0.0187])\n",
      "Epoch : 2983  Loss : 2.93  Grad : tensor([-0.0033,  0.0187])\n",
      "Epoch : 2984  Loss : 2.93  Grad : tensor([-0.0033,  0.0187])\n",
      "Epoch : 2985  Loss : 2.93  Grad : tensor([-0.0033,  0.0186])\n",
      "Epoch : 2986  Loss : 2.93  Grad : tensor([-0.0033,  0.0186])\n",
      "Epoch : 2987  Loss : 2.93  Grad : tensor([-0.0033,  0.0186])\n",
      "Epoch : 2988  Loss : 2.93  Grad : tensor([-0.0033,  0.0186])\n",
      "Epoch : 2989  Loss : 2.93  Grad : tensor([-0.0033,  0.0185])\n",
      "Epoch : 2990  Loss : 2.93  Grad : tensor([-0.0033,  0.0185])\n",
      "Epoch : 2991  Loss : 2.93  Grad : tensor([-0.0032,  0.0185])\n",
      "Epoch : 2992  Loss : 2.93  Grad : tensor([-0.0033,  0.0184])\n",
      "Epoch : 2993  Loss : 2.93  Grad : tensor([-0.0033,  0.0184])\n",
      "Epoch : 2994  Loss : 2.93  Grad : tensor([-0.0033,  0.0184])\n",
      "Epoch : 2995  Loss : 2.93  Grad : tensor([-0.0032,  0.0183])\n",
      "Epoch : 2996  Loss : 2.93  Grad : tensor([-0.0032,  0.0183])\n",
      "Epoch : 2997  Loss : 2.93  Grad : tensor([-0.0032,  0.0183])\n",
      "Epoch : 2998  Loss : 2.93  Grad : tensor([-0.0032,  0.0182])\n",
      "Epoch : 2999  Loss : 2.93  Grad : tensor([-0.0032,  0.0182])\n",
      "Epoch : 3000  Loss : 2.93  Grad : tensor([-0.0032,  0.0182])\n",
      "Epoch : 3001  Loss : 2.93  Grad : tensor([-0.0032,  0.0181])\n",
      "Epoch : 3002  Loss : 2.93  Grad : tensor([-0.0032,  0.0181])\n",
      "Epoch : 3003  Loss : 2.93  Grad : tensor([-0.0032,  0.0181])\n",
      "Epoch : 3004  Loss : 2.93  Grad : tensor([-0.0032,  0.0181])\n",
      "Epoch : 3005  Loss : 2.93  Grad : tensor([-0.0032,  0.0180])\n",
      "Epoch : 3006  Loss : 2.93  Grad : tensor([-0.0032,  0.0180])\n",
      "Epoch : 3007  Loss : 2.93  Grad : tensor([-0.0032,  0.0180])\n",
      "Epoch : 3008  Loss : 2.93  Grad : tensor([-0.0032,  0.0179])\n",
      "Epoch : 3009  Loss : 2.93  Grad : tensor([-0.0032,  0.0179])\n",
      "Epoch : 3010  Loss : 2.93  Grad : tensor([-0.0032,  0.0179])\n",
      "Epoch : 3011  Loss : 2.93  Grad : tensor([-0.0032,  0.0178])\n",
      "Epoch : 3012  Loss : 2.93  Grad : tensor([-0.0032,  0.0178])\n",
      "Epoch : 3013  Loss : 2.93  Grad : tensor([-0.0031,  0.0178])\n",
      "Epoch : 3014  Loss : 2.93  Grad : tensor([-0.0031,  0.0177])\n",
      "Epoch : 3015  Loss : 2.93  Grad : tensor([-0.0031,  0.0177])\n",
      "Epoch : 3016  Loss : 2.93  Grad : tensor([-0.0031,  0.0177])\n",
      "Epoch : 3017  Loss : 2.93  Grad : tensor([-0.0031,  0.0177])\n",
      "Epoch : 3018  Loss : 2.93  Grad : tensor([-0.0031,  0.0176])\n",
      "Epoch : 3019  Loss : 2.93  Grad : tensor([-0.0031,  0.0176])\n",
      "Epoch : 3020  Loss : 2.93  Grad : tensor([-0.0031,  0.0176])\n",
      "Epoch : 3021  Loss : 2.93  Grad : tensor([-0.0031,  0.0175])\n",
      "Epoch : 3022  Loss : 2.93  Grad : tensor([-0.0031,  0.0175])\n",
      "Epoch : 3023  Loss : 2.93  Grad : tensor([-0.0031,  0.0175])\n",
      "Epoch : 3024  Loss : 2.93  Grad : tensor([-0.0031,  0.0175])\n",
      "Epoch : 3025  Loss : 2.93  Grad : tensor([-0.0031,  0.0174])\n",
      "Epoch : 3026  Loss : 2.93  Grad : tensor([-0.0031,  0.0174])\n",
      "Epoch : 3027  Loss : 2.93  Grad : tensor([-0.0030,  0.0174])\n",
      "Epoch : 3028  Loss : 2.93  Grad : tensor([-0.0031,  0.0173])\n",
      "Epoch : 3029  Loss : 2.93  Grad : tensor([-0.0031,  0.0173])\n",
      "Epoch : 3030  Loss : 2.93  Grad : tensor([-0.0031,  0.0173])\n",
      "Epoch : 3031  Loss : 2.93  Grad : tensor([-0.0031,  0.0172])\n",
      "Epoch : 3032  Loss : 2.93  Grad : tensor([-0.0030,  0.0172])\n",
      "Epoch : 3033  Loss : 2.93  Grad : tensor([-0.0030,  0.0172])\n",
      "Epoch : 3034  Loss : 2.93  Grad : tensor([-0.0030,  0.0172])\n",
      "Epoch : 3035  Loss : 2.93  Grad : tensor([-0.0030,  0.0171])\n",
      "Epoch : 3036  Loss : 2.93  Grad : tensor([-0.0030,  0.0171])\n",
      "Epoch : 3037  Loss : 2.93  Grad : tensor([-0.0030,  0.0171])\n",
      "Epoch : 3038  Loss : 2.93  Grad : tensor([-0.0030,  0.0170])\n",
      "Epoch : 3039  Loss : 2.93  Grad : tensor([-0.0030,  0.0170])\n",
      "Epoch : 3040  Loss : 2.93  Grad : tensor([-0.0030,  0.0170])\n",
      "Epoch : 3041  Loss : 2.93  Grad : tensor([-0.0030,  0.0170])\n",
      "Epoch : 3042  Loss : 2.93  Grad : tensor([-0.0030,  0.0169])\n",
      "Epoch : 3043  Loss : 2.93  Grad : tensor([-0.0030,  0.0169])\n",
      "Epoch : 3044  Loss : 2.93  Grad : tensor([-0.0030,  0.0169])\n",
      "Epoch : 3045  Loss : 2.93  Grad : tensor([-0.0030,  0.0168])\n",
      "Epoch : 3046  Loss : 2.93  Grad : tensor([-0.0030,  0.0168])\n",
      "Epoch : 3047  Loss : 2.93  Grad : tensor([-0.0030,  0.0168])\n",
      "Epoch : 3048  Loss : 2.93  Grad : tensor([-0.0030,  0.0168])\n",
      "Epoch : 3049  Loss : 2.93  Grad : tensor([-0.0030,  0.0167])\n",
      "Epoch : 3050  Loss : 2.93  Grad : tensor([-0.0030,  0.0167])\n",
      "Epoch : 3051  Loss : 2.93  Grad : tensor([-0.0030,  0.0167])\n",
      "Epoch : 3052  Loss : 2.93  Grad : tensor([-0.0029,  0.0166])\n",
      "Epoch : 3053  Loss : 2.93  Grad : tensor([-0.0029,  0.0166])\n",
      "Epoch : 3054  Loss : 2.93  Grad : tensor([-0.0029,  0.0166])\n",
      "Epoch : 3055  Loss : 2.93  Grad : tensor([-0.0029,  0.0165])\n",
      "Epoch : 3056  Loss : 2.93  Grad : tensor([-0.0029,  0.0165])\n",
      "Epoch : 3057  Loss : 2.93  Grad : tensor([-0.0029,  0.0165])\n",
      "Epoch : 3058  Loss : 2.93  Grad : tensor([-0.0029,  0.0165])\n",
      "Epoch : 3059  Loss : 2.93  Grad : tensor([-0.0029,  0.0164])\n",
      "Epoch : 3060  Loss : 2.93  Grad : tensor([-0.0029,  0.0164])\n",
      "Epoch : 3061  Loss : 2.93  Grad : tensor([-0.0029,  0.0164])\n",
      "Epoch : 3062  Loss : 2.93  Grad : tensor([-0.0029,  0.0164])\n",
      "Epoch : 3063  Loss : 2.93  Grad : tensor([-0.0029,  0.0163])\n",
      "Epoch : 3064  Loss : 2.93  Grad : tensor([-0.0029,  0.0163])\n",
      "Epoch : 3065  Loss : 2.93  Grad : tensor([-0.0029,  0.0163])\n",
      "Epoch : 3066  Loss : 2.93  Grad : tensor([-0.0029,  0.0162])\n",
      "Epoch : 3067  Loss : 2.93  Grad : tensor([-0.0029,  0.0162])\n",
      "Epoch : 3068  Loss : 2.93  Grad : tensor([-0.0029,  0.0162])\n",
      "Epoch : 3069  Loss : 2.93  Grad : tensor([-0.0029,  0.0162])\n",
      "Epoch : 3070  Loss : 2.93  Grad : tensor([-0.0029,  0.0161])\n",
      "Epoch : 3071  Loss : 2.93  Grad : tensor([-0.0029,  0.0161])\n",
      "Epoch : 3072  Loss : 2.93  Grad : tensor([-0.0028,  0.0161])\n",
      "Epoch : 3073  Loss : 2.93  Grad : tensor([-0.0028,  0.0161])\n",
      "Epoch : 3074  Loss : 2.93  Grad : tensor([-0.0028,  0.0160])\n",
      "Epoch : 3075  Loss : 2.93  Grad : tensor([-0.0028,  0.0160])\n",
      "Epoch : 3076  Loss : 2.93  Grad : tensor([-0.0028,  0.0160])\n",
      "Epoch : 3077  Loss : 2.93  Grad : tensor([-0.0028,  0.0159])\n",
      "Epoch : 3078  Loss : 2.93  Grad : tensor([-0.0028,  0.0159])\n",
      "Epoch : 3079  Loss : 2.93  Grad : tensor([-0.0028,  0.0159])\n",
      "Epoch : 3080  Loss : 2.93  Grad : tensor([-0.0028,  0.0159])\n",
      "Epoch : 3081  Loss : 2.93  Grad : tensor([-0.0028,  0.0158])\n",
      "Epoch : 3082  Loss : 2.93  Grad : tensor([-0.0028,  0.0158])\n",
      "Epoch : 3083  Loss : 2.93  Grad : tensor([-0.0028,  0.0158])\n",
      "Epoch : 3084  Loss : 2.93  Grad : tensor([-0.0028,  0.0158])\n",
      "Epoch : 3085  Loss : 2.93  Grad : tensor([-0.0028,  0.0157])\n",
      "Epoch : 3086  Loss : 2.93  Grad : tensor([-0.0028,  0.0157])\n",
      "Epoch : 3087  Loss : 2.93  Grad : tensor([-0.0027,  0.0157])\n",
      "Epoch : 3088  Loss : 2.93  Grad : tensor([-0.0027,  0.0157])\n",
      "Epoch : 3089  Loss : 2.93  Grad : tensor([-0.0027,  0.0156])\n",
      "Epoch : 3090  Loss : 2.93  Grad : tensor([-0.0028,  0.0156])\n",
      "Epoch : 3091  Loss : 2.93  Grad : tensor([-0.0028,  0.0156])\n",
      "Epoch : 3092  Loss : 2.93  Grad : tensor([-0.0027,  0.0155])\n",
      "Epoch : 3093  Loss : 2.93  Grad : tensor([-0.0027,  0.0155])\n",
      "Epoch : 3094  Loss : 2.93  Grad : tensor([-0.0027,  0.0155])\n",
      "Epoch : 3095  Loss : 2.93  Grad : tensor([-0.0027,  0.0155])\n",
      "Epoch : 3096  Loss : 2.93  Grad : tensor([-0.0027,  0.0154])\n",
      "Epoch : 3097  Loss : 2.93  Grad : tensor([-0.0027,  0.0154])\n",
      "Epoch : 3098  Loss : 2.93  Grad : tensor([-0.0027,  0.0154])\n",
      "Epoch : 3099  Loss : 2.93  Grad : tensor([-0.0027,  0.0154])\n",
      "Epoch : 3100  Loss : 2.93  Grad : tensor([-0.0027,  0.0153])\n",
      "Epoch : 3101  Loss : 2.93  Grad : tensor([-0.0027,  0.0153])\n",
      "Epoch : 3102  Loss : 2.93  Grad : tensor([-0.0027,  0.0153])\n",
      "Epoch : 3103  Loss : 2.93  Grad : tensor([-0.0027,  0.0153])\n",
      "Epoch : 3104  Loss : 2.93  Grad : tensor([-0.0027,  0.0152])\n",
      "Epoch : 3105  Loss : 2.93  Grad : tensor([-0.0027,  0.0152])\n",
      "Epoch : 3106  Loss : 2.93  Grad : tensor([-0.0027,  0.0152])\n",
      "Epoch : 3107  Loss : 2.93  Grad : tensor([-0.0027,  0.0152])\n",
      "Epoch : 3108  Loss : 2.93  Grad : tensor([-0.0027,  0.0151])\n",
      "Epoch : 3109  Loss : 2.93  Grad : tensor([-0.0027,  0.0151])\n",
      "Epoch : 3110  Loss : 2.93  Grad : tensor([-0.0027,  0.0151])\n",
      "Epoch : 3111  Loss : 2.93  Grad : tensor([-0.0027,  0.0151])\n",
      "Epoch : 3112  Loss : 2.93  Grad : tensor([-0.0027,  0.0150])\n",
      "Epoch : 3113  Loss : 2.93  Grad : tensor([-0.0026,  0.0150])\n",
      "Epoch : 3114  Loss : 2.93  Grad : tensor([-0.0027,  0.0150])\n",
      "Epoch : 3115  Loss : 2.93  Grad : tensor([-0.0026,  0.0149])\n",
      "Epoch : 3116  Loss : 2.93  Grad : tensor([-0.0026,  0.0149])\n",
      "Epoch : 3117  Loss : 2.93  Grad : tensor([-0.0026,  0.0149])\n",
      "Epoch : 3118  Loss : 2.93  Grad : tensor([-0.0026,  0.0149])\n",
      "Epoch : 3119  Loss : 2.93  Grad : tensor([-0.0026,  0.0148])\n",
      "Epoch : 3120  Loss : 2.93  Grad : tensor([-0.0026,  0.0148])\n",
      "Epoch : 3121  Loss : 2.93  Grad : tensor([-0.0026,  0.0148])\n",
      "Epoch : 3122  Loss : 2.93  Grad : tensor([-0.0026,  0.0148])\n",
      "Epoch : 3123  Loss : 2.93  Grad : tensor([-0.0026,  0.0147])\n",
      "Epoch : 3124  Loss : 2.93  Grad : tensor([-0.0026,  0.0147])\n",
      "Epoch : 3125  Loss : 2.93  Grad : tensor([-0.0026,  0.0147])\n",
      "Epoch : 3126  Loss : 2.93  Grad : tensor([-0.0026,  0.0147])\n",
      "Epoch : 3127  Loss : 2.93  Grad : tensor([-0.0026,  0.0146])\n",
      "Epoch : 3128  Loss : 2.93  Grad : tensor([-0.0026,  0.0146])\n",
      "Epoch : 3129  Loss : 2.93  Grad : tensor([-0.0026,  0.0146])\n",
      "Epoch : 3130  Loss : 2.93  Grad : tensor([-0.0026,  0.0146])\n",
      "Epoch : 3131  Loss : 2.93  Grad : tensor([-0.0026,  0.0145])\n",
      "Epoch : 3132  Loss : 2.93  Grad : tensor([-0.0026,  0.0145])\n",
      "Epoch : 3133  Loss : 2.93  Grad : tensor([-0.0025,  0.0145])\n",
      "Epoch : 3134  Loss : 2.93  Grad : tensor([-0.0026,  0.0145])\n",
      "Epoch : 3135  Loss : 2.93  Grad : tensor([-0.0026,  0.0144])\n",
      "Epoch : 3136  Loss : 2.93  Grad : tensor([-0.0025,  0.0144])\n",
      "Epoch : 3137  Loss : 2.93  Grad : tensor([-0.0026,  0.0144])\n",
      "Epoch : 3138  Loss : 2.93  Grad : tensor([-0.0025,  0.0144])\n",
      "Epoch : 3139  Loss : 2.93  Grad : tensor([-0.0025,  0.0144])\n",
      "Epoch : 3140  Loss : 2.93  Grad : tensor([-0.0025,  0.0143])\n",
      "Epoch : 3141  Loss : 2.93  Grad : tensor([-0.0025,  0.0143])\n",
      "Epoch : 3142  Loss : 2.93  Grad : tensor([-0.0025,  0.0143])\n",
      "Epoch : 3143  Loss : 2.93  Grad : tensor([-0.0025,  0.0143])\n",
      "Epoch : 3144  Loss : 2.93  Grad : tensor([-0.0025,  0.0142])\n",
      "Epoch : 3145  Loss : 2.93  Grad : tensor([-0.0025,  0.0142])\n",
      "Epoch : 3146  Loss : 2.93  Grad : tensor([-0.0025,  0.0142])\n",
      "Epoch : 3147  Loss : 2.93  Grad : tensor([-0.0025,  0.0142])\n",
      "Epoch : 3148  Loss : 2.93  Grad : tensor([-0.0025,  0.0141])\n",
      "Epoch : 3149  Loss : 2.93  Grad : tensor([-0.0025,  0.0141])\n",
      "Epoch : 3150  Loss : 2.93  Grad : tensor([-0.0025,  0.0141])\n",
      "Epoch : 3151  Loss : 2.93  Grad : tensor([-0.0025,  0.0141])\n",
      "Epoch : 3152  Loss : 2.93  Grad : tensor([-0.0025,  0.0140])\n",
      "Epoch : 3153  Loss : 2.93  Grad : tensor([-0.0025,  0.0140])\n",
      "Epoch : 3154  Loss : 2.93  Grad : tensor([-0.0025,  0.0140])\n",
      "Epoch : 3155  Loss : 2.93  Grad : tensor([-0.0025,  0.0140])\n",
      "Epoch : 3156  Loss : 2.93  Grad : tensor([-0.0024,  0.0139])\n",
      "Epoch : 3157  Loss : 2.93  Grad : tensor([-0.0025,  0.0139])\n",
      "Epoch : 3158  Loss : 2.93  Grad : tensor([-0.0024,  0.0139])\n",
      "Epoch : 3159  Loss : 2.93  Grad : tensor([-0.0025,  0.0139])\n",
      "Epoch : 3160  Loss : 2.93  Grad : tensor([-0.0024,  0.0138])\n",
      "Epoch : 3161  Loss : 2.93  Grad : tensor([-0.0025,  0.0138])\n",
      "Epoch : 3162  Loss : 2.93  Grad : tensor([-0.0024,  0.0138])\n",
      "Epoch : 3163  Loss : 2.93  Grad : tensor([-0.0024,  0.0138])\n",
      "Epoch : 3164  Loss : 2.93  Grad : tensor([-0.0024,  0.0138])\n",
      "Epoch : 3165  Loss : 2.93  Grad : tensor([-0.0024,  0.0137])\n",
      "Epoch : 3166  Loss : 2.93  Grad : tensor([-0.0024,  0.0137])\n",
      "Epoch : 3167  Loss : 2.93  Grad : tensor([-0.0024,  0.0137])\n",
      "Epoch : 3168  Loss : 2.93  Grad : tensor([-0.0024,  0.0137])\n",
      "Epoch : 3169  Loss : 2.93  Grad : tensor([-0.0024,  0.0136])\n",
      "Epoch : 3170  Loss : 2.93  Grad : tensor([-0.0024,  0.0136])\n",
      "Epoch : 3171  Loss : 2.93  Grad : tensor([-0.0024,  0.0136])\n",
      "Epoch : 3172  Loss : 2.93  Grad : tensor([-0.0024,  0.0136])\n",
      "Epoch : 3173  Loss : 2.93  Grad : tensor([-0.0024,  0.0135])\n",
      "Epoch : 3174  Loss : 2.93  Grad : tensor([-0.0024,  0.0135])\n",
      "Epoch : 3175  Loss : 2.93  Grad : tensor([-0.0024,  0.0135])\n",
      "Epoch : 3176  Loss : 2.93  Grad : tensor([-0.0024,  0.0135])\n",
      "Epoch : 3177  Loss : 2.93  Grad : tensor([-0.0024,  0.0134])\n",
      "Epoch : 3178  Loss : 2.93  Grad : tensor([-0.0024,  0.0134])\n",
      "Epoch : 3179  Loss : 2.93  Grad : tensor([-0.0024,  0.0134])\n",
      "Epoch : 3180  Loss : 2.93  Grad : tensor([-0.0024,  0.0134])\n",
      "Epoch : 3181  Loss : 2.93  Grad : tensor([-0.0023,  0.0134])\n",
      "Epoch : 3182  Loss : 2.93  Grad : tensor([-0.0023,  0.0133])\n",
      "Epoch : 3183  Loss : 2.93  Grad : tensor([-0.0024,  0.0133])\n",
      "Epoch : 3184  Loss : 2.93  Grad : tensor([-0.0023,  0.0133])\n",
      "Epoch : 3185  Loss : 2.93  Grad : tensor([-0.0024,  0.0133])\n",
      "Epoch : 3186  Loss : 2.93  Grad : tensor([-0.0024,  0.0132])\n",
      "Epoch : 3187  Loss : 2.93  Grad : tensor([-0.0023,  0.0132])\n",
      "Epoch : 3188  Loss : 2.93  Grad : tensor([-0.0023,  0.0132])\n",
      "Epoch : 3189  Loss : 2.93  Grad : tensor([-0.0023,  0.0132])\n",
      "Epoch : 3190  Loss : 2.93  Grad : tensor([-0.0023,  0.0132])\n",
      "Epoch : 3191  Loss : 2.93  Grad : tensor([-0.0023,  0.0131])\n",
      "Epoch : 3192  Loss : 2.93  Grad : tensor([-0.0023,  0.0131])\n",
      "Epoch : 3193  Loss : 2.93  Grad : tensor([-0.0023,  0.0131])\n",
      "Epoch : 3194  Loss : 2.93  Grad : tensor([-0.0023,  0.0131])\n",
      "Epoch : 3195  Loss : 2.93  Grad : tensor([-0.0023,  0.0130])\n",
      "Epoch : 3196  Loss : 2.93  Grad : tensor([-0.0023,  0.0130])\n",
      "Epoch : 3197  Loss : 2.93  Grad : tensor([-0.0023,  0.0130])\n",
      "Epoch : 3198  Loss : 2.93  Grad : tensor([-0.0023,  0.0130])\n",
      "Epoch : 3199  Loss : 2.93  Grad : tensor([-0.0023,  0.0130])\n",
      "Epoch : 3200  Loss : 2.93  Grad : tensor([-0.0023,  0.0129])\n",
      "Epoch : 3201  Loss : 2.93  Grad : tensor([-0.0023,  0.0129])\n",
      "Epoch : 3202  Loss : 2.93  Grad : tensor([-0.0023,  0.0129])\n",
      "Epoch : 3203  Loss : 2.93  Grad : tensor([-0.0023,  0.0129])\n",
      "Epoch : 3204  Loss : 2.93  Grad : tensor([-0.0022,  0.0129])\n",
      "Epoch : 3205  Loss : 2.93  Grad : tensor([-0.0023,  0.0128])\n",
      "Epoch : 3206  Loss : 2.93  Grad : tensor([-0.0023,  0.0128])\n",
      "Epoch : 3207  Loss : 2.93  Grad : tensor([-0.0023,  0.0128])\n",
      "Epoch : 3208  Loss : 2.93  Grad : tensor([-0.0022,  0.0128])\n",
      "Epoch : 3209  Loss : 2.93  Grad : tensor([-0.0022,  0.0127])\n",
      "Epoch : 3210  Loss : 2.93  Grad : tensor([-0.0023,  0.0127])\n",
      "Epoch : 3211  Loss : 2.93  Grad : tensor([-0.0023,  0.0127])\n",
      "Epoch : 3212  Loss : 2.93  Grad : tensor([-0.0023,  0.0127])\n",
      "Epoch : 3213  Loss : 2.93  Grad : tensor([-0.0022,  0.0127])\n",
      "Epoch : 3214  Loss : 2.93  Grad : tensor([-0.0022,  0.0126])\n",
      "Epoch : 3215  Loss : 2.93  Grad : tensor([-0.0022,  0.0126])\n",
      "Epoch : 3216  Loss : 2.93  Grad : tensor([-0.0022,  0.0126])\n",
      "Epoch : 3217  Loss : 2.93  Grad : tensor([-0.0022,  0.0126])\n",
      "Epoch : 3218  Loss : 2.93  Grad : tensor([-0.0022,  0.0125])\n",
      "Epoch : 3219  Loss : 2.93  Grad : tensor([-0.0022,  0.0125])\n",
      "Epoch : 3220  Loss : 2.93  Grad : tensor([-0.0022,  0.0125])\n",
      "Epoch : 3221  Loss : 2.93  Grad : tensor([-0.0022,  0.0125])\n",
      "Epoch : 3222  Loss : 2.93  Grad : tensor([-0.0022,  0.0125])\n",
      "Epoch : 3223  Loss : 2.93  Grad : tensor([-0.0022,  0.0124])\n",
      "Epoch : 3224  Loss : 2.93  Grad : tensor([-0.0022,  0.0124])\n",
      "Epoch : 3225  Loss : 2.93  Grad : tensor([-0.0022,  0.0124])\n",
      "Epoch : 3226  Loss : 2.93  Grad : tensor([-0.0022,  0.0124])\n",
      "Epoch : 3227  Loss : 2.93  Grad : tensor([-0.0022,  0.0124])\n",
      "Epoch : 3228  Loss : 2.93  Grad : tensor([-0.0022,  0.0123])\n",
      "Epoch : 3229  Loss : 2.93  Grad : tensor([-0.0022,  0.0123])\n",
      "Epoch : 3230  Loss : 2.93  Grad : tensor([-0.0022,  0.0123])\n",
      "Epoch : 3231  Loss : 2.93  Grad : tensor([-0.0022,  0.0123])\n",
      "Epoch : 3232  Loss : 2.93  Grad : tensor([-0.0021,  0.0123])\n",
      "Epoch : 3233  Loss : 2.93  Grad : tensor([-0.0022,  0.0122])\n",
      "Epoch : 3234  Loss : 2.93  Grad : tensor([-0.0022,  0.0122])\n",
      "Epoch : 3235  Loss : 2.93  Grad : tensor([-0.0022,  0.0122])\n",
      "Epoch : 3236  Loss : 2.93  Grad : tensor([-0.0022,  0.0122])\n",
      "Epoch : 3237  Loss : 2.93  Grad : tensor([-0.0022,  0.0121])\n",
      "Epoch : 3238  Loss : 2.93  Grad : tensor([-0.0022,  0.0121])\n",
      "Epoch : 3239  Loss : 2.93  Grad : tensor([-0.0022,  0.0121])\n",
      "Epoch : 3240  Loss : 2.93  Grad : tensor([-0.0021,  0.0121])\n",
      "Epoch : 3241  Loss : 2.93  Grad : tensor([-0.0021,  0.0121])\n",
      "Epoch : 3242  Loss : 2.93  Grad : tensor([-0.0021,  0.0120])\n",
      "Epoch : 3243  Loss : 2.93  Grad : tensor([-0.0021,  0.0120])\n",
      "Epoch : 3244  Loss : 2.93  Grad : tensor([-0.0021,  0.0120])\n",
      "Epoch : 3245  Loss : 2.93  Grad : tensor([-0.0021,  0.0120])\n",
      "Epoch : 3246  Loss : 2.93  Grad : tensor([-0.0021,  0.0120])\n",
      "Epoch : 3247  Loss : 2.93  Grad : tensor([-0.0021,  0.0119])\n",
      "Epoch : 3248  Loss : 2.93  Grad : tensor([-0.0021,  0.0119])\n",
      "Epoch : 3249  Loss : 2.93  Grad : tensor([-0.0021,  0.0119])\n",
      "Epoch : 3250  Loss : 2.93  Grad : tensor([-0.0021,  0.0119])\n",
      "Epoch : 3251  Loss : 2.93  Grad : tensor([-0.0021,  0.0119])\n",
      "Epoch : 3252  Loss : 2.93  Grad : tensor([-0.0021,  0.0118])\n",
      "Epoch : 3253  Loss : 2.93  Grad : tensor([-0.0021,  0.0118])\n",
      "Epoch : 3254  Loss : 2.93  Grad : tensor([-0.0021,  0.0118])\n",
      "Epoch : 3255  Loss : 2.93  Grad : tensor([-0.0021,  0.0118])\n",
      "Epoch : 3256  Loss : 2.93  Grad : tensor([-0.0021,  0.0118])\n",
      "Epoch : 3257  Loss : 2.93  Grad : tensor([-0.0021,  0.0117])\n",
      "Epoch : 3258  Loss : 2.93  Grad : tensor([-0.0021,  0.0117])\n",
      "Epoch : 3259  Loss : 2.93  Grad : tensor([-0.0021,  0.0117])\n",
      "Epoch : 3260  Loss : 2.93  Grad : tensor([-0.0021,  0.0117])\n",
      "Epoch : 3261  Loss : 2.93  Grad : tensor([-0.0021,  0.0117])\n",
      "Epoch : 3262  Loss : 2.93  Grad : tensor([-0.0021,  0.0116])\n",
      "Epoch : 3263  Loss : 2.93  Grad : tensor([-0.0021,  0.0116])\n",
      "Epoch : 3264  Loss : 2.93  Grad : tensor([-0.0021,  0.0116])\n",
      "Epoch : 3265  Loss : 2.93  Grad : tensor([-0.0021,  0.0116])\n",
      "Epoch : 3266  Loss : 2.93  Grad : tensor([-0.0021,  0.0116])\n",
      "Epoch : 3267  Loss : 2.93  Grad : tensor([-0.0021,  0.0115])\n",
      "Epoch : 3268  Loss : 2.93  Grad : tensor([-0.0021,  0.0115])\n",
      "Epoch : 3269  Loss : 2.93  Grad : tensor([-0.0020,  0.0115])\n",
      "Epoch : 3270  Loss : 2.93  Grad : tensor([-0.0020,  0.0115])\n",
      "Epoch : 3271  Loss : 2.93  Grad : tensor([-0.0020,  0.0115])\n",
      "Epoch : 3272  Loss : 2.93  Grad : tensor([-0.0020,  0.0115])\n",
      "Epoch : 3273  Loss : 2.93  Grad : tensor([-0.0020,  0.0114])\n",
      "Epoch : 3274  Loss : 2.93  Grad : tensor([-0.0020,  0.0114])\n",
      "Epoch : 3275  Loss : 2.93  Grad : tensor([-0.0020,  0.0114])\n",
      "Epoch : 3276  Loss : 2.93  Grad : tensor([-0.0020,  0.0114])\n",
      "Epoch : 3277  Loss : 2.93  Grad : tensor([-0.0020,  0.0113])\n",
      "Epoch : 3278  Loss : 2.93  Grad : tensor([-0.0020,  0.0113])\n",
      "Epoch : 3279  Loss : 2.93  Grad : tensor([-0.0020,  0.0113])\n",
      "Epoch : 3280  Loss : 2.93  Grad : tensor([-0.0020,  0.0113])\n",
      "Epoch : 3281  Loss : 2.93  Grad : tensor([-0.0020,  0.0113])\n",
      "Epoch : 3282  Loss : 2.93  Grad : tensor([-0.0020,  0.0113])\n",
      "Epoch : 3283  Loss : 2.93  Grad : tensor([-0.0020,  0.0112])\n",
      "Epoch : 3284  Loss : 2.93  Grad : tensor([-0.0020,  0.0112])\n",
      "Epoch : 3285  Loss : 2.93  Grad : tensor([-0.0020,  0.0112])\n",
      "Epoch : 3286  Loss : 2.93  Grad : tensor([-0.0020,  0.0112])\n",
      "Epoch : 3287  Loss : 2.93  Grad : tensor([-0.0020,  0.0112])\n",
      "Epoch : 3288  Loss : 2.93  Grad : tensor([-0.0020,  0.0111])\n",
      "Epoch : 3289  Loss : 2.93  Grad : tensor([-0.0020,  0.0111])\n",
      "Epoch : 3290  Loss : 2.93  Grad : tensor([-0.0020,  0.0111])\n",
      "Epoch : 3291  Loss : 2.93  Grad : tensor([-0.0020,  0.0111])\n",
      "Epoch : 3292  Loss : 2.93  Grad : tensor([-0.0020,  0.0111])\n",
      "Epoch : 3293  Loss : 2.93  Grad : tensor([-0.0020,  0.0110])\n",
      "Epoch : 3294  Loss : 2.93  Grad : tensor([-0.0020,  0.0110])\n",
      "Epoch : 3295  Loss : 2.93  Grad : tensor([-0.0020,  0.0110])\n",
      "Epoch : 3296  Loss : 2.93  Grad : tensor([-0.0019,  0.0110])\n",
      "Epoch : 3297  Loss : 2.93  Grad : tensor([-0.0019,  0.0110])\n",
      "Epoch : 3298  Loss : 2.93  Grad : tensor([-0.0019,  0.0110])\n",
      "Epoch : 3299  Loss : 2.93  Grad : tensor([-0.0019,  0.0109])\n",
      "Epoch : 3300  Loss : 2.93  Grad : tensor([-0.0019,  0.0109])\n",
      "Epoch : 3301  Loss : 2.93  Grad : tensor([-0.0019,  0.0109])\n",
      "Epoch : 3302  Loss : 2.93  Grad : tensor([-0.0019,  0.0109])\n",
      "Epoch : 3303  Loss : 2.93  Grad : tensor([-0.0019,  0.0109])\n",
      "Epoch : 3304  Loss : 2.93  Grad : tensor([-0.0019,  0.0108])\n",
      "Epoch : 3305  Loss : 2.93  Grad : tensor([-0.0019,  0.0108])\n",
      "Epoch : 3306  Loss : 2.93  Grad : tensor([-0.0019,  0.0108])\n",
      "Epoch : 3307  Loss : 2.93  Grad : tensor([-0.0019,  0.0108])\n",
      "Epoch : 3308  Loss : 2.93  Grad : tensor([-0.0019,  0.0108])\n",
      "Epoch : 3309  Loss : 2.93  Grad : tensor([-0.0019,  0.0107])\n",
      "Epoch : 3310  Loss : 2.93  Grad : tensor([-0.0019,  0.0107])\n",
      "Epoch : 3311  Loss : 2.93  Grad : tensor([-0.0019,  0.0107])\n",
      "Epoch : 3312  Loss : 2.93  Grad : tensor([-0.0019,  0.0107])\n",
      "Epoch : 3313  Loss : 2.93  Grad : tensor([-0.0019,  0.0107])\n",
      "Epoch : 3314  Loss : 2.93  Grad : tensor([-0.0019,  0.0107])\n",
      "Epoch : 3315  Loss : 2.93  Grad : tensor([-0.0019,  0.0106])\n",
      "Epoch : 3316  Loss : 2.93  Grad : tensor([-0.0019,  0.0106])\n",
      "Epoch : 3317  Loss : 2.93  Grad : tensor([-0.0019,  0.0106])\n",
      "Epoch : 3318  Loss : 2.93  Grad : tensor([-0.0019,  0.0106])\n",
      "Epoch : 3319  Loss : 2.93  Grad : tensor([-0.0019,  0.0106])\n",
      "Epoch : 3320  Loss : 2.93  Grad : tensor([-0.0018,  0.0106])\n",
      "Epoch : 3321  Loss : 2.93  Grad : tensor([-0.0018,  0.0105])\n",
      "Epoch : 3322  Loss : 2.93  Grad : tensor([-0.0018,  0.0105])\n",
      "Epoch : 3323  Loss : 2.93  Grad : tensor([-0.0018,  0.0105])\n",
      "Epoch : 3324  Loss : 2.93  Grad : tensor([-0.0018,  0.0105])\n",
      "Epoch : 3325  Loss : 2.93  Grad : tensor([-0.0018,  0.0105])\n",
      "Epoch : 3326  Loss : 2.93  Grad : tensor([-0.0018,  0.0104])\n",
      "Epoch : 3327  Loss : 2.93  Grad : tensor([-0.0019,  0.0104])\n",
      "Epoch : 3328  Loss : 2.93  Grad : tensor([-0.0018,  0.0104])\n",
      "Epoch : 3329  Loss : 2.93  Grad : tensor([-0.0018,  0.0104])\n",
      "Epoch : 3330  Loss : 2.93  Grad : tensor([-0.0018,  0.0104])\n",
      "Epoch : 3331  Loss : 2.93  Grad : tensor([-0.0018,  0.0104])\n",
      "Epoch : 3332  Loss : 2.93  Grad : tensor([-0.0018,  0.0103])\n",
      "Epoch : 3333  Loss : 2.93  Grad : tensor([-0.0018,  0.0103])\n",
      "Epoch : 3334  Loss : 2.93  Grad : tensor([-0.0018,  0.0103])\n",
      "Epoch : 3335  Loss : 2.93  Grad : tensor([-0.0018,  0.0103])\n",
      "Epoch : 3336  Loss : 2.93  Grad : tensor([-0.0018,  0.0103])\n",
      "Epoch : 3337  Loss : 2.93  Grad : tensor([-0.0018,  0.0102])\n",
      "Epoch : 3338  Loss : 2.93  Grad : tensor([-0.0018,  0.0102])\n",
      "Epoch : 3339  Loss : 2.93  Grad : tensor([-0.0018,  0.0102])\n",
      "Epoch : 3340  Loss : 2.93  Grad : tensor([-0.0018,  0.0102])\n",
      "Epoch : 3341  Loss : 2.93  Grad : tensor([-0.0018,  0.0102])\n",
      "Epoch : 3342  Loss : 2.93  Grad : tensor([-0.0018,  0.0102])\n",
      "Epoch : 3343  Loss : 2.93  Grad : tensor([-0.0018,  0.0101])\n",
      "Epoch : 3344  Loss : 2.93  Grad : tensor([-0.0018,  0.0101])\n",
      "Epoch : 3345  Loss : 2.93  Grad : tensor([-0.0018,  0.0101])\n",
      "Epoch : 3346  Loss : 2.93  Grad : tensor([-0.0018,  0.0101])\n",
      "Epoch : 3347  Loss : 2.93  Grad : tensor([-0.0018,  0.0101])\n",
      "Epoch : 3348  Loss : 2.93  Grad : tensor([-0.0018,  0.0101])\n",
      "Epoch : 3349  Loss : 2.93  Grad : tensor([-0.0018,  0.0100])\n",
      "Epoch : 3350  Loss : 2.93  Grad : tensor([-0.0018,  0.0100])\n",
      "Epoch : 3351  Loss : 2.93  Grad : tensor([-0.0018,  0.0100])\n",
      "Epoch : 3352  Loss : 2.93  Grad : tensor([-0.0018,  0.0100])\n",
      "Epoch : 3353  Loss : 2.93  Grad : tensor([-0.0017,  0.0100])\n",
      "Epoch : 3354  Loss : 2.93  Grad : tensor([-0.0018,  0.0100])\n",
      "Epoch : 3355  Loss : 2.93  Grad : tensor([-0.0017,  0.0099])\n",
      "Epoch : 3356  Loss : 2.93  Grad : tensor([-0.0017,  0.0099])\n",
      "Epoch : 3357  Loss : 2.93  Grad : tensor([-0.0018,  0.0099])\n",
      "Epoch : 3358  Loss : 2.93  Grad : tensor([-0.0017,  0.0099])\n",
      "Epoch : 3359  Loss : 2.93  Grad : tensor([-0.0017,  0.0099])\n",
      "Epoch : 3360  Loss : 2.93  Grad : tensor([-0.0018,  0.0099])\n",
      "Epoch : 3361  Loss : 2.93  Grad : tensor([-0.0017,  0.0098])\n",
      "Epoch : 3362  Loss : 2.93  Grad : tensor([-0.0017,  0.0098])\n",
      "Epoch : 3363  Loss : 2.93  Grad : tensor([-0.0018,  0.0098])\n",
      "Epoch : 3364  Loss : 2.93  Grad : tensor([-0.0017,  0.0098])\n",
      "Epoch : 3365  Loss : 2.93  Grad : tensor([-0.0017,  0.0098])\n",
      "Epoch : 3366  Loss : 2.93  Grad : tensor([-0.0017,  0.0098])\n",
      "Epoch : 3367  Loss : 2.93  Grad : tensor([-0.0017,  0.0097])\n",
      "Epoch : 3368  Loss : 2.93  Grad : tensor([-0.0017,  0.0097])\n",
      "Epoch : 3369  Loss : 2.93  Grad : tensor([-0.0017,  0.0097])\n",
      "Epoch : 3370  Loss : 2.93  Grad : tensor([-0.0017,  0.0097])\n",
      "Epoch : 3371  Loss : 2.93  Grad : tensor([-0.0017,  0.0097])\n",
      "Epoch : 3372  Loss : 2.93  Grad : tensor([-0.0017,  0.0097])\n",
      "Epoch : 3373  Loss : 2.93  Grad : tensor([-0.0017,  0.0096])\n",
      "Epoch : 3374  Loss : 2.93  Grad : tensor([-0.0017,  0.0096])\n",
      "Epoch : 3375  Loss : 2.93  Grad : tensor([-0.0017,  0.0096])\n",
      "Epoch : 3376  Loss : 2.93  Grad : tensor([-0.0017,  0.0096])\n",
      "Epoch : 3377  Loss : 2.93  Grad : tensor([-0.0017,  0.0096])\n",
      "Epoch : 3378  Loss : 2.93  Grad : tensor([-0.0017,  0.0096])\n",
      "Epoch : 3379  Loss : 2.93  Grad : tensor([-0.0017,  0.0095])\n",
      "Epoch : 3380  Loss : 2.93  Grad : tensor([-0.0017,  0.0095])\n",
      "Epoch : 3381  Loss : 2.93  Grad : tensor([-0.0017,  0.0095])\n",
      "Epoch : 3382  Loss : 2.93  Grad : tensor([-0.0017,  0.0095])\n",
      "Epoch : 3383  Loss : 2.93  Grad : tensor([-0.0017,  0.0095])\n",
      "Epoch : 3384  Loss : 2.93  Grad : tensor([-0.0017,  0.0095])\n",
      "Epoch : 3385  Loss : 2.93  Grad : tensor([-0.0017,  0.0094])\n",
      "Epoch : 3386  Loss : 2.93  Grad : tensor([-0.0017,  0.0094])\n",
      "Epoch : 3387  Loss : 2.93  Grad : tensor([-0.0017,  0.0094])\n",
      "Epoch : 3388  Loss : 2.93  Grad : tensor([-0.0016,  0.0094])\n",
      "Epoch : 3389  Loss : 2.93  Grad : tensor([-0.0017,  0.0094])\n",
      "Epoch : 3390  Loss : 2.93  Grad : tensor([-0.0016,  0.0094])\n",
      "Epoch : 3391  Loss : 2.93  Grad : tensor([-0.0017,  0.0093])\n",
      "Epoch : 3392  Loss : 2.93  Grad : tensor([-0.0016,  0.0093])\n",
      "Epoch : 3393  Loss : 2.93  Grad : tensor([-0.0016,  0.0093])\n",
      "Epoch : 3394  Loss : 2.93  Grad : tensor([-0.0016,  0.0093])\n",
      "Epoch : 3395  Loss : 2.93  Grad : tensor([-0.0016,  0.0093])\n",
      "Epoch : 3396  Loss : 2.93  Grad : tensor([-0.0017,  0.0093])\n",
      "Epoch : 3397  Loss : 2.93  Grad : tensor([-0.0016,  0.0093])\n",
      "Epoch : 3398  Loss : 2.93  Grad : tensor([-0.0017,  0.0092])\n",
      "Epoch : 3399  Loss : 2.93  Grad : tensor([-0.0016,  0.0092])\n",
      "Epoch : 3400  Loss : 2.93  Grad : tensor([-0.0016,  0.0092])\n",
      "Epoch : 3401  Loss : 2.93  Grad : tensor([-0.0016,  0.0092])\n",
      "Epoch : 3402  Loss : 2.93  Grad : tensor([-0.0016,  0.0092])\n",
      "Epoch : 3403  Loss : 2.93  Grad : tensor([-0.0016,  0.0092])\n",
      "Epoch : 3404  Loss : 2.93  Grad : tensor([-0.0016,  0.0091])\n",
      "Epoch : 3405  Loss : 2.93  Grad : tensor([-0.0016,  0.0091])\n",
      "Epoch : 3406  Loss : 2.93  Grad : tensor([-0.0016,  0.0091])\n",
      "Epoch : 3407  Loss : 2.93  Grad : tensor([-0.0016,  0.0091])\n",
      "Epoch : 3408  Loss : 2.93  Grad : tensor([-0.0016,  0.0091])\n",
      "Epoch : 3409  Loss : 2.93  Grad : tensor([-0.0016,  0.0091])\n",
      "Epoch : 3410  Loss : 2.93  Grad : tensor([-0.0016,  0.0091])\n",
      "Epoch : 3411  Loss : 2.93  Grad : tensor([-0.0016,  0.0090])\n",
      "Epoch : 3412  Loss : 2.93  Grad : tensor([-0.0016,  0.0090])\n",
      "Epoch : 3413  Loss : 2.93  Grad : tensor([-0.0016,  0.0090])\n",
      "Epoch : 3414  Loss : 2.93  Grad : tensor([-0.0016,  0.0090])\n",
      "Epoch : 3415  Loss : 2.93  Grad : tensor([-0.0016,  0.0090])\n",
      "Epoch : 3416  Loss : 2.93  Grad : tensor([-0.0016,  0.0090])\n",
      "Epoch : 3417  Loss : 2.93  Grad : tensor([-0.0016,  0.0089])\n",
      "Epoch : 3418  Loss : 2.93  Grad : tensor([-0.0016,  0.0089])\n",
      "Epoch : 3419  Loss : 2.93  Grad : tensor([-0.0016,  0.0089])\n",
      "Epoch : 3420  Loss : 2.93  Grad : tensor([-0.0016,  0.0089])\n",
      "Epoch : 3421  Loss : 2.93  Grad : tensor([-0.0016,  0.0089])\n",
      "Epoch : 3422  Loss : 2.93  Grad : tensor([-0.0016,  0.0089])\n",
      "Epoch : 3423  Loss : 2.93  Grad : tensor([-0.0016,  0.0089])\n",
      "Epoch : 3424  Loss : 2.93  Grad : tensor([-0.0015,  0.0088])\n",
      "Epoch : 3425  Loss : 2.93  Grad : tensor([-0.0015,  0.0088])\n",
      "Epoch : 3426  Loss : 2.93  Grad : tensor([-0.0016,  0.0088])\n",
      "Epoch : 3427  Loss : 2.93  Grad : tensor([-0.0015,  0.0088])\n",
      "Epoch : 3428  Loss : 2.93  Grad : tensor([-0.0016,  0.0088])\n",
      "Epoch : 3429  Loss : 2.93  Grad : tensor([-0.0015,  0.0088])\n",
      "Epoch : 3430  Loss : 2.93  Grad : tensor([-0.0016,  0.0087])\n",
      "Epoch : 3431  Loss : 2.93  Grad : tensor([-0.0015,  0.0087])\n",
      "Epoch : 3432  Loss : 2.93  Grad : tensor([-0.0015,  0.0087])\n",
      "Epoch : 3433  Loss : 2.93  Grad : tensor([-0.0015,  0.0087])\n",
      "Epoch : 3434  Loss : 2.93  Grad : tensor([-0.0016,  0.0087])\n",
      "Epoch : 3435  Loss : 2.93  Grad : tensor([-0.0016,  0.0087])\n",
      "Epoch : 3436  Loss : 2.93  Grad : tensor([-0.0015,  0.0087])\n",
      "Epoch : 3437  Loss : 2.93  Grad : tensor([-0.0015,  0.0087])\n",
      "Epoch : 3438  Loss : 2.93  Grad : tensor([-0.0015,  0.0086])\n",
      "Epoch : 3439  Loss : 2.93  Grad : tensor([-0.0015,  0.0086])\n",
      "Epoch : 3440  Loss : 2.93  Grad : tensor([-0.0015,  0.0086])\n",
      "Epoch : 3441  Loss : 2.93  Grad : tensor([-0.0015,  0.0086])\n",
      "Epoch : 3442  Loss : 2.93  Grad : tensor([-0.0015,  0.0086])\n",
      "Epoch : 3443  Loss : 2.93  Grad : tensor([-0.0015,  0.0086])\n",
      "Epoch : 3444  Loss : 2.93  Grad : tensor([-0.0015,  0.0085])\n",
      "Epoch : 3445  Loss : 2.93  Grad : tensor([-0.0015,  0.0085])\n",
      "Epoch : 3446  Loss : 2.93  Grad : tensor([-0.0015,  0.0085])\n",
      "Epoch : 3447  Loss : 2.93  Grad : tensor([-0.0015,  0.0085])\n",
      "Epoch : 3448  Loss : 2.93  Grad : tensor([-0.0015,  0.0085])\n",
      "Epoch : 3449  Loss : 2.93  Grad : tensor([-0.0015,  0.0085])\n",
      "Epoch : 3450  Loss : 2.93  Grad : tensor([-0.0015,  0.0085])\n",
      "Epoch : 3451  Loss : 2.93  Grad : tensor([-0.0015,  0.0084])\n",
      "Epoch : 3452  Loss : 2.93  Grad : tensor([-0.0015,  0.0084])\n",
      "Epoch : 3453  Loss : 2.93  Grad : tensor([-0.0015,  0.0084])\n",
      "Epoch : 3454  Loss : 2.93  Grad : tensor([-0.0015,  0.0084])\n",
      "Epoch : 3455  Loss : 2.93  Grad : tensor([-0.0015,  0.0084])\n",
      "Epoch : 3456  Loss : 2.93  Grad : tensor([-0.0015,  0.0084])\n",
      "Epoch : 3457  Loss : 2.93  Grad : tensor([-0.0015,  0.0084])\n",
      "Epoch : 3458  Loss : 2.93  Grad : tensor([-0.0015,  0.0083])\n",
      "Epoch : 3459  Loss : 2.93  Grad : tensor([-0.0015,  0.0083])\n",
      "Epoch : 3460  Loss : 2.93  Grad : tensor([-0.0015,  0.0083])\n",
      "Epoch : 3461  Loss : 2.93  Grad : tensor([-0.0015,  0.0083])\n",
      "Epoch : 3462  Loss : 2.93  Grad : tensor([-0.0015,  0.0083])\n",
      "Epoch : 3463  Loss : 2.93  Grad : tensor([-0.0015,  0.0083])\n",
      "Epoch : 3464  Loss : 2.93  Grad : tensor([-0.0015,  0.0083])\n",
      "Epoch : 3465  Loss : 2.93  Grad : tensor([-0.0014,  0.0082])\n",
      "Epoch : 3466  Loss : 2.93  Grad : tensor([-0.0015,  0.0082])\n",
      "Epoch : 3467  Loss : 2.93  Grad : tensor([-0.0014,  0.0082])\n",
      "Epoch : 3468  Loss : 2.93  Grad : tensor([-0.0014,  0.0082])\n",
      "Epoch : 3469  Loss : 2.93  Grad : tensor([-0.0015,  0.0082])\n",
      "Epoch : 3470  Loss : 2.93  Grad : tensor([-0.0015,  0.0082])\n",
      "Epoch : 3471  Loss : 2.93  Grad : tensor([-0.0014,  0.0082])\n",
      "Epoch : 3472  Loss : 2.93  Grad : tensor([-0.0014,  0.0081])\n",
      "Epoch : 3473  Loss : 2.93  Grad : tensor([-0.0015,  0.0081])\n",
      "Epoch : 3474  Loss : 2.93  Grad : tensor([-0.0015,  0.0081])\n",
      "Epoch : 3475  Loss : 2.93  Grad : tensor([-0.0014,  0.0081])\n",
      "Epoch : 3476  Loss : 2.93  Grad : tensor([-0.0014,  0.0081])\n",
      "Epoch : 3477  Loss : 2.93  Grad : tensor([-0.0014,  0.0081])\n",
      "Epoch : 3478  Loss : 2.93  Grad : tensor([-0.0014,  0.0081])\n",
      "Epoch : 3479  Loss : 2.93  Grad : tensor([-0.0014,  0.0081])\n",
      "Epoch : 3480  Loss : 2.93  Grad : tensor([-0.0014,  0.0080])\n",
      "Epoch : 3481  Loss : 2.93  Grad : tensor([-0.0014,  0.0080])\n",
      "Epoch : 3482  Loss : 2.93  Grad : tensor([-0.0014,  0.0080])\n",
      "Epoch : 3483  Loss : 2.93  Grad : tensor([-0.0014,  0.0080])\n",
      "Epoch : 3484  Loss : 2.93  Grad : tensor([-0.0014,  0.0080])\n",
      "Epoch : 3485  Loss : 2.93  Grad : tensor([-0.0014,  0.0080])\n",
      "Epoch : 3486  Loss : 2.93  Grad : tensor([-0.0014,  0.0080])\n",
      "Epoch : 3487  Loss : 2.93  Grad : tensor([-0.0014,  0.0079])\n",
      "Epoch : 3488  Loss : 2.93  Grad : tensor([-0.0014,  0.0079])\n",
      "Epoch : 3489  Loss : 2.93  Grad : tensor([-0.0014,  0.0079])\n",
      "Epoch : 3490  Loss : 2.93  Grad : tensor([-0.0014,  0.0079])\n",
      "Epoch : 3491  Loss : 2.93  Grad : tensor([-0.0014,  0.0079])\n",
      "Epoch : 3492  Loss : 2.93  Grad : tensor([-0.0014,  0.0079])\n",
      "Epoch : 3493  Loss : 2.93  Grad : tensor([-0.0014,  0.0079])\n",
      "Epoch : 3494  Loss : 2.93  Grad : tensor([-0.0014,  0.0079])\n",
      "Epoch : 3495  Loss : 2.93  Grad : tensor([-0.0014,  0.0078])\n",
      "Epoch : 3496  Loss : 2.93  Grad : tensor([-0.0014,  0.0078])\n",
      "Epoch : 3497  Loss : 2.93  Grad : tensor([-0.0014,  0.0078])\n",
      "Epoch : 3498  Loss : 2.93  Grad : tensor([-0.0014,  0.0078])\n",
      "Epoch : 3499  Loss : 2.93  Grad : tensor([-0.0014,  0.0078])\n",
      "Epoch : 3500  Loss : 2.93  Grad : tensor([-0.0014,  0.0078])\n",
      "Epoch : 3501  Loss : 2.93  Grad : tensor([-0.0014,  0.0078])\n",
      "Epoch : 3502  Loss : 2.93  Grad : tensor([-0.0014,  0.0077])\n",
      "Epoch : 3503  Loss : 2.93  Grad : tensor([-0.0014,  0.0077])\n",
      "Epoch : 3504  Loss : 2.93  Grad : tensor([-0.0014,  0.0077])\n",
      "Epoch : 3505  Loss : 2.93  Grad : tensor([-0.0014,  0.0077])\n",
      "Epoch : 3506  Loss : 2.93  Grad : tensor([-0.0014,  0.0077])\n",
      "Epoch : 3507  Loss : 2.93  Grad : tensor([-0.0014,  0.0077])\n",
      "Epoch : 3508  Loss : 2.93  Grad : tensor([-0.0013,  0.0077])\n",
      "Epoch : 3509  Loss : 2.93  Grad : tensor([-0.0013,  0.0077])\n",
      "Epoch : 3510  Loss : 2.93  Grad : tensor([-0.0013,  0.0076])\n",
      "Epoch : 3511  Loss : 2.93  Grad : tensor([-0.0014,  0.0076])\n",
      "Epoch : 3512  Loss : 2.93  Grad : tensor([-0.0014,  0.0076])\n",
      "Epoch : 3513  Loss : 2.93  Grad : tensor([-0.0013,  0.0076])\n",
      "Epoch : 3514  Loss : 2.93  Grad : tensor([-0.0013,  0.0076])\n",
      "Epoch : 3515  Loss : 2.93  Grad : tensor([-0.0014,  0.0076])\n",
      "Epoch : 3516  Loss : 2.93  Grad : tensor([-0.0014,  0.0076])\n",
      "Epoch : 3517  Loss : 2.93  Grad : tensor([-0.0014,  0.0075])\n",
      "Epoch : 3518  Loss : 2.93  Grad : tensor([-0.0014,  0.0075])\n",
      "Epoch : 3519  Loss : 2.93  Grad : tensor([-0.0013,  0.0075])\n",
      "Epoch : 3520  Loss : 2.93  Grad : tensor([-0.0013,  0.0075])\n",
      "Epoch : 3521  Loss : 2.93  Grad : tensor([-0.0013,  0.0075])\n",
      "Epoch : 3522  Loss : 2.93  Grad : tensor([-0.0013,  0.0075])\n",
      "Epoch : 3523  Loss : 2.93  Grad : tensor([-0.0013,  0.0075])\n",
      "Epoch : 3524  Loss : 2.93  Grad : tensor([-0.0013,  0.0075])\n",
      "Epoch : 3525  Loss : 2.93  Grad : tensor([-0.0013,  0.0074])\n",
      "Epoch : 3526  Loss : 2.93  Grad : tensor([-0.0013,  0.0074])\n",
      "Epoch : 3527  Loss : 2.93  Grad : tensor([-0.0013,  0.0074])\n",
      "Epoch : 3528  Loss : 2.93  Grad : tensor([-0.0013,  0.0074])\n",
      "Epoch : 3529  Loss : 2.93  Grad : tensor([-0.0013,  0.0074])\n",
      "Epoch : 3530  Loss : 2.93  Grad : tensor([-0.0013,  0.0074])\n",
      "Epoch : 3531  Loss : 2.93  Grad : tensor([-0.0013,  0.0074])\n",
      "Epoch : 3532  Loss : 2.93  Grad : tensor([-0.0013,  0.0074])\n",
      "Epoch : 3533  Loss : 2.93  Grad : tensor([-0.0013,  0.0073])\n",
      "Epoch : 3534  Loss : 2.93  Grad : tensor([-0.0013,  0.0073])\n",
      "Epoch : 3535  Loss : 2.93  Grad : tensor([-0.0013,  0.0073])\n",
      "Epoch : 3536  Loss : 2.93  Grad : tensor([-0.0013,  0.0073])\n",
      "Epoch : 3537  Loss : 2.93  Grad : tensor([-0.0013,  0.0073])\n",
      "Epoch : 3538  Loss : 2.93  Grad : tensor([-0.0013,  0.0073])\n",
      "Epoch : 3539  Loss : 2.93  Grad : tensor([-0.0013,  0.0073])\n",
      "Epoch : 3540  Loss : 2.93  Grad : tensor([-0.0013,  0.0073])\n",
      "Epoch : 3541  Loss : 2.93  Grad : tensor([-0.0013,  0.0073])\n",
      "Epoch : 3542  Loss : 2.93  Grad : tensor([-0.0013,  0.0072])\n",
      "Epoch : 3543  Loss : 2.93  Grad : tensor([-0.0013,  0.0072])\n",
      "Epoch : 3544  Loss : 2.93  Grad : tensor([-0.0013,  0.0072])\n",
      "Epoch : 3545  Loss : 2.93  Grad : tensor([-0.0013,  0.0072])\n",
      "Epoch : 3546  Loss : 2.93  Grad : tensor([-0.0013,  0.0072])\n",
      "Epoch : 3547  Loss : 2.93  Grad : tensor([-0.0013,  0.0072])\n",
      "Epoch : 3548  Loss : 2.93  Grad : tensor([-0.0013,  0.0072])\n",
      "Epoch : 3549  Loss : 2.93  Grad : tensor([-0.0013,  0.0071])\n",
      "Epoch : 3550  Loss : 2.93  Grad : tensor([-0.0013,  0.0071])\n",
      "Epoch : 3551  Loss : 2.93  Grad : tensor([-0.0012,  0.0071])\n",
      "Epoch : 3552  Loss : 2.93  Grad : tensor([-0.0012,  0.0071])\n",
      "Epoch : 3553  Loss : 2.93  Grad : tensor([-0.0012,  0.0071])\n",
      "Epoch : 3554  Loss : 2.93  Grad : tensor([-0.0012,  0.0071])\n",
      "Epoch : 3555  Loss : 2.93  Grad : tensor([-0.0013,  0.0071])\n",
      "Epoch : 3556  Loss : 2.93  Grad : tensor([-0.0013,  0.0071])\n",
      "Epoch : 3557  Loss : 2.93  Grad : tensor([-0.0013,  0.0071])\n",
      "Epoch : 3558  Loss : 2.93  Grad : tensor([-0.0013,  0.0070])\n",
      "Epoch : 3559  Loss : 2.93  Grad : tensor([-0.0013,  0.0070])\n",
      "Epoch : 3560  Loss : 2.93  Grad : tensor([-0.0013,  0.0070])\n",
      "Epoch : 3561  Loss : 2.93  Grad : tensor([-0.0013,  0.0070])\n",
      "Epoch : 3562  Loss : 2.93  Grad : tensor([-0.0013,  0.0070])\n",
      "Epoch : 3563  Loss : 2.93  Grad : tensor([-0.0013,  0.0070])\n",
      "Epoch : 3564  Loss : 2.93  Grad : tensor([-0.0013,  0.0070])\n",
      "Epoch : 3565  Loss : 2.93  Grad : tensor([-0.0012,  0.0070])\n",
      "Epoch : 3566  Loss : 2.93  Grad : tensor([-0.0012,  0.0069])\n",
      "Epoch : 3567  Loss : 2.93  Grad : tensor([-0.0012,  0.0069])\n",
      "Epoch : 3568  Loss : 2.93  Grad : tensor([-0.0012,  0.0069])\n",
      "Epoch : 3569  Loss : 2.93  Grad : tensor([-0.0012,  0.0069])\n",
      "Epoch : 3570  Loss : 2.93  Grad : tensor([-0.0012,  0.0069])\n",
      "Epoch : 3571  Loss : 2.93  Grad : tensor([-0.0012,  0.0069])\n",
      "Epoch : 3572  Loss : 2.93  Grad : tensor([-0.0012,  0.0069])\n",
      "Epoch : 3573  Loss : 2.93  Grad : tensor([-0.0012,  0.0069])\n",
      "Epoch : 3574  Loss : 2.93  Grad : tensor([-0.0012,  0.0069])\n",
      "Epoch : 3575  Loss : 2.93  Grad : tensor([-0.0012,  0.0068])\n",
      "Epoch : 3576  Loss : 2.93  Grad : tensor([-0.0012,  0.0068])\n",
      "Epoch : 3577  Loss : 2.93  Grad : tensor([-0.0012,  0.0068])\n",
      "Epoch : 3578  Loss : 2.93  Grad : tensor([-0.0012,  0.0068])\n",
      "Epoch : 3579  Loss : 2.93  Grad : tensor([-0.0012,  0.0068])\n",
      "Epoch : 3580  Loss : 2.93  Grad : tensor([-0.0012,  0.0068])\n",
      "Epoch : 3581  Loss : 2.93  Grad : tensor([-0.0012,  0.0068])\n",
      "Epoch : 3582  Loss : 2.93  Grad : tensor([-0.0012,  0.0068])\n",
      "Epoch : 3583  Loss : 2.93  Grad : tensor([-0.0012,  0.0067])\n",
      "Epoch : 3584  Loss : 2.93  Grad : tensor([-0.0012,  0.0067])\n",
      "Epoch : 3585  Loss : 2.93  Grad : tensor([-0.0012,  0.0067])\n",
      "Epoch : 3586  Loss : 2.93  Grad : tensor([-0.0012,  0.0067])\n",
      "Epoch : 3587  Loss : 2.93  Grad : tensor([-0.0012,  0.0067])\n",
      "Epoch : 3588  Loss : 2.93  Grad : tensor([-0.0012,  0.0067])\n",
      "Epoch : 3589  Loss : 2.93  Grad : tensor([-0.0012,  0.0067])\n",
      "Epoch : 3590  Loss : 2.93  Grad : tensor([-0.0012,  0.0067])\n",
      "Epoch : 3591  Loss : 2.93  Grad : tensor([-0.0012,  0.0067])\n",
      "Epoch : 3592  Loss : 2.93  Grad : tensor([-0.0012,  0.0066])\n",
      "Epoch : 3593  Loss : 2.93  Grad : tensor([-0.0012,  0.0066])\n",
      "Epoch : 3594  Loss : 2.93  Grad : tensor([-0.0012,  0.0066])\n",
      "Epoch : 3595  Loss : 2.93  Grad : tensor([-0.0012,  0.0066])\n",
      "Epoch : 3596  Loss : 2.93  Grad : tensor([-0.0012,  0.0066])\n",
      "Epoch : 3597  Loss : 2.93  Grad : tensor([-0.0012,  0.0066])\n",
      "Epoch : 3598  Loss : 2.93  Grad : tensor([-0.0012,  0.0066])\n",
      "Epoch : 3599  Loss : 2.93  Grad : tensor([-0.0012,  0.0066])\n",
      "Epoch : 3600  Loss : 2.93  Grad : tensor([-0.0012,  0.0066])\n",
      "Epoch : 3601  Loss : 2.93  Grad : tensor([-0.0012,  0.0065])\n",
      "Epoch : 3602  Loss : 2.93  Grad : tensor([-0.0012,  0.0065])\n",
      "Epoch : 3603  Loss : 2.93  Grad : tensor([-0.0012,  0.0065])\n",
      "Epoch : 3604  Loss : 2.93  Grad : tensor([-0.0012,  0.0065])\n",
      "Epoch : 3605  Loss : 2.93  Grad : tensor([-0.0011,  0.0065])\n",
      "Epoch : 3606  Loss : 2.93  Grad : tensor([-0.0011,  0.0065])\n",
      "Epoch : 3607  Loss : 2.93  Grad : tensor([-0.0011,  0.0065])\n",
      "Epoch : 3608  Loss : 2.93  Grad : tensor([-0.0011,  0.0065])\n",
      "Epoch : 3609  Loss : 2.93  Grad : tensor([-0.0011,  0.0065])\n",
      "Epoch : 3610  Loss : 2.93  Grad : tensor([-0.0011,  0.0064])\n",
      "Epoch : 3611  Loss : 2.93  Grad : tensor([-0.0011,  0.0064])\n",
      "Epoch : 3612  Loss : 2.93  Grad : tensor([-0.0011,  0.0064])\n",
      "Epoch : 3613  Loss : 2.93  Grad : tensor([-0.0011,  0.0064])\n",
      "Epoch : 3614  Loss : 2.93  Grad : tensor([-0.0011,  0.0064])\n",
      "Epoch : 3615  Loss : 2.93  Grad : tensor([-0.0011,  0.0064])\n",
      "Epoch : 3616  Loss : 2.93  Grad : tensor([-0.0011,  0.0064])\n",
      "Epoch : 3617  Loss : 2.93  Grad : tensor([-0.0011,  0.0064])\n",
      "Epoch : 3618  Loss : 2.93  Grad : tensor([-0.0011,  0.0064])\n",
      "Epoch : 3619  Loss : 2.93  Grad : tensor([-0.0011,  0.0064])\n",
      "Epoch : 3620  Loss : 2.93  Grad : tensor([-0.0011,  0.0063])\n",
      "Epoch : 3621  Loss : 2.93  Grad : tensor([-0.0011,  0.0063])\n",
      "Epoch : 3622  Loss : 2.93  Grad : tensor([-0.0011,  0.0063])\n",
      "Epoch : 3623  Loss : 2.93  Grad : tensor([-0.0011,  0.0063])\n",
      "Epoch : 3624  Loss : 2.93  Grad : tensor([-0.0011,  0.0063])\n",
      "Epoch : 3625  Loss : 2.93  Grad : tensor([-0.0011,  0.0063])\n",
      "Epoch : 3626  Loss : 2.93  Grad : tensor([-0.0011,  0.0063])\n",
      "Epoch : 3627  Loss : 2.93  Grad : tensor([-0.0011,  0.0063])\n",
      "Epoch : 3628  Loss : 2.93  Grad : tensor([-0.0011,  0.0063])\n",
      "Epoch : 3629  Loss : 2.93  Grad : tensor([-0.0011,  0.0062])\n",
      "Epoch : 3630  Loss : 2.93  Grad : tensor([-0.0011,  0.0062])\n",
      "Epoch : 3631  Loss : 2.93  Grad : tensor([-0.0011,  0.0062])\n",
      "Epoch : 3632  Loss : 2.93  Grad : tensor([-0.0011,  0.0062])\n",
      "Epoch : 3633  Loss : 2.93  Grad : tensor([-0.0011,  0.0062])\n",
      "Epoch : 3634  Loss : 2.93  Grad : tensor([-0.0011,  0.0062])\n",
      "Epoch : 3635  Loss : 2.93  Grad : tensor([-0.0011,  0.0062])\n",
      "Epoch : 3636  Loss : 2.93  Grad : tensor([-0.0011,  0.0062])\n",
      "Epoch : 3637  Loss : 2.93  Grad : tensor([-0.0011,  0.0062])\n",
      "Epoch : 3638  Loss : 2.93  Grad : tensor([-0.0011,  0.0061])\n",
      "Epoch : 3639  Loss : 2.93  Grad : tensor([-0.0011,  0.0061])\n",
      "Epoch : 3640  Loss : 2.93  Grad : tensor([-0.0011,  0.0061])\n",
      "Epoch : 3641  Loss : 2.93  Grad : tensor([-0.0011,  0.0061])\n",
      "Epoch : 3642  Loss : 2.93  Grad : tensor([-0.0011,  0.0061])\n",
      "Epoch : 3643  Loss : 2.93  Grad : tensor([-0.0011,  0.0061])\n",
      "Epoch : 3644  Loss : 2.93  Grad : tensor([-0.0011,  0.0061])\n",
      "Epoch : 3645  Loss : 2.93  Grad : tensor([-0.0011,  0.0061])\n",
      "Epoch : 3646  Loss : 2.93  Grad : tensor([-0.0011,  0.0061])\n",
      "Epoch : 3647  Loss : 2.93  Grad : tensor([-0.0011,  0.0061])\n",
      "Epoch : 3648  Loss : 2.93  Grad : tensor([-0.0011,  0.0060])\n",
      "Epoch : 3649  Loss : 2.93  Grad : tensor([-0.0011,  0.0060])\n",
      "Epoch : 3650  Loss : 2.93  Grad : tensor([-0.0011,  0.0060])\n",
      "Epoch : 3651  Loss : 2.93  Grad : tensor([-0.0011,  0.0060])\n",
      "Epoch : 3652  Loss : 2.93  Grad : tensor([-0.0010,  0.0060])\n",
      "Epoch : 3653  Loss : 2.93  Grad : tensor([-0.0010,  0.0060])\n",
      "Epoch : 3654  Loss : 2.93  Grad : tensor([-0.0010,  0.0060])\n",
      "Epoch : 3655  Loss : 2.93  Grad : tensor([-0.0010,  0.0060])\n",
      "Epoch : 3656  Loss : 2.93  Grad : tensor([-0.0010,  0.0060])\n",
      "Epoch : 3657  Loss : 2.93  Grad : tensor([-0.0010,  0.0060])\n",
      "Epoch : 3658  Loss : 2.93  Grad : tensor([-0.0010,  0.0059])\n",
      "Epoch : 3659  Loss : 2.93  Grad : tensor([-0.0010,  0.0059])\n",
      "Epoch : 3660  Loss : 2.93  Grad : tensor([-0.0010,  0.0059])\n",
      "Epoch : 3661  Loss : 2.93  Grad : tensor([-0.0010,  0.0059])\n",
      "Epoch : 3662  Loss : 2.93  Grad : tensor([-0.0010,  0.0059])\n",
      "Epoch : 3663  Loss : 2.93  Grad : tensor([-0.0010,  0.0059])\n",
      "Epoch : 3664  Loss : 2.93  Grad : tensor([-0.0011,  0.0059])\n",
      "Epoch : 3665  Loss : 2.93  Grad : tensor([-0.0010,  0.0059])\n",
      "Epoch : 3666  Loss : 2.93  Grad : tensor([-0.0010,  0.0059])\n",
      "Epoch : 3667  Loss : 2.93  Grad : tensor([-0.0010,  0.0058])\n",
      "Epoch : 3668  Loss : 2.93  Grad : tensor([-0.0010,  0.0058])\n",
      "Epoch : 3669  Loss : 2.93  Grad : tensor([-0.0010,  0.0058])\n",
      "Epoch : 3670  Loss : 2.93  Grad : tensor([-0.0010,  0.0058])\n",
      "Epoch : 3671  Loss : 2.93  Grad : tensor([-0.0010,  0.0058])\n",
      "Epoch : 3672  Loss : 2.93  Grad : tensor([-0.0010,  0.0058])\n",
      "Epoch : 3673  Loss : 2.93  Grad : tensor([-0.0010,  0.0058])\n",
      "Epoch : 3674  Loss : 2.93  Grad : tensor([-0.0010,  0.0058])\n",
      "Epoch : 3675  Loss : 2.93  Grad : tensor([-0.0010,  0.0058])\n",
      "Epoch : 3676  Loss : 2.93  Grad : tensor([-0.0010,  0.0058])\n",
      "Epoch : 3677  Loss : 2.93  Grad : tensor([-0.0010,  0.0058])\n",
      "Epoch : 3678  Loss : 2.93  Grad : tensor([-0.0010,  0.0057])\n",
      "Epoch : 3679  Loss : 2.93  Grad : tensor([-0.0010,  0.0057])\n",
      "Epoch : 3680  Loss : 2.93  Grad : tensor([-0.0010,  0.0057])\n",
      "Epoch : 3681  Loss : 2.93  Grad : tensor([-0.0010,  0.0057])\n",
      "Epoch : 3682  Loss : 2.93  Grad : tensor([-0.0010,  0.0057])\n",
      "Epoch : 3683  Loss : 2.93  Grad : tensor([-0.0010,  0.0057])\n",
      "Epoch : 3684  Loss : 2.93  Grad : tensor([-0.0010,  0.0057])\n",
      "Epoch : 3685  Loss : 2.93  Grad : tensor([-0.0010,  0.0057])\n",
      "Epoch : 3686  Loss : 2.93  Grad : tensor([-0.0010,  0.0057])\n",
      "Epoch : 3687  Loss : 2.93  Grad : tensor([-0.0010,  0.0056])\n",
      "Epoch : 3688  Loss : 2.93  Grad : tensor([-0.0010,  0.0056])\n",
      "Epoch : 3689  Loss : 2.93  Grad : tensor([-0.0010,  0.0056])\n",
      "Epoch : 3690  Loss : 2.93  Grad : tensor([-0.0010,  0.0056])\n",
      "Epoch : 3691  Loss : 2.93  Grad : tensor([-0.0010,  0.0056])\n",
      "Epoch : 3692  Loss : 2.93  Grad : tensor([-0.0010,  0.0056])\n",
      "Epoch : 3693  Loss : 2.93  Grad : tensor([-0.0010,  0.0056])\n",
      "Epoch : 3694  Loss : 2.93  Grad : tensor([-0.0010,  0.0056])\n",
      "Epoch : 3695  Loss : 2.93  Grad : tensor([-0.0010,  0.0056])\n",
      "Epoch : 3696  Loss : 2.93  Grad : tensor([-0.0010,  0.0056])\n",
      "Epoch : 3697  Loss : 2.93  Grad : tensor([-0.0010,  0.0056])\n",
      "Epoch : 3698  Loss : 2.93  Grad : tensor([-0.0010,  0.0056])\n",
      "Epoch : 3699  Loss : 2.93  Grad : tensor([-0.0010,  0.0055])\n",
      "Epoch : 3700  Loss : 2.93  Grad : tensor([-0.0010,  0.0055])\n",
      "Epoch : 3701  Loss : 2.93  Grad : tensor([-0.0010,  0.0055])\n",
      "Epoch : 3702  Loss : 2.93  Grad : tensor([-0.0010,  0.0055])\n",
      "Epoch : 3703  Loss : 2.93  Grad : tensor([-0.0010,  0.0055])\n",
      "Epoch : 3704  Loss : 2.93  Grad : tensor([-0.0010,  0.0055])\n",
      "Epoch : 3705  Loss : 2.93  Grad : tensor([-0.0010,  0.0055])\n",
      "Epoch : 3706  Loss : 2.93  Grad : tensor([-0.0010,  0.0055])\n",
      "Epoch : 3707  Loss : 2.93  Grad : tensor([-0.0010,  0.0055])\n",
      "Epoch : 3708  Loss : 2.93  Grad : tensor([-0.0010,  0.0055])\n",
      "Epoch : 3709  Loss : 2.93  Grad : tensor([-0.0010,  0.0054])\n",
      "Epoch : 3710  Loss : 2.93  Grad : tensor([-0.0010,  0.0054])\n",
      "Epoch : 3711  Loss : 2.93  Grad : tensor([-0.0010,  0.0054])\n",
      "Epoch : 3712  Loss : 2.93  Grad : tensor([-0.0010,  0.0054])\n",
      "Epoch : 3713  Loss : 2.93  Grad : tensor([-0.0009,  0.0054])\n",
      "Epoch : 3714  Loss : 2.93  Grad : tensor([-0.0009,  0.0054])\n",
      "Epoch : 3715  Loss : 2.93  Grad : tensor([-0.0009,  0.0054])\n",
      "Epoch : 3716  Loss : 2.93  Grad : tensor([-0.0009,  0.0054])\n",
      "Epoch : 3717  Loss : 2.93  Grad : tensor([-0.0009,  0.0054])\n",
      "Epoch : 3718  Loss : 2.93  Grad : tensor([-0.0009,  0.0054])\n",
      "Epoch : 3719  Loss : 2.93  Grad : tensor([-0.0009,  0.0054])\n",
      "Epoch : 3720  Loss : 2.93  Grad : tensor([-0.0010,  0.0053])\n",
      "Epoch : 3721  Loss : 2.93  Grad : tensor([-0.0009,  0.0053])\n",
      "Epoch : 3722  Loss : 2.93  Grad : tensor([-0.0009,  0.0053])\n",
      "Epoch : 3723  Loss : 2.93  Grad : tensor([-0.0009,  0.0053])\n",
      "Epoch : 3724  Loss : 2.93  Grad : tensor([-0.0009,  0.0053])\n",
      "Epoch : 3725  Loss : 2.93  Grad : tensor([-0.0009,  0.0053])\n",
      "Epoch : 3726  Loss : 2.93  Grad : tensor([-0.0009,  0.0053])\n",
      "Epoch : 3727  Loss : 2.93  Grad : tensor([-0.0009,  0.0053])\n",
      "Epoch : 3728  Loss : 2.93  Grad : tensor([-0.0010,  0.0053])\n",
      "Epoch : 3729  Loss : 2.93  Grad : tensor([-0.0009,  0.0053])\n",
      "Epoch : 3730  Loss : 2.93  Grad : tensor([-0.0009,  0.0053])\n",
      "Epoch : 3731  Loss : 2.93  Grad : tensor([-0.0009,  0.0052])\n",
      "Epoch : 3732  Loss : 2.93  Grad : tensor([-0.0009,  0.0052])\n",
      "Epoch : 3733  Loss : 2.93  Grad : tensor([-0.0009,  0.0052])\n",
      "Epoch : 3734  Loss : 2.93  Grad : tensor([-0.0009,  0.0052])\n",
      "Epoch : 3735  Loss : 2.93  Grad : tensor([-0.0009,  0.0052])\n",
      "Epoch : 3736  Loss : 2.93  Grad : tensor([-0.0009,  0.0052])\n",
      "Epoch : 3737  Loss : 2.93  Grad : tensor([-0.0009,  0.0052])\n",
      "Epoch : 3738  Loss : 2.93  Grad : tensor([-0.0009,  0.0052])\n",
      "Epoch : 3739  Loss : 2.93  Grad : tensor([-0.0009,  0.0052])\n",
      "Epoch : 3740  Loss : 2.93  Grad : tensor([-0.0009,  0.0052])\n",
      "Epoch : 3741  Loss : 2.93  Grad : tensor([-0.0009,  0.0052])\n",
      "Epoch : 3742  Loss : 2.93  Grad : tensor([-0.0009,  0.0051])\n",
      "Epoch : 3743  Loss : 2.93  Grad : tensor([-0.0009,  0.0051])\n",
      "Epoch : 3744  Loss : 2.93  Grad : tensor([-0.0009,  0.0051])\n",
      "Epoch : 3745  Loss : 2.93  Grad : tensor([-0.0009,  0.0051])\n",
      "Epoch : 3746  Loss : 2.93  Grad : tensor([-0.0009,  0.0051])\n",
      "Epoch : 3747  Loss : 2.93  Grad : tensor([-0.0009,  0.0051])\n",
      "Epoch : 3748  Loss : 2.93  Grad : tensor([-0.0009,  0.0051])\n",
      "Epoch : 3749  Loss : 2.93  Grad : tensor([-0.0009,  0.0051])\n",
      "Epoch : 3750  Loss : 2.93  Grad : tensor([-0.0009,  0.0051])\n",
      "Epoch : 3751  Loss : 2.93  Grad : tensor([-0.0009,  0.0051])\n",
      "Epoch : 3752  Loss : 2.93  Grad : tensor([-0.0009,  0.0051])\n",
      "Epoch : 3753  Loss : 2.93  Grad : tensor([-0.0009,  0.0051])\n",
      "Epoch : 3754  Loss : 2.93  Grad : tensor([-0.0009,  0.0050])\n",
      "Epoch : 3755  Loss : 2.93  Grad : tensor([-0.0009,  0.0050])\n",
      "Epoch : 3756  Loss : 2.93  Grad : tensor([-0.0009,  0.0050])\n",
      "Epoch : 3757  Loss : 2.93  Grad : tensor([-0.0009,  0.0050])\n",
      "Epoch : 3758  Loss : 2.93  Grad : tensor([-0.0009,  0.0050])\n",
      "Epoch : 3759  Loss : 2.93  Grad : tensor([-0.0009,  0.0050])\n",
      "Epoch : 3760  Loss : 2.93  Grad : tensor([-0.0009,  0.0050])\n",
      "Epoch : 3761  Loss : 2.93  Grad : tensor([-0.0009,  0.0050])\n",
      "Epoch : 3762  Loss : 2.93  Grad : tensor([-0.0009,  0.0050])\n",
      "Epoch : 3763  Loss : 2.93  Grad : tensor([-0.0009,  0.0050])\n",
      "Epoch : 3764  Loss : 2.93  Grad : tensor([-0.0009,  0.0050])\n",
      "Epoch : 3765  Loss : 2.93  Grad : tensor([-0.0009,  0.0050])\n",
      "Epoch : 3766  Loss : 2.93  Grad : tensor([-0.0009,  0.0049])\n",
      "Epoch : 3767  Loss : 2.93  Grad : tensor([-0.0009,  0.0049])\n",
      "Epoch : 3768  Loss : 2.93  Grad : tensor([-0.0009,  0.0049])\n",
      "Epoch : 3769  Loss : 2.93  Grad : tensor([-0.0009,  0.0049])\n",
      "Epoch : 3770  Loss : 2.93  Grad : tensor([-0.0009,  0.0049])\n",
      "Epoch : 3771  Loss : 2.93  Grad : tensor([-0.0009,  0.0049])\n",
      "Epoch : 3772  Loss : 2.93  Grad : tensor([-0.0009,  0.0049])\n",
      "Epoch : 3773  Loss : 2.93  Grad : tensor([-0.0009,  0.0049])\n",
      "Epoch : 3774  Loss : 2.93  Grad : tensor([-0.0009,  0.0049])\n",
      "Epoch : 3775  Loss : 2.93  Grad : tensor([-0.0009,  0.0049])\n",
      "Epoch : 3776  Loss : 2.93  Grad : tensor([-0.0009,  0.0049])\n",
      "Epoch : 3777  Loss : 2.93  Grad : tensor([-0.0008,  0.0049])\n",
      "Epoch : 3778  Loss : 2.93  Grad : tensor([-0.0008,  0.0048])\n",
      "Epoch : 3779  Loss : 2.93  Grad : tensor([-0.0008,  0.0048])\n",
      "Epoch : 3780  Loss : 2.93  Grad : tensor([-0.0008,  0.0048])\n",
      "Epoch : 3781  Loss : 2.93  Grad : tensor([-0.0008,  0.0048])\n",
      "Epoch : 3782  Loss : 2.93  Grad : tensor([-0.0008,  0.0048])\n",
      "Epoch : 3783  Loss : 2.93  Grad : tensor([-0.0008,  0.0048])\n",
      "Epoch : 3784  Loss : 2.93  Grad : tensor([-0.0009,  0.0048])\n",
      "Epoch : 3785  Loss : 2.93  Grad : tensor([-0.0008,  0.0048])\n",
      "Epoch : 3786  Loss : 2.93  Grad : tensor([-0.0008,  0.0048])\n",
      "Epoch : 3787  Loss : 2.93  Grad : tensor([-0.0008,  0.0048])\n",
      "Epoch : 3788  Loss : 2.93  Grad : tensor([-0.0008,  0.0048])\n",
      "Epoch : 3789  Loss : 2.93  Grad : tensor([-0.0008,  0.0048])\n",
      "Epoch : 3790  Loss : 2.93  Grad : tensor([-0.0008,  0.0047])\n",
      "Epoch : 3791  Loss : 2.93  Grad : tensor([-0.0008,  0.0047])\n",
      "Epoch : 3792  Loss : 2.93  Grad : tensor([-0.0008,  0.0047])\n",
      "Epoch : 3793  Loss : 2.93  Grad : tensor([-0.0008,  0.0047])\n",
      "Epoch : 3794  Loss : 2.93  Grad : tensor([-0.0008,  0.0047])\n",
      "Epoch : 3795  Loss : 2.93  Grad : tensor([-0.0008,  0.0047])\n",
      "Epoch : 3796  Loss : 2.93  Grad : tensor([-0.0008,  0.0047])\n",
      "Epoch : 3797  Loss : 2.93  Grad : tensor([-0.0008,  0.0047])\n",
      "Epoch : 3798  Loss : 2.93  Grad : tensor([-0.0008,  0.0047])\n",
      "Epoch : 3799  Loss : 2.93  Grad : tensor([-0.0008,  0.0047])\n",
      "Epoch : 3800  Loss : 2.93  Grad : tensor([-0.0008,  0.0047])\n",
      "Epoch : 3801  Loss : 2.93  Grad : tensor([-0.0008,  0.0047])\n",
      "Epoch : 3802  Loss : 2.93  Grad : tensor([-0.0008,  0.0047])\n",
      "Epoch : 3803  Loss : 2.93  Grad : tensor([-0.0008,  0.0046])\n",
      "Epoch : 3804  Loss : 2.93  Grad : tensor([-0.0008,  0.0046])\n",
      "Epoch : 3805  Loss : 2.93  Grad : tensor([-0.0008,  0.0046])\n",
      "Epoch : 3806  Loss : 2.93  Grad : tensor([-0.0008,  0.0046])\n",
      "Epoch : 3807  Loss : 2.93  Grad : tensor([-0.0008,  0.0046])\n",
      "Epoch : 3808  Loss : 2.93  Grad : tensor([-0.0008,  0.0046])\n",
      "Epoch : 3809  Loss : 2.93  Grad : tensor([-0.0008,  0.0046])\n",
      "Epoch : 3810  Loss : 2.93  Grad : tensor([-0.0008,  0.0046])\n",
      "Epoch : 3811  Loss : 2.93  Grad : tensor([-0.0008,  0.0046])\n",
      "Epoch : 3812  Loss : 2.93  Grad : tensor([-0.0008,  0.0046])\n",
      "Epoch : 3813  Loss : 2.93  Grad : tensor([-0.0008,  0.0046])\n",
      "Epoch : 3814  Loss : 2.93  Grad : tensor([-0.0008,  0.0046])\n",
      "Epoch : 3815  Loss : 2.93  Grad : tensor([-0.0008,  0.0045])\n",
      "Epoch : 3816  Loss : 2.93  Grad : tensor([-0.0008,  0.0045])\n",
      "Epoch : 3817  Loss : 2.93  Grad : tensor([-0.0008,  0.0045])\n",
      "Epoch : 3818  Loss : 2.93  Grad : tensor([-0.0008,  0.0045])\n",
      "Epoch : 3819  Loss : 2.93  Grad : tensor([-0.0008,  0.0045])\n",
      "Epoch : 3820  Loss : 2.93  Grad : tensor([-0.0008,  0.0045])\n",
      "Epoch : 3821  Loss : 2.93  Grad : tensor([-0.0008,  0.0045])\n",
      "Epoch : 3822  Loss : 2.93  Grad : tensor([-0.0008,  0.0045])\n",
      "Epoch : 3823  Loss : 2.93  Grad : tensor([-0.0008,  0.0045])\n",
      "Epoch : 3824  Loss : 2.93  Grad : tensor([-0.0008,  0.0045])\n",
      "Epoch : 3825  Loss : 2.93  Grad : tensor([-0.0008,  0.0045])\n",
      "Epoch : 3826  Loss : 2.93  Grad : tensor([-0.0008,  0.0045])\n",
      "Epoch : 3827  Loss : 2.93  Grad : tensor([-0.0008,  0.0045])\n",
      "Epoch : 3828  Loss : 2.93  Grad : tensor([-0.0008,  0.0044])\n",
      "Epoch : 3829  Loss : 2.93  Grad : tensor([-0.0008,  0.0044])\n",
      "Epoch : 3830  Loss : 2.93  Grad : tensor([-0.0008,  0.0044])\n",
      "Epoch : 3831  Loss : 2.93  Grad : tensor([-0.0008,  0.0044])\n",
      "Epoch : 3832  Loss : 2.93  Grad : tensor([-0.0008,  0.0044])\n",
      "Epoch : 3833  Loss : 2.93  Grad : tensor([-0.0008,  0.0044])\n",
      "Epoch : 3834  Loss : 2.93  Grad : tensor([-0.0008,  0.0044])\n",
      "Epoch : 3835  Loss : 2.93  Grad : tensor([-0.0008,  0.0044])\n",
      "Epoch : 3836  Loss : 2.93  Grad : tensor([-0.0008,  0.0044])\n",
      "Epoch : 3837  Loss : 2.93  Grad : tensor([-0.0008,  0.0044])\n",
      "Epoch : 3838  Loss : 2.93  Grad : tensor([-0.0008,  0.0044])\n",
      "Epoch : 3839  Loss : 2.93  Grad : tensor([-0.0008,  0.0044])\n",
      "Epoch : 3840  Loss : 2.93  Grad : tensor([-0.0008,  0.0044])\n",
      "Epoch : 3841  Loss : 2.93  Grad : tensor([-0.0008,  0.0044])\n",
      "Epoch : 3842  Loss : 2.93  Grad : tensor([-0.0008,  0.0043])\n",
      "Epoch : 3843  Loss : 2.93  Grad : tensor([-0.0008,  0.0043])\n",
      "Epoch : 3844  Loss : 2.93  Grad : tensor([-0.0008,  0.0043])\n",
      "Epoch : 3845  Loss : 2.93  Grad : tensor([-0.0008,  0.0043])\n",
      "Epoch : 3846  Loss : 2.93  Grad : tensor([-0.0008,  0.0043])\n",
      "Epoch : 3847  Loss : 2.93  Grad : tensor([-0.0008,  0.0043])\n",
      "Epoch : 3848  Loss : 2.93  Grad : tensor([-0.0008,  0.0043])\n",
      "Epoch : 3849  Loss : 2.93  Grad : tensor([-0.0008,  0.0043])\n",
      "Epoch : 3850  Loss : 2.93  Grad : tensor([-0.0007,  0.0043])\n",
      "Epoch : 3851  Loss : 2.93  Grad : tensor([-0.0007,  0.0043])\n",
      "Epoch : 3852  Loss : 2.93  Grad : tensor([-0.0007,  0.0043])\n",
      "Epoch : 3853  Loss : 2.93  Grad : tensor([-0.0007,  0.0043])\n",
      "Epoch : 3854  Loss : 2.93  Grad : tensor([-0.0007,  0.0043])\n",
      "Epoch : 3855  Loss : 2.93  Grad : tensor([-0.0007,  0.0043])\n",
      "Epoch : 3856  Loss : 2.93  Grad : tensor([-0.0007,  0.0042])\n",
      "Epoch : 3857  Loss : 2.93  Grad : tensor([-0.0007,  0.0042])\n",
      "Epoch : 3858  Loss : 2.93  Grad : tensor([-0.0007,  0.0042])\n",
      "Epoch : 3859  Loss : 2.93  Grad : tensor([-0.0007,  0.0042])\n",
      "Epoch : 3860  Loss : 2.93  Grad : tensor([-0.0008,  0.0042])\n",
      "Epoch : 3861  Loss : 2.93  Grad : tensor([-0.0007,  0.0042])\n",
      "Epoch : 3862  Loss : 2.93  Grad : tensor([-0.0007,  0.0042])\n",
      "Epoch : 3863  Loss : 2.93  Grad : tensor([-0.0007,  0.0042])\n",
      "Epoch : 3864  Loss : 2.93  Grad : tensor([-0.0007,  0.0042])\n",
      "Epoch : 3865  Loss : 2.93  Grad : tensor([-0.0007,  0.0042])\n",
      "Epoch : 3866  Loss : 2.93  Grad : tensor([-0.0007,  0.0042])\n",
      "Epoch : 3867  Loss : 2.93  Grad : tensor([-0.0007,  0.0042])\n",
      "Epoch : 3868  Loss : 2.93  Grad : tensor([-0.0007,  0.0042])\n",
      "Epoch : 3869  Loss : 2.93  Grad : tensor([-0.0007,  0.0042])\n",
      "Epoch : 3870  Loss : 2.93  Grad : tensor([-0.0007,  0.0041])\n",
      "Epoch : 3871  Loss : 2.93  Grad : tensor([-0.0007,  0.0041])\n",
      "Epoch : 3872  Loss : 2.93  Grad : tensor([-0.0007,  0.0041])\n",
      "Epoch : 3873  Loss : 2.93  Grad : tensor([-0.0007,  0.0041])\n",
      "Epoch : 3874  Loss : 2.93  Grad : tensor([-0.0007,  0.0041])\n",
      "Epoch : 3875  Loss : 2.93  Grad : tensor([-0.0007,  0.0041])\n",
      "Epoch : 3876  Loss : 2.93  Grad : tensor([-0.0007,  0.0041])\n",
      "Epoch : 3877  Loss : 2.93  Grad : tensor([-0.0007,  0.0041])\n",
      "Epoch : 3878  Loss : 2.93  Grad : tensor([-0.0007,  0.0041])\n",
      "Epoch : 3879  Loss : 2.93  Grad : tensor([-0.0007,  0.0041])\n",
      "Epoch : 3880  Loss : 2.93  Grad : tensor([-0.0007,  0.0041])\n",
      "Epoch : 3881  Loss : 2.93  Grad : tensor([-0.0007,  0.0041])\n",
      "Epoch : 3882  Loss : 2.93  Grad : tensor([-0.0007,  0.0041])\n",
      "Epoch : 3883  Loss : 2.93  Grad : tensor([-0.0007,  0.0041])\n",
      "Epoch : 3884  Loss : 2.93  Grad : tensor([-0.0007,  0.0040])\n",
      "Epoch : 3885  Loss : 2.93  Grad : tensor([-0.0007,  0.0040])\n",
      "Epoch : 3886  Loss : 2.93  Grad : tensor([-0.0007,  0.0040])\n",
      "Epoch : 3887  Loss : 2.93  Grad : tensor([-0.0007,  0.0040])\n",
      "Epoch : 3888  Loss : 2.93  Grad : tensor([-0.0007,  0.0040])\n",
      "Epoch : 3889  Loss : 2.93  Grad : tensor([-0.0007,  0.0040])\n",
      "Epoch : 3890  Loss : 2.93  Grad : tensor([-0.0007,  0.0040])\n",
      "Epoch : 3891  Loss : 2.93  Grad : tensor([-0.0007,  0.0040])\n",
      "Epoch : 3892  Loss : 2.93  Grad : tensor([-0.0007,  0.0040])\n",
      "Epoch : 3893  Loss : 2.93  Grad : tensor([-0.0007,  0.0040])\n",
      "Epoch : 3894  Loss : 2.93  Grad : tensor([-0.0007,  0.0040])\n",
      "Epoch : 3895  Loss : 2.93  Grad : tensor([-0.0007,  0.0040])\n",
      "Epoch : 3896  Loss : 2.93  Grad : tensor([-0.0007,  0.0040])\n",
      "Epoch : 3897  Loss : 2.93  Grad : tensor([-0.0007,  0.0040])\n",
      "Epoch : 3898  Loss : 2.93  Grad : tensor([-0.0007,  0.0039])\n",
      "Epoch : 3899  Loss : 2.93  Grad : tensor([-0.0007,  0.0039])\n",
      "Epoch : 3900  Loss : 2.93  Grad : tensor([-0.0007,  0.0039])\n",
      "Epoch : 3901  Loss : 2.93  Grad : tensor([-0.0007,  0.0039])\n",
      "Epoch : 3902  Loss : 2.93  Grad : tensor([-0.0007,  0.0039])\n",
      "Epoch : 3903  Loss : 2.93  Grad : tensor([-0.0007,  0.0039])\n",
      "Epoch : 3904  Loss : 2.93  Grad : tensor([-0.0007,  0.0039])\n",
      "Epoch : 3905  Loss : 2.93  Grad : tensor([-0.0007,  0.0039])\n",
      "Epoch : 3906  Loss : 2.93  Grad : tensor([-0.0007,  0.0039])\n",
      "Epoch : 3907  Loss : 2.93  Grad : tensor([-0.0007,  0.0039])\n",
      "Epoch : 3908  Loss : 2.93  Grad : tensor([-0.0007,  0.0039])\n",
      "Epoch : 3909  Loss : 2.93  Grad : tensor([-0.0007,  0.0039])\n",
      "Epoch : 3910  Loss : 2.93  Grad : tensor([-0.0007,  0.0039])\n",
      "Epoch : 3911  Loss : 2.93  Grad : tensor([-0.0007,  0.0039])\n",
      "Epoch : 3912  Loss : 2.93  Grad : tensor([-0.0007,  0.0039])\n",
      "Epoch : 3913  Loss : 2.93  Grad : tensor([-0.0007,  0.0039])\n",
      "Epoch : 3914  Loss : 2.93  Grad : tensor([-0.0007,  0.0038])\n",
      "Epoch : 3915  Loss : 2.93  Grad : tensor([-0.0007,  0.0038])\n",
      "Epoch : 3916  Loss : 2.93  Grad : tensor([-0.0007,  0.0038])\n",
      "Epoch : 3917  Loss : 2.93  Grad : tensor([-0.0007,  0.0038])\n",
      "Epoch : 3918  Loss : 2.93  Grad : tensor([-0.0007,  0.0038])\n",
      "Epoch : 3919  Loss : 2.93  Grad : tensor([-0.0007,  0.0038])\n",
      "Epoch : 3920  Loss : 2.93  Grad : tensor([-0.0007,  0.0038])\n",
      "Epoch : 3921  Loss : 2.93  Grad : tensor([-0.0007,  0.0038])\n",
      "Epoch : 3922  Loss : 2.93  Grad : tensor([-0.0007,  0.0038])\n",
      "Epoch : 3923  Loss : 2.93  Grad : tensor([-0.0007,  0.0038])\n",
      "Epoch : 3924  Loss : 2.93  Grad : tensor([-0.0007,  0.0038])\n",
      "Epoch : 3925  Loss : 2.93  Grad : tensor([-0.0007,  0.0038])\n",
      "Epoch : 3926  Loss : 2.93  Grad : tensor([-0.0007,  0.0038])\n",
      "Epoch : 3927  Loss : 2.93  Grad : tensor([-0.0007,  0.0038])\n",
      "Epoch : 3928  Loss : 2.93  Grad : tensor([-0.0007,  0.0038])\n",
      "Epoch : 3929  Loss : 2.93  Grad : tensor([-0.0007,  0.0037])\n",
      "Epoch : 3930  Loss : 2.93  Grad : tensor([-0.0007,  0.0037])\n",
      "Epoch : 3931  Loss : 2.93  Grad : tensor([-0.0007,  0.0037])\n",
      "Epoch : 3932  Loss : 2.93  Grad : tensor([-0.0007,  0.0037])\n",
      "Epoch : 3933  Loss : 2.93  Grad : tensor([-0.0007,  0.0037])\n",
      "Epoch : 3934  Loss : 2.93  Grad : tensor([-0.0006,  0.0037])\n",
      "Epoch : 3935  Loss : 2.93  Grad : tensor([-0.0006,  0.0037])\n",
      "Epoch : 3936  Loss : 2.93  Grad : tensor([-0.0006,  0.0037])\n",
      "Epoch : 3937  Loss : 2.93  Grad : tensor([-0.0006,  0.0037])\n",
      "Epoch : 3938  Loss : 2.93  Grad : tensor([-0.0006,  0.0037])\n",
      "Epoch : 3939  Loss : 2.93  Grad : tensor([-0.0006,  0.0037])\n",
      "Epoch : 3940  Loss : 2.93  Grad : tensor([-0.0007,  0.0037])\n",
      "Epoch : 3941  Loss : 2.93  Grad : tensor([-0.0006,  0.0037])\n",
      "Epoch : 3942  Loss : 2.93  Grad : tensor([-0.0006,  0.0037])\n",
      "Epoch : 3943  Loss : 2.93  Grad : tensor([-0.0006,  0.0037])\n",
      "Epoch : 3944  Loss : 2.93  Grad : tensor([-0.0006,  0.0037])\n",
      "Epoch : 3945  Loss : 2.93  Grad : tensor([-0.0007,  0.0036])\n",
      "Epoch : 3946  Loss : 2.93  Grad : tensor([-0.0006,  0.0036])\n",
      "Epoch : 3947  Loss : 2.93  Grad : tensor([-0.0006,  0.0036])\n",
      "Epoch : 3948  Loss : 2.93  Grad : tensor([-0.0006,  0.0036])\n",
      "Epoch : 3949  Loss : 2.93  Grad : tensor([-0.0006,  0.0036])\n",
      "Epoch : 3950  Loss : 2.93  Grad : tensor([-0.0007,  0.0036])\n",
      "Epoch : 3951  Loss : 2.93  Grad : tensor([-0.0006,  0.0036])\n",
      "Epoch : 3952  Loss : 2.93  Grad : tensor([-0.0006,  0.0036])\n",
      "Epoch : 3953  Loss : 2.93  Grad : tensor([-0.0006,  0.0036])\n",
      "Epoch : 3954  Loss : 2.93  Grad : tensor([-0.0006,  0.0036])\n",
      "Epoch : 3955  Loss : 2.93  Grad : tensor([-0.0006,  0.0036])\n",
      "Epoch : 3956  Loss : 2.93  Grad : tensor([-0.0006,  0.0036])\n",
      "Epoch : 3957  Loss : 2.93  Grad : tensor([-0.0006,  0.0036])\n",
      "Epoch : 3958  Loss : 2.93  Grad : tensor([-0.0007,  0.0036])\n",
      "Epoch : 3959  Loss : 2.93  Grad : tensor([-0.0006,  0.0036])\n",
      "Epoch : 3960  Loss : 2.93  Grad : tensor([-0.0007,  0.0036])\n",
      "Epoch : 3961  Loss : 2.93  Grad : tensor([-0.0006,  0.0035])\n",
      "Epoch : 3962  Loss : 2.93  Grad : tensor([-0.0006,  0.0035])\n",
      "Epoch : 3963  Loss : 2.93  Grad : tensor([-0.0007,  0.0035])\n",
      "Epoch : 3964  Loss : 2.93  Grad : tensor([-0.0006,  0.0035])\n",
      "Epoch : 3965  Loss : 2.93  Grad : tensor([-0.0006,  0.0035])\n",
      "Epoch : 3966  Loss : 2.93  Grad : tensor([-0.0006,  0.0035])\n",
      "Epoch : 3967  Loss : 2.93  Grad : tensor([-0.0006,  0.0035])\n",
      "Epoch : 3968  Loss : 2.93  Grad : tensor([-0.0006,  0.0035])\n",
      "Epoch : 3969  Loss : 2.93  Grad : tensor([-0.0006,  0.0035])\n",
      "Epoch : 3970  Loss : 2.93  Grad : tensor([-0.0006,  0.0035])\n",
      "Epoch : 3971  Loss : 2.93  Grad : tensor([-0.0006,  0.0035])\n",
      "Epoch : 3972  Loss : 2.93  Grad : tensor([-0.0006,  0.0035])\n",
      "Epoch : 3973  Loss : 2.93  Grad : tensor([-0.0006,  0.0035])\n",
      "Epoch : 3974  Loss : 2.93  Grad : tensor([-0.0006,  0.0035])\n",
      "Epoch : 3975  Loss : 2.93  Grad : tensor([-0.0006,  0.0035])\n",
      "Epoch : 3976  Loss : 2.93  Grad : tensor([-0.0006,  0.0035])\n",
      "Epoch : 3977  Loss : 2.93  Grad : tensor([-0.0006,  0.0035])\n",
      "Epoch : 3978  Loss : 2.93  Grad : tensor([-0.0006,  0.0035])\n",
      "Epoch : 3979  Loss : 2.93  Grad : tensor([-0.0006,  0.0034])\n",
      "Epoch : 3980  Loss : 2.93  Grad : tensor([-0.0006,  0.0034])\n",
      "Epoch : 3981  Loss : 2.93  Grad : tensor([-0.0006,  0.0034])\n",
      "Epoch : 3982  Loss : 2.93  Grad : tensor([-0.0006,  0.0034])\n",
      "Epoch : 3983  Loss : 2.93  Grad : tensor([-0.0006,  0.0034])\n",
      "Epoch : 3984  Loss : 2.93  Grad : tensor([-0.0006,  0.0034])\n",
      "Epoch : 3985  Loss : 2.93  Grad : tensor([-0.0006,  0.0034])\n",
      "Epoch : 3986  Loss : 2.93  Grad : tensor([-0.0006,  0.0034])\n",
      "Epoch : 3987  Loss : 2.93  Grad : tensor([-0.0006,  0.0034])\n",
      "Epoch : 3988  Loss : 2.93  Grad : tensor([-0.0006,  0.0034])\n",
      "Epoch : 3989  Loss : 2.93  Grad : tensor([-0.0006,  0.0034])\n",
      "Epoch : 3990  Loss : 2.93  Grad : tensor([-0.0006,  0.0034])\n",
      "Epoch : 3991  Loss : 2.93  Grad : tensor([-0.0006,  0.0034])\n",
      "Epoch : 3992  Loss : 2.93  Grad : tensor([-0.0006,  0.0034])\n",
      "Epoch : 3993  Loss : 2.93  Grad : tensor([-0.0006,  0.0034])\n",
      "Epoch : 3994  Loss : 2.93  Grad : tensor([-0.0006,  0.0034])\n",
      "Epoch : 3995  Loss : 2.93  Grad : tensor([-0.0006,  0.0033])\n",
      "Epoch : 3996  Loss : 2.93  Grad : tensor([-0.0006,  0.0033])\n",
      "Epoch : 3997  Loss : 2.93  Grad : tensor([-0.0006,  0.0033])\n",
      "Epoch : 3998  Loss : 2.93  Grad : tensor([-0.0006,  0.0033])\n",
      "Epoch : 3999  Loss : 2.93  Grad : tensor([-0.0006,  0.0033])\n",
      "Epoch : 4000  Loss : 2.93  Grad : tensor([-0.0006,  0.0033])\n",
      "Epoch : 4001  Loss : 2.93  Grad : tensor([-0.0006,  0.0033])\n",
      "Epoch : 4002  Loss : 2.93  Grad : tensor([-0.0006,  0.0033])\n",
      "Epoch : 4003  Loss : 2.93  Grad : tensor([-0.0006,  0.0033])\n",
      "Epoch : 4004  Loss : 2.93  Grad : tensor([-0.0006,  0.0033])\n",
      "Epoch : 4005  Loss : 2.93  Grad : tensor([-0.0006,  0.0033])\n",
      "Epoch : 4006  Loss : 2.93  Grad : tensor([-0.0006,  0.0033])\n",
      "Epoch : 4007  Loss : 2.93  Grad : tensor([-0.0006,  0.0033])\n",
      "Epoch : 4008  Loss : 2.93  Grad : tensor([-0.0006,  0.0033])\n",
      "Epoch : 4009  Loss : 2.93  Grad : tensor([-0.0006,  0.0033])\n",
      "Epoch : 4010  Loss : 2.93  Grad : tensor([-0.0006,  0.0033])\n",
      "Epoch : 4011  Loss : 2.93  Grad : tensor([-0.0006,  0.0033])\n",
      "Epoch : 4012  Loss : 2.93  Grad : tensor([-0.0006,  0.0033])\n",
      "Epoch : 4013  Loss : 2.93  Grad : tensor([-0.0006,  0.0032])\n",
      "Epoch : 4014  Loss : 2.93  Grad : tensor([-0.0006,  0.0032])\n",
      "Epoch : 4015  Loss : 2.93  Grad : tensor([-0.0006,  0.0032])\n",
      "Epoch : 4016  Loss : 2.93  Grad : tensor([-0.0006,  0.0032])\n",
      "Epoch : 4017  Loss : 2.93  Grad : tensor([-0.0006,  0.0032])\n",
      "Epoch : 4018  Loss : 2.93  Grad : tensor([-0.0006,  0.0032])\n",
      "Epoch : 4019  Loss : 2.93  Grad : tensor([-0.0006,  0.0032])\n",
      "Epoch : 4020  Loss : 2.93  Grad : tensor([-0.0006,  0.0032])\n",
      "Epoch : 4021  Loss : 2.93  Grad : tensor([-0.0006,  0.0032])\n",
      "Epoch : 4022  Loss : 2.93  Grad : tensor([-0.0006,  0.0032])\n",
      "Epoch : 4023  Loss : 2.93  Grad : tensor([-0.0006,  0.0032])\n",
      "Epoch : 4024  Loss : 2.93  Grad : tensor([-0.0006,  0.0032])\n",
      "Epoch : 4025  Loss : 2.93  Grad : tensor([-0.0006,  0.0032])\n",
      "Epoch : 4026  Loss : 2.93  Grad : tensor([-0.0006,  0.0032])\n",
      "Epoch : 4027  Loss : 2.93  Grad : tensor([-0.0006,  0.0032])\n",
      "Epoch : 4028  Loss : 2.93  Grad : tensor([-0.0006,  0.0032])\n",
      "Epoch : 4029  Loss : 2.93  Grad : tensor([-0.0006,  0.0032])\n",
      "Epoch : 4030  Loss : 2.93  Grad : tensor([-0.0006,  0.0032])\n",
      "Epoch : 4031  Loss : 2.93  Grad : tensor([-0.0006,  0.0031])\n",
      "Epoch : 4032  Loss : 2.93  Grad : tensor([-0.0006,  0.0031])\n",
      "Epoch : 4033  Loss : 2.93  Grad : tensor([-0.0006,  0.0031])\n",
      "Epoch : 4034  Loss : 2.93  Grad : tensor([-0.0005,  0.0031])\n",
      "Epoch : 4035  Loss : 2.93  Grad : tensor([-0.0005,  0.0031])\n",
      "Epoch : 4036  Loss : 2.93  Grad : tensor([-0.0005,  0.0031])\n",
      "Epoch : 4037  Loss : 2.93  Grad : tensor([-0.0006,  0.0031])\n",
      "Epoch : 4038  Loss : 2.93  Grad : tensor([-0.0005,  0.0031])\n",
      "Epoch : 4039  Loss : 2.93  Grad : tensor([-0.0005,  0.0031])\n",
      "Epoch : 4040  Loss : 2.93  Grad : tensor([-0.0005,  0.0031])\n",
      "Epoch : 4041  Loss : 2.93  Grad : tensor([-0.0006,  0.0031])\n",
      "Epoch : 4042  Loss : 2.93  Grad : tensor([-0.0005,  0.0031])\n",
      "Epoch : 4043  Loss : 2.93  Grad : tensor([-0.0005,  0.0031])\n",
      "Epoch : 4044  Loss : 2.93  Grad : tensor([-0.0005,  0.0031])\n",
      "Epoch : 4045  Loss : 2.93  Grad : tensor([-0.0006,  0.0031])\n",
      "Epoch : 4046  Loss : 2.93  Grad : tensor([-0.0005,  0.0031])\n",
      "Epoch : 4047  Loss : 2.93  Grad : tensor([-0.0005,  0.0031])\n",
      "Epoch : 4048  Loss : 2.93  Grad : tensor([-0.0006,  0.0031])\n",
      "Epoch : 4049  Loss : 2.93  Grad : tensor([-0.0005,  0.0031])\n",
      "Epoch : 4050  Loss : 2.93  Grad : tensor([-0.0005,  0.0031])\n",
      "Epoch : 4051  Loss : 2.93  Grad : tensor([-0.0005,  0.0030])\n",
      "Epoch : 4052  Loss : 2.93  Grad : tensor([-0.0006,  0.0030])\n",
      "Epoch : 4053  Loss : 2.93  Grad : tensor([-0.0005,  0.0030])\n",
      "Epoch : 4054  Loss : 2.93  Grad : tensor([-0.0005,  0.0030])\n",
      "Epoch : 4055  Loss : 2.93  Grad : tensor([-0.0005,  0.0030])\n",
      "Epoch : 4056  Loss : 2.93  Grad : tensor([-0.0006,  0.0030])\n",
      "Epoch : 4057  Loss : 2.93  Grad : tensor([-0.0005,  0.0030])\n",
      "Epoch : 4058  Loss : 2.93  Grad : tensor([-0.0005,  0.0030])\n",
      "Epoch : 4059  Loss : 2.93  Grad : tensor([-0.0006,  0.0030])\n",
      "Epoch : 4060  Loss : 2.93  Grad : tensor([-0.0005,  0.0030])\n",
      "Epoch : 4061  Loss : 2.93  Grad : tensor([-0.0005,  0.0030])\n",
      "Epoch : 4062  Loss : 2.93  Grad : tensor([-0.0005,  0.0030])\n",
      "Epoch : 4063  Loss : 2.93  Grad : tensor([-0.0006,  0.0030])\n",
      "Epoch : 4064  Loss : 2.93  Grad : tensor([-0.0005,  0.0030])\n",
      "Epoch : 4065  Loss : 2.93  Grad : tensor([-0.0005,  0.0030])\n",
      "Epoch : 4066  Loss : 2.93  Grad : tensor([-0.0005,  0.0030])\n",
      "Epoch : 4067  Loss : 2.93  Grad : tensor([-0.0005,  0.0030])\n",
      "Epoch : 4068  Loss : 2.93  Grad : tensor([-0.0005,  0.0030])\n",
      "Epoch : 4069  Loss : 2.93  Grad : tensor([-0.0005,  0.0030])\n",
      "Epoch : 4070  Loss : 2.93  Grad : tensor([-0.0005,  0.0030])\n",
      "Epoch : 4071  Loss : 2.93  Grad : tensor([-0.0005,  0.0029])\n",
      "Epoch : 4072  Loss : 2.93  Grad : tensor([-0.0005,  0.0029])\n",
      "Epoch : 4073  Loss : 2.93  Grad : tensor([-0.0005,  0.0029])\n",
      "Epoch : 4074  Loss : 2.93  Grad : tensor([-0.0005,  0.0029])\n",
      "Epoch : 4075  Loss : 2.93  Grad : tensor([-0.0005,  0.0029])\n",
      "Epoch : 4076  Loss : 2.93  Grad : tensor([-0.0005,  0.0029])\n",
      "Epoch : 4077  Loss : 2.93  Grad : tensor([-0.0005,  0.0029])\n",
      "Epoch : 4078  Loss : 2.93  Grad : tensor([-0.0005,  0.0029])\n",
      "Epoch : 4079  Loss : 2.93  Grad : tensor([-0.0005,  0.0029])\n",
      "Epoch : 4080  Loss : 2.93  Grad : tensor([-0.0005,  0.0029])\n",
      "Epoch : 4081  Loss : 2.93  Grad : tensor([-0.0005,  0.0029])\n",
      "Epoch : 4082  Loss : 2.93  Grad : tensor([-0.0005,  0.0029])\n",
      "Epoch : 4083  Loss : 2.93  Grad : tensor([-0.0005,  0.0029])\n",
      "Epoch : 4084  Loss : 2.93  Grad : tensor([-0.0005,  0.0029])\n",
      "Epoch : 4085  Loss : 2.93  Grad : tensor([-0.0005,  0.0029])\n",
      "Epoch : 4086  Loss : 2.93  Grad : tensor([-0.0005,  0.0029])\n",
      "Epoch : 4087  Loss : 2.93  Grad : tensor([-0.0005,  0.0029])\n",
      "Epoch : 4088  Loss : 2.93  Grad : tensor([-0.0005,  0.0029])\n",
      "Epoch : 4089  Loss : 2.93  Grad : tensor([-0.0005,  0.0029])\n",
      "Epoch : 4090  Loss : 2.93  Grad : tensor([-0.0005,  0.0029])\n",
      "Epoch : 4091  Loss : 2.93  Grad : tensor([-0.0005,  0.0028])\n",
      "Epoch : 4092  Loss : 2.93  Grad : tensor([-0.0005,  0.0028])\n",
      "Epoch : 4093  Loss : 2.93  Grad : tensor([-0.0005,  0.0028])\n",
      "Epoch : 4094  Loss : 2.93  Grad : tensor([-0.0005,  0.0028])\n",
      "Epoch : 4095  Loss : 2.93  Grad : tensor([-0.0005,  0.0028])\n",
      "Epoch : 4096  Loss : 2.93  Grad : tensor([-0.0005,  0.0028])\n",
      "Epoch : 4097  Loss : 2.93  Grad : tensor([-0.0005,  0.0028])\n",
      "Epoch : 4098  Loss : 2.93  Grad : tensor([-0.0005,  0.0028])\n",
      "Epoch : 4099  Loss : 2.93  Grad : tensor([-0.0005,  0.0028])\n",
      "Epoch : 4100  Loss : 2.93  Grad : tensor([-0.0005,  0.0028])\n",
      "Epoch : 4101  Loss : 2.93  Grad : tensor([-0.0005,  0.0028])\n",
      "Epoch : 4102  Loss : 2.93  Grad : tensor([-0.0005,  0.0028])\n",
      "Epoch : 4103  Loss : 2.93  Grad : tensor([-0.0005,  0.0028])\n",
      "Epoch : 4104  Loss : 2.93  Grad : tensor([-0.0005,  0.0028])\n",
      "Epoch : 4105  Loss : 2.93  Grad : tensor([-0.0005,  0.0028])\n",
      "Epoch : 4106  Loss : 2.93  Grad : tensor([-0.0005,  0.0028])\n",
      "Epoch : 4107  Loss : 2.93  Grad : tensor([-0.0005,  0.0028])\n",
      "Epoch : 4108  Loss : 2.93  Grad : tensor([-0.0005,  0.0028])\n",
      "Epoch : 4109  Loss : 2.93  Grad : tensor([-0.0005,  0.0028])\n",
      "Epoch : 4110  Loss : 2.93  Grad : tensor([-0.0005,  0.0028])\n",
      "Epoch : 4111  Loss : 2.93  Grad : tensor([-0.0005,  0.0028])\n",
      "Epoch : 4112  Loss : 2.93  Grad : tensor([-0.0005,  0.0027])\n",
      "Epoch : 4113  Loss : 2.93  Grad : tensor([-0.0005,  0.0027])\n",
      "Epoch : 4114  Loss : 2.93  Grad : tensor([-0.0005,  0.0027])\n",
      "Epoch : 4115  Loss : 2.93  Grad : tensor([-0.0005,  0.0027])\n",
      "Epoch : 4116  Loss : 2.93  Grad : tensor([-0.0005,  0.0027])\n",
      "Epoch : 4117  Loss : 2.93  Grad : tensor([-0.0004,  0.0027])\n",
      "Epoch : 4118  Loss : 2.93  Grad : tensor([-0.0005,  0.0027])\n",
      "Epoch : 4119  Loss : 2.93  Grad : tensor([-0.0005,  0.0027])\n",
      "Epoch : 4120  Loss : 2.93  Grad : tensor([-0.0005,  0.0027])\n",
      "Epoch : 4121  Loss : 2.93  Grad : tensor([-0.0005,  0.0027])\n",
      "Epoch : 4122  Loss : 2.93  Grad : tensor([-0.0005,  0.0027])\n",
      "Epoch : 4123  Loss : 2.93  Grad : tensor([-0.0005,  0.0027])\n",
      "Epoch : 4124  Loss : 2.93  Grad : tensor([-0.0005,  0.0027])\n",
      "Epoch : 4125  Loss : 2.93  Grad : tensor([-0.0005,  0.0027])\n",
      "Epoch : 4126  Loss : 2.93  Grad : tensor([-0.0005,  0.0027])\n",
      "Epoch : 4127  Loss : 2.93  Grad : tensor([-0.0005,  0.0027])\n",
      "Epoch : 4128  Loss : 2.93  Grad : tensor([-0.0005,  0.0027])\n",
      "Epoch : 4129  Loss : 2.93  Grad : tensor([-0.0005,  0.0027])\n",
      "Epoch : 4130  Loss : 2.93  Grad : tensor([-0.0005,  0.0027])\n",
      "Epoch : 4131  Loss : 2.93  Grad : tensor([-0.0004,  0.0027])\n",
      "Epoch : 4132  Loss : 2.93  Grad : tensor([-0.0005,  0.0027])\n",
      "Epoch : 4133  Loss : 2.93  Grad : tensor([-0.0005,  0.0027])\n",
      "Epoch : 4134  Loss : 2.93  Grad : tensor([-0.0005,  0.0026])\n",
      "Epoch : 4135  Loss : 2.93  Grad : tensor([-0.0005,  0.0026])\n",
      "Epoch : 4136  Loss : 2.93  Grad : tensor([-0.0005,  0.0026])\n",
      "Epoch : 4137  Loss : 2.93  Grad : tensor([-0.0005,  0.0026])\n",
      "Epoch : 4138  Loss : 2.93  Grad : tensor([-0.0004,  0.0026])\n",
      "Epoch : 4139  Loss : 2.93  Grad : tensor([-0.0005,  0.0026])\n",
      "Epoch : 4140  Loss : 2.93  Grad : tensor([-0.0005,  0.0026])\n",
      "Epoch : 4141  Loss : 2.93  Grad : tensor([-0.0005,  0.0026])\n",
      "Epoch : 4142  Loss : 2.93  Grad : tensor([-0.0005,  0.0026])\n",
      "Epoch : 4143  Loss : 2.93  Grad : tensor([-0.0005,  0.0026])\n",
      "Epoch : 4144  Loss : 2.93  Grad : tensor([-0.0005,  0.0026])\n",
      "Epoch : 4145  Loss : 2.93  Grad : tensor([-0.0005,  0.0026])\n",
      "Epoch : 4146  Loss : 2.93  Grad : tensor([-0.0005,  0.0026])\n",
      "Epoch : 4147  Loss : 2.93  Grad : tensor([-0.0005,  0.0026])\n",
      "Epoch : 4148  Loss : 2.93  Grad : tensor([-0.0005,  0.0026])\n",
      "Epoch : 4149  Loss : 2.93  Grad : tensor([-0.0005,  0.0026])\n",
      "Epoch : 4150  Loss : 2.93  Grad : tensor([-0.0005,  0.0026])\n",
      "Epoch : 4151  Loss : 2.93  Grad : tensor([-0.0004,  0.0026])\n",
      "Epoch : 4152  Loss : 2.93  Grad : tensor([-0.0004,  0.0026])\n",
      "Epoch : 4153  Loss : 2.93  Grad : tensor([-0.0005,  0.0026])\n",
      "Epoch : 4154  Loss : 2.93  Grad : tensor([-0.0005,  0.0026])\n",
      "Epoch : 4155  Loss : 2.93  Grad : tensor([-0.0004,  0.0026])\n",
      "Epoch : 4156  Loss : 2.93  Grad : tensor([-0.0004,  0.0026])\n",
      "Epoch : 4157  Loss : 2.93  Grad : tensor([-0.0004,  0.0025])\n",
      "Epoch : 4158  Loss : 2.93  Grad : tensor([-0.0004,  0.0025])\n",
      "Epoch : 4159  Loss : 2.93  Grad : tensor([-0.0004,  0.0025])\n",
      "Epoch : 4160  Loss : 2.93  Grad : tensor([-0.0004,  0.0025])\n",
      "Epoch : 4161  Loss : 2.93  Grad : tensor([-0.0005,  0.0025])\n",
      "Epoch : 4162  Loss : 2.93  Grad : tensor([-0.0004,  0.0025])\n",
      "Epoch : 4163  Loss : 2.93  Grad : tensor([-0.0004,  0.0025])\n",
      "Epoch : 4164  Loss : 2.93  Grad : tensor([-0.0004,  0.0025])\n",
      "Epoch : 4165  Loss : 2.93  Grad : tensor([-0.0004,  0.0025])\n",
      "Epoch : 4166  Loss : 2.93  Grad : tensor([-0.0004,  0.0025])\n",
      "Epoch : 4167  Loss : 2.93  Grad : tensor([-0.0005,  0.0025])\n",
      "Epoch : 4168  Loss : 2.93  Grad : tensor([-0.0004,  0.0025])\n",
      "Epoch : 4169  Loss : 2.93  Grad : tensor([-0.0004,  0.0025])\n",
      "Epoch : 4170  Loss : 2.93  Grad : tensor([-0.0004,  0.0025])\n",
      "Epoch : 4171  Loss : 2.93  Grad : tensor([-0.0004,  0.0025])\n",
      "Epoch : 4172  Loss : 2.93  Grad : tensor([-0.0004,  0.0025])\n",
      "Epoch : 4173  Loss : 2.93  Grad : tensor([-0.0005,  0.0025])\n",
      "Epoch : 4174  Loss : 2.93  Grad : tensor([-0.0004,  0.0025])\n",
      "Epoch : 4175  Loss : 2.93  Grad : tensor([-0.0004,  0.0025])\n",
      "Epoch : 4176  Loss : 2.93  Grad : tensor([-0.0004,  0.0025])\n",
      "Epoch : 4177  Loss : 2.93  Grad : tensor([-0.0004,  0.0025])\n",
      "Epoch : 4178  Loss : 2.93  Grad : tensor([-0.0005,  0.0025])\n",
      "Epoch : 4179  Loss : 2.93  Grad : tensor([-0.0004,  0.0024])\n",
      "Epoch : 4180  Loss : 2.93  Grad : tensor([-0.0005,  0.0024])\n",
      "Epoch : 4181  Loss : 2.93  Grad : tensor([-0.0004,  0.0024])\n",
      "Epoch : 4182  Loss : 2.93  Grad : tensor([-0.0004,  0.0024])\n",
      "Epoch : 4183  Loss : 2.93  Grad : tensor([-0.0004,  0.0024])\n",
      "Epoch : 4184  Loss : 2.93  Grad : tensor([-0.0004,  0.0024])\n",
      "Epoch : 4185  Loss : 2.93  Grad : tensor([-0.0004,  0.0024])\n",
      "Epoch : 4186  Loss : 2.93  Grad : tensor([-0.0005,  0.0024])\n",
      "Epoch : 4187  Loss : 2.93  Grad : tensor([-0.0004,  0.0024])\n",
      "Epoch : 4188  Loss : 2.93  Grad : tensor([-0.0004,  0.0024])\n",
      "Epoch : 4189  Loss : 2.93  Grad : tensor([-0.0004,  0.0024])\n",
      "Epoch : 4190  Loss : 2.93  Grad : tensor([-0.0004,  0.0024])\n",
      "Epoch : 4191  Loss : 2.93  Grad : tensor([-0.0005,  0.0024])\n",
      "Epoch : 4192  Loss : 2.93  Grad : tensor([-0.0005,  0.0024])\n",
      "Epoch : 4193  Loss : 2.93  Grad : tensor([-0.0004,  0.0024])\n",
      "Epoch : 4194  Loss : 2.93  Grad : tensor([-0.0004,  0.0024])\n",
      "Epoch : 4195  Loss : 2.93  Grad : tensor([-0.0004,  0.0024])\n",
      "Epoch : 4196  Loss : 2.93  Grad : tensor([-0.0004,  0.0024])\n",
      "Epoch : 4197  Loss : 2.93  Grad : tensor([-0.0004,  0.0024])\n",
      "Epoch : 4198  Loss : 2.93  Grad : tensor([-0.0004,  0.0024])\n",
      "Epoch : 4199  Loss : 2.93  Grad : tensor([-0.0004,  0.0024])\n",
      "Epoch : 4200  Loss : 2.93  Grad : tensor([-0.0004,  0.0024])\n",
      "Epoch : 4201  Loss : 2.93  Grad : tensor([-0.0004,  0.0024])\n",
      "Epoch : 4202  Loss : 2.93  Grad : tensor([-0.0004,  0.0024])\n",
      "Epoch : 4203  Loss : 2.93  Grad : tensor([-0.0004,  0.0024])\n",
      "Epoch : 4204  Loss : 2.93  Grad : tensor([-0.0004,  0.0024])\n",
      "Epoch : 4205  Loss : 2.93  Grad : tensor([-0.0004,  0.0023])\n",
      "Epoch : 4206  Loss : 2.93  Grad : tensor([-0.0004,  0.0023])\n",
      "Epoch : 4207  Loss : 2.93  Grad : tensor([-0.0004,  0.0023])\n",
      "Epoch : 4208  Loss : 2.93  Grad : tensor([-0.0004,  0.0023])\n",
      "Epoch : 4209  Loss : 2.93  Grad : tensor([-0.0004,  0.0023])\n",
      "Epoch : 4210  Loss : 2.93  Grad : tensor([-0.0004,  0.0023])\n",
      "Epoch : 4211  Loss : 2.93  Grad : tensor([-0.0004,  0.0023])\n",
      "Epoch : 4212  Loss : 2.93  Grad : tensor([-0.0004,  0.0023])\n",
      "Epoch : 4213  Loss : 2.93  Grad : tensor([-0.0004,  0.0023])\n",
      "Epoch : 4214  Loss : 2.93  Grad : tensor([-0.0004,  0.0023])\n",
      "Epoch : 4215  Loss : 2.93  Grad : tensor([-0.0004,  0.0023])\n",
      "Epoch : 4216  Loss : 2.93  Grad : tensor([-0.0004,  0.0023])\n",
      "Epoch : 4217  Loss : 2.93  Grad : tensor([-0.0004,  0.0023])\n",
      "Epoch : 4218  Loss : 2.93  Grad : tensor([-0.0004,  0.0023])\n",
      "Epoch : 4219  Loss : 2.93  Grad : tensor([-0.0004,  0.0023])\n",
      "Epoch : 4220  Loss : 2.93  Grad : tensor([-0.0004,  0.0023])\n",
      "Epoch : 4221  Loss : 2.93  Grad : tensor([-0.0004,  0.0023])\n",
      "Epoch : 4222  Loss : 2.93  Grad : tensor([-0.0004,  0.0023])\n",
      "Epoch : 4223  Loss : 2.93  Grad : tensor([-0.0004,  0.0023])\n",
      "Epoch : 4224  Loss : 2.93  Grad : tensor([-0.0004,  0.0023])\n",
      "Epoch : 4225  Loss : 2.93  Grad : tensor([-0.0004,  0.0023])\n",
      "Epoch : 4226  Loss : 2.93  Grad : tensor([-0.0004,  0.0023])\n",
      "Epoch : 4227  Loss : 2.93  Grad : tensor([-0.0004,  0.0023])\n",
      "Epoch : 4228  Loss : 2.93  Grad : tensor([-0.0004,  0.0023])\n",
      "Epoch : 4229  Loss : 2.93  Grad : tensor([-0.0004,  0.0023])\n",
      "Epoch : 4230  Loss : 2.93  Grad : tensor([-0.0004,  0.0022])\n",
      "Epoch : 4231  Loss : 2.93  Grad : tensor([-0.0004,  0.0022])\n",
      "Epoch : 4232  Loss : 2.93  Grad : tensor([-0.0004,  0.0022])\n",
      "Epoch : 4233  Loss : 2.93  Grad : tensor([-0.0004,  0.0022])\n",
      "Epoch : 4234  Loss : 2.93  Grad : tensor([-0.0004,  0.0022])\n",
      "Epoch : 4235  Loss : 2.93  Grad : tensor([-0.0004,  0.0022])\n",
      "Epoch : 4236  Loss : 2.93  Grad : tensor([-0.0004,  0.0022])\n",
      "Epoch : 4237  Loss : 2.93  Grad : tensor([-0.0004,  0.0022])\n",
      "Epoch : 4238  Loss : 2.93  Grad : tensor([-0.0004,  0.0022])\n",
      "Epoch : 4239  Loss : 2.93  Grad : tensor([-0.0004,  0.0022])\n",
      "Epoch : 4240  Loss : 2.93  Grad : tensor([-0.0004,  0.0022])\n",
      "Epoch : 4241  Loss : 2.93  Grad : tensor([-0.0004,  0.0022])\n",
      "Epoch : 4242  Loss : 2.93  Grad : tensor([-0.0004,  0.0022])\n",
      "Epoch : 4243  Loss : 2.93  Grad : tensor([-0.0004,  0.0022])\n",
      "Epoch : 4244  Loss : 2.93  Grad : tensor([-0.0004,  0.0022])\n",
      "Epoch : 4245  Loss : 2.93  Grad : tensor([-0.0004,  0.0022])\n",
      "Epoch : 4246  Loss : 2.93  Grad : tensor([-0.0004,  0.0022])\n",
      "Epoch : 4247  Loss : 2.93  Grad : tensor([-0.0004,  0.0022])\n",
      "Epoch : 4248  Loss : 2.93  Grad : tensor([-0.0004,  0.0022])\n",
      "Epoch : 4249  Loss : 2.93  Grad : tensor([-0.0004,  0.0022])\n",
      "Epoch : 4250  Loss : 2.93  Grad : tensor([-0.0004,  0.0022])\n",
      "Epoch : 4251  Loss : 2.93  Grad : tensor([-0.0004,  0.0022])\n",
      "Epoch : 4252  Loss : 2.93  Grad : tensor([-0.0004,  0.0022])\n",
      "Epoch : 4253  Loss : 2.93  Grad : tensor([-0.0004,  0.0022])\n",
      "Epoch : 4254  Loss : 2.93  Grad : tensor([-0.0004,  0.0022])\n",
      "Epoch : 4255  Loss : 2.93  Grad : tensor([-0.0004,  0.0022])\n",
      "Epoch : 4256  Loss : 2.93  Grad : tensor([-0.0004,  0.0022])\n",
      "Epoch : 4257  Loss : 2.93  Grad : tensor([-0.0004,  0.0022])\n",
      "Epoch : 4258  Loss : 2.93  Grad : tensor([-0.0004,  0.0021])\n",
      "Epoch : 4259  Loss : 2.93  Grad : tensor([-0.0004,  0.0021])\n",
      "Epoch : 4260  Loss : 2.93  Grad : tensor([-0.0004,  0.0021])\n",
      "Epoch : 4261  Loss : 2.93  Grad : tensor([-0.0004,  0.0021])\n",
      "Epoch : 4262  Loss : 2.93  Grad : tensor([-0.0003,  0.0021])\n",
      "Epoch : 4263  Loss : 2.93  Grad : tensor([-0.0004,  0.0021])\n",
      "Epoch : 4264  Loss : 2.93  Grad : tensor([-0.0004,  0.0021])\n",
      "Epoch : 4265  Loss : 2.93  Grad : tensor([-0.0004,  0.0021])\n",
      "Epoch : 4266  Loss : 2.93  Grad : tensor([-0.0004,  0.0021])\n",
      "Epoch : 4267  Loss : 2.93  Grad : tensor([-0.0004,  0.0021])\n",
      "Epoch : 4268  Loss : 2.93  Grad : tensor([-0.0004,  0.0021])\n",
      "Epoch : 4269  Loss : 2.93  Grad : tensor([-0.0004,  0.0021])\n",
      "Epoch : 4270  Loss : 2.93  Grad : tensor([-0.0004,  0.0021])\n",
      "Epoch : 4271  Loss : 2.93  Grad : tensor([-0.0004,  0.0021])\n",
      "Epoch : 4272  Loss : 2.93  Grad : tensor([-0.0004,  0.0021])\n",
      "Epoch : 4273  Loss : 2.93  Grad : tensor([-0.0004,  0.0021])\n",
      "Epoch : 4274  Loss : 2.93  Grad : tensor([-0.0004,  0.0021])\n",
      "Epoch : 4275  Loss : 2.93  Grad : tensor([-0.0004,  0.0021])\n",
      "Epoch : 4276  Loss : 2.93  Grad : tensor([-0.0004,  0.0021])\n",
      "Epoch : 4277  Loss : 2.93  Grad : tensor([-0.0004,  0.0021])\n",
      "Epoch : 4278  Loss : 2.93  Grad : tensor([-0.0004,  0.0021])\n",
      "Epoch : 4279  Loss : 2.93  Grad : tensor([-0.0004,  0.0021])\n",
      "Epoch : 4280  Loss : 2.93  Grad : tensor([-0.0004,  0.0021])\n",
      "Epoch : 4281  Loss : 2.93  Grad : tensor([-0.0004,  0.0021])\n",
      "Epoch : 4282  Loss : 2.93  Grad : tensor([-0.0004,  0.0021])\n",
      "Epoch : 4283  Loss : 2.93  Grad : tensor([-0.0004,  0.0021])\n",
      "Epoch : 4284  Loss : 2.93  Grad : tensor([-0.0004,  0.0021])\n",
      "Epoch : 4285  Loss : 2.93  Grad : tensor([-0.0004,  0.0020])\n",
      "Epoch : 4286  Loss : 2.93  Grad : tensor([-0.0004,  0.0020])\n",
      "Epoch : 4287  Loss : 2.93  Grad : tensor([-0.0004,  0.0020])\n",
      "Epoch : 4288  Loss : 2.93  Grad : tensor([-0.0004,  0.0020])\n",
      "Epoch : 4289  Loss : 2.93  Grad : tensor([-0.0004,  0.0020])\n",
      "Epoch : 4290  Loss : 2.93  Grad : tensor([-0.0004,  0.0020])\n",
      "Epoch : 4291  Loss : 2.93  Grad : tensor([-0.0004,  0.0020])\n",
      "Epoch : 4292  Loss : 2.93  Grad : tensor([-0.0004,  0.0020])\n",
      "Epoch : 4293  Loss : 2.93  Grad : tensor([-0.0004,  0.0020])\n",
      "Epoch : 4294  Loss : 2.93  Grad : tensor([-0.0003,  0.0020])\n",
      "Epoch : 4295  Loss : 2.93  Grad : tensor([-0.0004,  0.0020])\n",
      "Epoch : 4296  Loss : 2.93  Grad : tensor([-0.0004,  0.0020])\n",
      "Epoch : 4297  Loss : 2.93  Grad : tensor([-0.0004,  0.0020])\n",
      "Epoch : 4298  Loss : 2.93  Grad : tensor([-0.0004,  0.0020])\n",
      "Epoch : 4299  Loss : 2.93  Grad : tensor([-0.0004,  0.0020])\n",
      "Epoch : 4300  Loss : 2.93  Grad : tensor([-0.0004,  0.0020])\n",
      "Epoch : 4301  Loss : 2.93  Grad : tensor([-0.0004,  0.0020])\n",
      "Epoch : 4302  Loss : 2.93  Grad : tensor([-0.0003,  0.0020])\n",
      "Epoch : 4303  Loss : 2.93  Grad : tensor([-0.0003,  0.0020])\n",
      "Epoch : 4304  Loss : 2.93  Grad : tensor([-0.0003,  0.0020])\n",
      "Epoch : 4305  Loss : 2.93  Grad : tensor([-0.0003,  0.0020])\n",
      "Epoch : 4306  Loss : 2.93  Grad : tensor([-0.0003,  0.0020])\n",
      "Epoch : 4307  Loss : 2.93  Grad : tensor([-0.0003,  0.0020])\n",
      "Epoch : 4308  Loss : 2.93  Grad : tensor([-0.0003,  0.0020])\n",
      "Epoch : 4309  Loss : 2.93  Grad : tensor([-0.0003,  0.0020])\n",
      "Epoch : 4310  Loss : 2.93  Grad : tensor([-0.0003,  0.0020])\n",
      "Epoch : 4311  Loss : 2.93  Grad : tensor([-0.0003,  0.0020])\n",
      "Epoch : 4312  Loss : 2.93  Grad : tensor([-0.0003,  0.0020])\n",
      "Epoch : 4313  Loss : 2.93  Grad : tensor([-0.0003,  0.0020])\n",
      "Epoch : 4314  Loss : 2.93  Grad : tensor([-0.0003,  0.0019])\n",
      "Epoch : 4315  Loss : 2.93  Grad : tensor([-0.0003,  0.0019])\n",
      "Epoch : 4316  Loss : 2.93  Grad : tensor([-0.0003,  0.0019])\n",
      "Epoch : 4317  Loss : 2.93  Grad : tensor([-0.0003,  0.0019])\n",
      "Epoch : 4318  Loss : 2.93  Grad : tensor([-0.0003,  0.0019])\n",
      "Epoch : 4319  Loss : 2.93  Grad : tensor([-0.0004,  0.0019])\n",
      "Epoch : 4320  Loss : 2.93  Grad : tensor([-0.0004,  0.0019])\n",
      "Epoch : 4321  Loss : 2.93  Grad : tensor([-0.0004,  0.0019])\n",
      "Epoch : 4322  Loss : 2.93  Grad : tensor([-0.0003,  0.0019])\n",
      "Epoch : 4323  Loss : 2.93  Grad : tensor([-0.0004,  0.0019])\n",
      "Epoch : 4324  Loss : 2.93  Grad : tensor([-0.0004,  0.0019])\n",
      "Epoch : 4325  Loss : 2.93  Grad : tensor([-0.0004,  0.0019])\n",
      "Epoch : 4326  Loss : 2.93  Grad : tensor([-0.0003,  0.0019])\n",
      "Epoch : 4327  Loss : 2.93  Grad : tensor([-0.0003,  0.0019])\n",
      "Epoch : 4328  Loss : 2.93  Grad : tensor([-0.0003,  0.0019])\n",
      "Epoch : 4329  Loss : 2.93  Grad : tensor([-0.0003,  0.0019])\n",
      "Epoch : 4330  Loss : 2.93  Grad : tensor([-0.0003,  0.0019])\n",
      "Epoch : 4331  Loss : 2.93  Grad : tensor([-0.0003,  0.0019])\n",
      "Epoch : 4332  Loss : 2.93  Grad : tensor([-0.0003,  0.0019])\n",
      "Epoch : 4333  Loss : 2.93  Grad : tensor([-0.0003,  0.0019])\n",
      "Epoch : 4334  Loss : 2.93  Grad : tensor([-0.0003,  0.0019])\n",
      "Epoch : 4335  Loss : 2.93  Grad : tensor([-0.0003,  0.0019])\n",
      "Epoch : 4336  Loss : 2.93  Grad : tensor([-0.0003,  0.0019])\n",
      "Epoch : 4337  Loss : 2.93  Grad : tensor([-0.0003,  0.0019])\n",
      "Epoch : 4338  Loss : 2.93  Grad : tensor([-0.0003,  0.0019])\n",
      "Epoch : 4339  Loss : 2.93  Grad : tensor([-0.0003,  0.0019])\n",
      "Epoch : 4340  Loss : 2.93  Grad : tensor([-0.0003,  0.0019])\n",
      "Epoch : 4341  Loss : 2.93  Grad : tensor([-0.0003,  0.0019])\n",
      "Epoch : 4342  Loss : 2.93  Grad : tensor([-0.0004,  0.0019])\n",
      "Epoch : 4343  Loss : 2.93  Grad : tensor([-0.0004,  0.0019])\n",
      "Epoch : 4344  Loss : 2.93  Grad : tensor([-0.0004,  0.0018])\n",
      "Epoch : 4345  Loss : 2.93  Grad : tensor([-0.0004,  0.0018])\n",
      "Epoch : 4346  Loss : 2.93  Grad : tensor([-0.0004,  0.0018])\n",
      "Epoch : 4347  Loss : 2.93  Grad : tensor([-0.0003,  0.0018])\n",
      "Epoch : 4348  Loss : 2.93  Grad : tensor([-0.0003,  0.0018])\n",
      "Epoch : 4349  Loss : 2.93  Grad : tensor([-0.0003,  0.0018])\n",
      "Epoch : 4350  Loss : 2.93  Grad : tensor([-0.0003,  0.0018])\n",
      "Epoch : 4351  Loss : 2.93  Grad : tensor([-0.0003,  0.0018])\n",
      "Epoch : 4352  Loss : 2.93  Grad : tensor([-0.0003,  0.0018])\n",
      "Epoch : 4353  Loss : 2.93  Grad : tensor([-0.0003,  0.0018])\n",
      "Epoch : 4354  Loss : 2.93  Grad : tensor([-0.0003,  0.0018])\n",
      "Epoch : 4355  Loss : 2.93  Grad : tensor([-0.0003,  0.0018])\n",
      "Epoch : 4356  Loss : 2.93  Grad : tensor([-0.0003,  0.0018])\n",
      "Epoch : 4357  Loss : 2.93  Grad : tensor([-0.0003,  0.0018])\n",
      "Epoch : 4358  Loss : 2.93  Grad : tensor([-0.0003,  0.0018])\n",
      "Epoch : 4359  Loss : 2.93  Grad : tensor([-0.0003,  0.0018])\n",
      "Epoch : 4360  Loss : 2.93  Grad : tensor([-0.0003,  0.0018])\n",
      "Epoch : 4361  Loss : 2.93  Grad : tensor([-0.0003,  0.0018])\n",
      "Epoch : 4362  Loss : 2.93  Grad : tensor([-0.0003,  0.0018])\n",
      "Epoch : 4363  Loss : 2.93  Grad : tensor([-0.0003,  0.0018])\n",
      "Epoch : 4364  Loss : 2.93  Grad : tensor([-0.0003,  0.0018])\n",
      "Epoch : 4365  Loss : 2.93  Grad : tensor([-0.0003,  0.0018])\n",
      "Epoch : 4366  Loss : 2.93  Grad : tensor([-0.0003,  0.0018])\n",
      "Epoch : 4367  Loss : 2.93  Grad : tensor([-0.0003,  0.0018])\n",
      "Epoch : 4368  Loss : 2.93  Grad : tensor([-0.0003,  0.0018])\n",
      "Epoch : 4369  Loss : 2.93  Grad : tensor([-0.0003,  0.0018])\n",
      "Epoch : 4370  Loss : 2.93  Grad : tensor([-0.0003,  0.0018])\n",
      "Epoch : 4371  Loss : 2.93  Grad : tensor([-0.0003,  0.0018])\n",
      "Epoch : 4372  Loss : 2.93  Grad : tensor([-0.0003,  0.0018])\n",
      "Epoch : 4373  Loss : 2.93  Grad : tensor([-0.0003,  0.0018])\n",
      "Epoch : 4374  Loss : 2.93  Grad : tensor([-0.0003,  0.0018])\n",
      "Epoch : 4375  Loss : 2.93  Grad : tensor([-0.0003,  0.0018])\n",
      "Epoch : 4376  Loss : 2.93  Grad : tensor([-0.0003,  0.0018])\n",
      "Epoch : 4377  Loss : 2.93  Grad : tensor([-0.0003,  0.0018])\n",
      "Epoch : 4378  Loss : 2.93  Grad : tensor([-0.0003,  0.0018])\n",
      "Epoch : 4379  Loss : 2.93  Grad : tensor([-0.0003,  0.0017])\n",
      "Epoch : 4380  Loss : 2.93  Grad : tensor([-0.0003,  0.0017])\n",
      "Epoch : 4381  Loss : 2.93  Grad : tensor([-0.0003,  0.0017])\n",
      "Epoch : 4382  Loss : 2.93  Grad : tensor([-0.0003,  0.0017])\n",
      "Epoch : 4383  Loss : 2.93  Grad : tensor([-0.0003,  0.0017])\n",
      "Epoch : 4384  Loss : 2.93  Grad : tensor([-0.0003,  0.0017])\n",
      "Epoch : 4385  Loss : 2.93  Grad : tensor([-0.0003,  0.0017])\n",
      "Epoch : 4386  Loss : 2.93  Grad : tensor([-0.0003,  0.0017])\n",
      "Epoch : 4387  Loss : 2.93  Grad : tensor([-0.0003,  0.0017])\n",
      "Epoch : 4388  Loss : 2.93  Grad : tensor([-0.0003,  0.0017])\n",
      "Epoch : 4389  Loss : 2.93  Grad : tensor([-0.0003,  0.0017])\n",
      "Epoch : 4390  Loss : 2.93  Grad : tensor([-0.0003,  0.0017])\n",
      "Epoch : 4391  Loss : 2.93  Grad : tensor([-0.0003,  0.0017])\n",
      "Epoch : 4392  Loss : 2.93  Grad : tensor([-0.0003,  0.0017])\n",
      "Epoch : 4393  Loss : 2.93  Grad : tensor([-0.0003,  0.0017])\n",
      "Epoch : 4394  Loss : 2.93  Grad : tensor([-0.0003,  0.0017])\n",
      "Epoch : 4395  Loss : 2.93  Grad : tensor([-0.0003,  0.0017])\n",
      "Epoch : 4396  Loss : 2.93  Grad : tensor([-0.0003,  0.0017])\n",
      "Epoch : 4397  Loss : 2.93  Grad : tensor([-0.0003,  0.0017])\n",
      "Epoch : 4398  Loss : 2.93  Grad : tensor([-0.0003,  0.0017])\n",
      "Epoch : 4399  Loss : 2.93  Grad : tensor([-0.0003,  0.0017])\n",
      "Epoch : 4400  Loss : 2.93  Grad : tensor([-0.0003,  0.0017])\n",
      "Epoch : 4401  Loss : 2.93  Grad : tensor([-0.0003,  0.0017])\n",
      "Epoch : 4402  Loss : 2.93  Grad : tensor([-0.0003,  0.0017])\n",
      "Epoch : 4403  Loss : 2.93  Grad : tensor([-0.0003,  0.0017])\n",
      "Epoch : 4404  Loss : 2.93  Grad : tensor([-0.0003,  0.0017])\n",
      "Epoch : 4405  Loss : 2.93  Grad : tensor([-0.0003,  0.0017])\n",
      "Epoch : 4406  Loss : 2.93  Grad : tensor([-0.0003,  0.0017])\n",
      "Epoch : 4407  Loss : 2.93  Grad : tensor([-0.0003,  0.0017])\n",
      "Epoch : 4408  Loss : 2.93  Grad : tensor([-0.0003,  0.0017])\n",
      "Epoch : 4409  Loss : 2.93  Grad : tensor([-0.0003,  0.0017])\n",
      "Epoch : 4410  Loss : 2.93  Grad : tensor([-0.0003,  0.0017])\n",
      "Epoch : 4411  Loss : 2.93  Grad : tensor([-0.0003,  0.0017])\n",
      "Epoch : 4412  Loss : 2.93  Grad : tensor([-0.0003,  0.0016])\n",
      "Epoch : 4413  Loss : 2.93  Grad : tensor([-0.0003,  0.0016])\n",
      "Epoch : 4414  Loss : 2.93  Grad : tensor([-0.0003,  0.0016])\n",
      "Epoch : 4415  Loss : 2.93  Grad : tensor([-0.0003,  0.0016])\n",
      "Epoch : 4416  Loss : 2.93  Grad : tensor([-0.0003,  0.0016])\n",
      "Epoch : 4417  Loss : 2.93  Grad : tensor([-0.0003,  0.0016])\n",
      "Epoch : 4418  Loss : 2.93  Grad : tensor([-0.0003,  0.0016])\n",
      "Epoch : 4419  Loss : 2.93  Grad : tensor([-0.0003,  0.0016])\n",
      "Epoch : 4420  Loss : 2.93  Grad : tensor([-0.0003,  0.0016])\n",
      "Epoch : 4421  Loss : 2.93  Grad : tensor([-0.0003,  0.0016])\n",
      "Epoch : 4422  Loss : 2.93  Grad : tensor([-0.0003,  0.0016])\n",
      "Epoch : 4423  Loss : 2.93  Grad : tensor([-0.0003,  0.0016])\n",
      "Epoch : 4424  Loss : 2.93  Grad : tensor([-0.0003,  0.0016])\n",
      "Epoch : 4425  Loss : 2.93  Grad : tensor([-0.0003,  0.0016])\n",
      "Epoch : 4426  Loss : 2.93  Grad : tensor([-0.0003,  0.0016])\n",
      "Epoch : 4427  Loss : 2.93  Grad : tensor([-0.0003,  0.0016])\n",
      "Epoch : 4428  Loss : 2.93  Grad : tensor([-0.0003,  0.0016])\n",
      "Epoch : 4429  Loss : 2.93  Grad : tensor([-0.0003,  0.0016])\n",
      "Epoch : 4430  Loss : 2.93  Grad : tensor([-0.0003,  0.0016])\n",
      "Epoch : 4431  Loss : 2.93  Grad : tensor([-0.0003,  0.0016])\n",
      "Epoch : 4432  Loss : 2.93  Grad : tensor([-0.0003,  0.0016])\n",
      "Epoch : 4433  Loss : 2.93  Grad : tensor([-0.0003,  0.0016])\n",
      "Epoch : 4434  Loss : 2.93  Grad : tensor([-0.0003,  0.0016])\n",
      "Epoch : 4435  Loss : 2.93  Grad : tensor([-0.0003,  0.0016])\n",
      "Epoch : 4436  Loss : 2.93  Grad : tensor([-0.0003,  0.0016])\n",
      "Epoch : 4437  Loss : 2.93  Grad : tensor([-0.0003,  0.0016])\n",
      "Epoch : 4438  Loss : 2.93  Grad : tensor([-0.0003,  0.0016])\n",
      "Epoch : 4439  Loss : 2.93  Grad : tensor([-0.0003,  0.0016])\n",
      "Epoch : 4440  Loss : 2.93  Grad : tensor([-0.0003,  0.0016])\n",
      "Epoch : 4441  Loss : 2.93  Grad : tensor([-0.0003,  0.0016])\n",
      "Epoch : 4442  Loss : 2.93  Grad : tensor([-0.0003,  0.0016])\n",
      "Epoch : 4443  Loss : 2.93  Grad : tensor([-0.0002,  0.0016])\n",
      "Epoch : 4444  Loss : 2.93  Grad : tensor([-0.0003,  0.0016])\n",
      "Epoch : 4445  Loss : 2.93  Grad : tensor([-0.0003,  0.0016])\n",
      "Epoch : 4446  Loss : 2.93  Grad : tensor([-0.0003,  0.0016])\n",
      "Epoch : 4447  Loss : 2.93  Grad : tensor([-0.0003,  0.0016])\n",
      "Epoch : 4448  Loss : 2.93  Grad : tensor([-0.0003,  0.0016])\n",
      "Epoch : 4449  Loss : 2.93  Grad : tensor([-0.0003,  0.0016])\n",
      "Epoch : 4450  Loss : 2.93  Grad : tensor([-0.0003,  0.0015])\n",
      "Epoch : 4451  Loss : 2.93  Grad : tensor([-0.0003,  0.0015])\n",
      "Epoch : 4452  Loss : 2.93  Grad : tensor([-0.0003,  0.0015])\n",
      "Epoch : 4453  Loss : 2.93  Grad : tensor([-0.0003,  0.0015])\n",
      "Epoch : 4454  Loss : 2.93  Grad : tensor([-0.0003,  0.0015])\n",
      "Epoch : 4455  Loss : 2.93  Grad : tensor([-0.0003,  0.0015])\n",
      "Epoch : 4456  Loss : 2.93  Grad : tensor([-0.0003,  0.0015])\n",
      "Epoch : 4457  Loss : 2.93  Grad : tensor([-0.0003,  0.0015])\n",
      "Epoch : 4458  Loss : 2.93  Grad : tensor([-0.0003,  0.0015])\n",
      "Epoch : 4459  Loss : 2.93  Grad : tensor([-0.0003,  0.0015])\n",
      "Epoch : 4460  Loss : 2.93  Grad : tensor([-0.0003,  0.0015])\n",
      "Epoch : 4461  Loss : 2.93  Grad : tensor([-0.0003,  0.0015])\n",
      "Epoch : 4462  Loss : 2.93  Grad : tensor([-0.0003,  0.0015])\n",
      "Epoch : 4463  Loss : 2.93  Grad : tensor([-0.0003,  0.0015])\n",
      "Epoch : 4464  Loss : 2.93  Grad : tensor([-0.0003,  0.0015])\n",
      "Epoch : 4465  Loss : 2.93  Grad : tensor([-0.0003,  0.0015])\n",
      "Epoch : 4466  Loss : 2.93  Grad : tensor([-0.0003,  0.0015])\n",
      "Epoch : 4467  Loss : 2.93  Grad : tensor([-0.0003,  0.0015])\n",
      "Epoch : 4468  Loss : 2.93  Grad : tensor([-0.0003,  0.0015])\n",
      "Epoch : 4469  Loss : 2.93  Grad : tensor([-0.0003,  0.0015])\n",
      "Epoch : 4470  Loss : 2.93  Grad : tensor([-0.0003,  0.0015])\n",
      "Epoch : 4471  Loss : 2.93  Grad : tensor([-0.0003,  0.0015])\n",
      "Epoch : 4472  Loss : 2.93  Grad : tensor([-0.0003,  0.0015])\n",
      "Epoch : 4473  Loss : 2.93  Grad : tensor([-0.0003,  0.0015])\n",
      "Epoch : 4474  Loss : 2.93  Grad : tensor([-0.0003,  0.0015])\n",
      "Epoch : 4475  Loss : 2.93  Grad : tensor([-0.0003,  0.0015])\n",
      "Epoch : 4476  Loss : 2.93  Grad : tensor([-0.0003,  0.0015])\n",
      "Epoch : 4477  Loss : 2.93  Grad : tensor([-0.0003,  0.0015])\n",
      "Epoch : 4478  Loss : 2.93  Grad : tensor([-0.0003,  0.0015])\n",
      "Epoch : 4479  Loss : 2.93  Grad : tensor([-0.0003,  0.0015])\n",
      "Epoch : 4480  Loss : 2.93  Grad : tensor([-0.0003,  0.0015])\n",
      "Epoch : 4481  Loss : 2.93  Grad : tensor([-0.0003,  0.0015])\n",
      "Epoch : 4482  Loss : 2.93  Grad : tensor([-0.0003,  0.0015])\n",
      "Epoch : 4483  Loss : 2.93  Grad : tensor([-0.0003,  0.0015])\n",
      "Epoch : 4484  Loss : 2.93  Grad : tensor([-0.0003,  0.0015])\n",
      "Epoch : 4485  Loss : 2.93  Grad : tensor([-0.0003,  0.0015])\n",
      "Epoch : 4486  Loss : 2.93  Grad : tensor([-0.0003,  0.0015])\n",
      "Epoch : 4487  Loss : 2.93  Grad : tensor([-0.0003,  0.0015])\n",
      "Epoch : 4488  Loss : 2.93  Grad : tensor([-0.0003,  0.0014])\n",
      "Epoch : 4489  Loss : 2.93  Grad : tensor([-0.0003,  0.0014])\n",
      "Epoch : 4490  Loss : 2.93  Grad : tensor([-0.0003,  0.0014])\n",
      "Epoch : 4491  Loss : 2.93  Grad : tensor([-0.0003,  0.0014])\n",
      "Epoch : 4492  Loss : 2.93  Grad : tensor([-0.0003,  0.0014])\n",
      "Epoch : 4493  Loss : 2.93  Grad : tensor([-0.0003,  0.0014])\n",
      "Epoch : 4494  Loss : 2.93  Grad : tensor([-0.0003,  0.0014])\n",
      "Epoch : 4495  Loss : 2.93  Grad : tensor([-0.0003,  0.0014])\n",
      "Epoch : 4496  Loss : 2.93  Grad : tensor([-0.0003,  0.0014])\n",
      "Epoch : 4497  Loss : 2.93  Grad : tensor([-0.0002,  0.0014])\n",
      "Epoch : 4498  Loss : 2.93  Grad : tensor([-0.0002,  0.0014])\n",
      "Epoch : 4499  Loss : 2.93  Grad : tensor([-0.0002,  0.0014])\n",
      "Epoch : 4500  Loss : 2.93  Grad : tensor([-0.0002,  0.0014])\n",
      "Epoch : 4501  Loss : 2.93  Grad : tensor([-0.0002,  0.0014])\n",
      "Epoch : 4502  Loss : 2.93  Grad : tensor([-0.0002,  0.0014])\n",
      "Epoch : 4503  Loss : 2.93  Grad : tensor([-0.0002,  0.0014])\n",
      "Epoch : 4504  Loss : 2.93  Grad : tensor([-0.0002,  0.0014])\n",
      "Epoch : 4505  Loss : 2.93  Grad : tensor([-0.0002,  0.0014])\n",
      "Epoch : 4506  Loss : 2.93  Grad : tensor([-0.0002,  0.0014])\n",
      "Epoch : 4507  Loss : 2.93  Grad : tensor([-0.0002,  0.0014])\n",
      "Epoch : 4508  Loss : 2.93  Grad : tensor([-0.0002,  0.0014])\n",
      "Epoch : 4509  Loss : 2.93  Grad : tensor([-0.0002,  0.0014])\n",
      "Epoch : 4510  Loss : 2.93  Grad : tensor([-0.0002,  0.0014])\n",
      "Epoch : 4511  Loss : 2.93  Grad : tensor([-0.0002,  0.0014])\n",
      "Epoch : 4512  Loss : 2.93  Grad : tensor([-0.0002,  0.0014])\n",
      "Epoch : 4513  Loss : 2.93  Grad : tensor([-0.0002,  0.0014])\n",
      "Epoch : 4514  Loss : 2.93  Grad : tensor([-0.0002,  0.0014])\n",
      "Epoch : 4515  Loss : 2.93  Grad : tensor([-0.0002,  0.0014])\n",
      "Epoch : 4516  Loss : 2.93  Grad : tensor([-0.0002,  0.0014])\n",
      "Epoch : 4517  Loss : 2.93  Grad : tensor([-0.0002,  0.0014])\n",
      "Epoch : 4518  Loss : 2.93  Grad : tensor([-0.0002,  0.0014])\n",
      "Epoch : 4519  Loss : 2.93  Grad : tensor([-0.0002,  0.0014])\n",
      "Epoch : 4520  Loss : 2.93  Grad : tensor([-0.0002,  0.0014])\n",
      "Epoch : 4521  Loss : 2.93  Grad : tensor([-0.0002,  0.0014])\n",
      "Epoch : 4522  Loss : 2.93  Grad : tensor([-0.0002,  0.0014])\n",
      "Epoch : 4523  Loss : 2.93  Grad : tensor([-0.0002,  0.0014])\n",
      "Epoch : 4524  Loss : 2.93  Grad : tensor([-0.0002,  0.0014])\n",
      "Epoch : 4525  Loss : 2.93  Grad : tensor([-0.0002,  0.0014])\n",
      "Epoch : 4526  Loss : 2.93  Grad : tensor([-0.0002,  0.0014])\n",
      "Epoch : 4527  Loss : 2.93  Grad : tensor([-0.0002,  0.0014])\n",
      "Epoch : 4528  Loss : 2.93  Grad : tensor([-0.0002,  0.0014])\n",
      "Epoch : 4529  Loss : 2.93  Grad : tensor([-0.0002,  0.0014])\n",
      "Epoch : 4530  Loss : 2.93  Grad : tensor([-0.0002,  0.0014])\n",
      "Epoch : 4531  Loss : 2.93  Grad : tensor([-0.0002,  0.0014])\n",
      "Epoch : 4532  Loss : 2.93  Grad : tensor([-0.0002,  0.0013])\n",
      "Epoch : 4533  Loss : 2.93  Grad : tensor([-0.0002,  0.0013])\n",
      "Epoch : 4534  Loss : 2.93  Grad : tensor([-0.0002,  0.0013])\n",
      "Epoch : 4535  Loss : 2.93  Grad : tensor([-0.0002,  0.0013])\n",
      "Epoch : 4536  Loss : 2.93  Grad : tensor([-0.0002,  0.0013])\n",
      "Epoch : 4537  Loss : 2.93  Grad : tensor([-0.0002,  0.0013])\n",
      "Epoch : 4538  Loss : 2.93  Grad : tensor([-0.0002,  0.0013])\n",
      "Epoch : 4539  Loss : 2.93  Grad : tensor([-0.0002,  0.0013])\n",
      "Epoch : 4540  Loss : 2.93  Grad : tensor([-0.0002,  0.0013])\n",
      "Epoch : 4541  Loss : 2.93  Grad : tensor([-0.0002,  0.0013])\n",
      "Epoch : 4542  Loss : 2.93  Grad : tensor([-0.0002,  0.0013])\n",
      "Epoch : 4543  Loss : 2.93  Grad : tensor([-0.0002,  0.0013])\n",
      "Epoch : 4544  Loss : 2.93  Grad : tensor([-0.0002,  0.0013])\n",
      "Epoch : 4545  Loss : 2.93  Grad : tensor([-0.0002,  0.0013])\n",
      "Epoch : 4546  Loss : 2.93  Grad : tensor([-0.0002,  0.0013])\n",
      "Epoch : 4547  Loss : 2.93  Grad : tensor([-0.0002,  0.0013])\n",
      "Epoch : 4548  Loss : 2.93  Grad : tensor([-0.0002,  0.0013])\n",
      "Epoch : 4549  Loss : 2.93  Grad : tensor([-0.0002,  0.0013])\n",
      "Epoch : 4550  Loss : 2.93  Grad : tensor([-0.0002,  0.0013])\n",
      "Epoch : 4551  Loss : 2.93  Grad : tensor([-0.0002,  0.0013])\n",
      "Epoch : 4552  Loss : 2.93  Grad : tensor([-0.0002,  0.0013])\n",
      "Epoch : 4553  Loss : 2.93  Grad : tensor([-0.0002,  0.0013])\n",
      "Epoch : 4554  Loss : 2.93  Grad : tensor([-0.0002,  0.0013])\n",
      "Epoch : 4555  Loss : 2.93  Grad : tensor([-0.0002,  0.0013])\n",
      "Epoch : 4556  Loss : 2.93  Grad : tensor([-0.0002,  0.0013])\n",
      "Epoch : 4557  Loss : 2.93  Grad : tensor([-0.0002,  0.0013])\n",
      "Epoch : 4558  Loss : 2.93  Grad : tensor([-0.0002,  0.0013])\n",
      "Epoch : 4559  Loss : 2.93  Grad : tensor([-0.0002,  0.0013])\n",
      "Epoch : 4560  Loss : 2.93  Grad : tensor([-0.0002,  0.0013])\n",
      "Epoch : 4561  Loss : 2.93  Grad : tensor([-0.0002,  0.0013])\n",
      "Epoch : 4562  Loss : 2.93  Grad : tensor([-0.0002,  0.0013])\n",
      "Epoch : 4563  Loss : 2.93  Grad : tensor([-0.0002,  0.0013])\n",
      "Epoch : 4564  Loss : 2.93  Grad : tensor([-0.0002,  0.0013])\n",
      "Epoch : 4565  Loss : 2.93  Grad : tensor([-0.0002,  0.0013])\n",
      "Epoch : 4566  Loss : 2.93  Grad : tensor([-0.0002,  0.0013])\n",
      "Epoch : 4567  Loss : 2.93  Grad : tensor([-0.0002,  0.0013])\n",
      "Epoch : 4568  Loss : 2.93  Grad : tensor([-0.0002,  0.0013])\n",
      "Epoch : 4569  Loss : 2.93  Grad : tensor([-0.0002,  0.0013])\n",
      "Epoch : 4570  Loss : 2.93  Grad : tensor([-0.0002,  0.0013])\n",
      "Epoch : 4571  Loss : 2.93  Grad : tensor([-0.0002,  0.0013])\n",
      "Epoch : 4572  Loss : 2.93  Grad : tensor([-0.0002,  0.0013])\n",
      "Epoch : 4573  Loss : 2.93  Grad : tensor([-0.0002,  0.0013])\n",
      "Epoch : 4574  Loss : 2.93  Grad : tensor([-0.0002,  0.0012])\n",
      "Epoch : 4575  Loss : 2.93  Grad : tensor([-0.0002,  0.0012])\n",
      "Epoch : 4576  Loss : 2.93  Grad : tensor([-0.0002,  0.0012])\n",
      "Epoch : 4577  Loss : 2.93  Grad : tensor([-0.0002,  0.0012])\n",
      "Epoch : 4578  Loss : 2.93  Grad : tensor([-0.0002,  0.0012])\n",
      "Epoch : 4579  Loss : 2.93  Grad : tensor([-0.0002,  0.0012])\n",
      "Epoch : 4580  Loss : 2.93  Grad : tensor([-0.0002,  0.0012])\n",
      "Epoch : 4581  Loss : 2.93  Grad : tensor([-0.0002,  0.0012])\n",
      "Epoch : 4582  Loss : 2.93  Grad : tensor([-0.0002,  0.0012])\n",
      "Epoch : 4583  Loss : 2.93  Grad : tensor([-0.0002,  0.0012])\n",
      "Epoch : 4584  Loss : 2.93  Grad : tensor([-0.0002,  0.0012])\n",
      "Epoch : 4585  Loss : 2.93  Grad : tensor([-0.0002,  0.0012])\n",
      "Epoch : 4586  Loss : 2.93  Grad : tensor([-0.0002,  0.0012])\n",
      "Epoch : 4587  Loss : 2.93  Grad : tensor([-0.0002,  0.0012])\n",
      "Epoch : 4588  Loss : 2.93  Grad : tensor([-0.0002,  0.0012])\n",
      "Epoch : 4589  Loss : 2.93  Grad : tensor([-0.0002,  0.0012])\n",
      "Epoch : 4590  Loss : 2.93  Grad : tensor([-0.0002,  0.0012])\n",
      "Epoch : 4591  Loss : 2.93  Grad : tensor([-0.0002,  0.0012])\n",
      "Epoch : 4592  Loss : 2.93  Grad : tensor([-0.0002,  0.0012])\n",
      "Epoch : 4593  Loss : 2.93  Grad : tensor([-0.0002,  0.0012])\n",
      "Epoch : 4594  Loss : 2.93  Grad : tensor([-0.0002,  0.0012])\n",
      "Epoch : 4595  Loss : 2.93  Grad : tensor([-0.0002,  0.0012])\n",
      "Epoch : 4596  Loss : 2.93  Grad : tensor([-0.0002,  0.0012])\n",
      "Epoch : 4597  Loss : 2.93  Grad : tensor([-0.0002,  0.0012])\n",
      "Epoch : 4598  Loss : 2.93  Grad : tensor([-0.0002,  0.0012])\n",
      "Epoch : 4599  Loss : 2.93  Grad : tensor([-0.0002,  0.0012])\n",
      "Epoch : 4600  Loss : 2.93  Grad : tensor([-0.0002,  0.0012])\n",
      "Epoch : 4601  Loss : 2.93  Grad : tensor([-0.0002,  0.0012])\n",
      "Epoch : 4602  Loss : 2.93  Grad : tensor([-0.0002,  0.0012])\n",
      "Epoch : 4603  Loss : 2.93  Grad : tensor([-0.0002,  0.0012])\n",
      "Epoch : 4604  Loss : 2.93  Grad : tensor([-0.0002,  0.0012])\n",
      "Epoch : 4605  Loss : 2.93  Grad : tensor([-0.0002,  0.0012])\n",
      "Epoch : 4606  Loss : 2.93  Grad : tensor([-0.0002,  0.0012])\n",
      "Epoch : 4607  Loss : 2.93  Grad : tensor([-0.0002,  0.0012])\n",
      "Epoch : 4608  Loss : 2.93  Grad : tensor([-0.0002,  0.0012])\n",
      "Epoch : 4609  Loss : 2.93  Grad : tensor([-0.0002,  0.0012])\n",
      "Epoch : 4610  Loss : 2.93  Grad : tensor([-0.0002,  0.0012])\n",
      "Epoch : 4611  Loss : 2.93  Grad : tensor([-0.0002,  0.0012])\n",
      "Epoch : 4612  Loss : 2.93  Grad : tensor([-0.0002,  0.0012])\n",
      "Epoch : 4613  Loss : 2.93  Grad : tensor([-0.0002,  0.0012])\n",
      "Epoch : 4614  Loss : 2.93  Grad : tensor([-0.0002,  0.0012])\n",
      "Epoch : 4615  Loss : 2.93  Grad : tensor([-0.0002,  0.0012])\n",
      "Epoch : 4616  Loss : 2.93  Grad : tensor([-0.0002,  0.0012])\n",
      "Epoch : 4617  Loss : 2.93  Grad : tensor([-0.0002,  0.0012])\n",
      "Epoch : 4618  Loss : 2.93  Grad : tensor([-0.0002,  0.0012])\n",
      "Epoch : 4619  Loss : 2.93  Grad : tensor([-0.0002,  0.0012])\n",
      "Epoch : 4620  Loss : 2.93  Grad : tensor([-0.0002,  0.0012])\n",
      "Epoch : 4621  Loss : 2.93  Grad : tensor([-0.0002,  0.0012])\n",
      "Epoch : 4622  Loss : 2.93  Grad : tensor([-0.0002,  0.0012])\n",
      "Epoch : 4623  Loss : 2.93  Grad : tensor([-0.0002,  0.0012])\n",
      "Epoch : 4624  Loss : 2.93  Grad : tensor([-0.0002,  0.0012])\n",
      "Epoch : 4625  Loss : 2.93  Grad : tensor([-0.0002,  0.0012])\n",
      "Epoch : 4626  Loss : 2.93  Grad : tensor([-0.0002,  0.0012])\n",
      "Epoch : 4627  Loss : 2.93  Grad : tensor([-0.0002,  0.0011])\n",
      "Epoch : 4628  Loss : 2.93  Grad : tensor([-0.0002,  0.0011])\n",
      "Epoch : 4629  Loss : 2.93  Grad : tensor([-0.0002,  0.0011])\n",
      "Epoch : 4630  Loss : 2.93  Grad : tensor([-0.0002,  0.0011])\n",
      "Epoch : 4631  Loss : 2.93  Grad : tensor([-0.0002,  0.0011])\n",
      "Epoch : 4632  Loss : 2.93  Grad : tensor([-0.0002,  0.0011])\n",
      "Epoch : 4633  Loss : 2.93  Grad : tensor([-0.0002,  0.0011])\n",
      "Epoch : 4634  Loss : 2.93  Grad : tensor([-0.0002,  0.0011])\n",
      "Epoch : 4635  Loss : 2.93  Grad : tensor([-0.0002,  0.0011])\n",
      "Epoch : 4636  Loss : 2.93  Grad : tensor([-0.0002,  0.0011])\n",
      "Epoch : 4637  Loss : 2.93  Grad : tensor([-0.0002,  0.0011])\n",
      "Epoch : 4638  Loss : 2.93  Grad : tensor([-0.0002,  0.0011])\n",
      "Epoch : 4639  Loss : 2.93  Grad : tensor([-0.0002,  0.0011])\n",
      "Epoch : 4640  Loss : 2.93  Grad : tensor([-0.0002,  0.0011])\n",
      "Epoch : 4641  Loss : 2.93  Grad : tensor([-0.0002,  0.0011])\n",
      "Epoch : 4642  Loss : 2.93  Grad : tensor([-0.0002,  0.0011])\n",
      "Epoch : 4643  Loss : 2.93  Grad : tensor([-0.0002,  0.0011])\n",
      "Epoch : 4644  Loss : 2.93  Grad : tensor([-0.0002,  0.0011])\n",
      "Epoch : 4645  Loss : 2.93  Grad : tensor([-0.0002,  0.0011])\n",
      "Epoch : 4646  Loss : 2.93  Grad : tensor([-0.0002,  0.0011])\n",
      "Epoch : 4647  Loss : 2.93  Grad : tensor([-0.0002,  0.0011])\n",
      "Epoch : 4648  Loss : 2.93  Grad : tensor([-0.0002,  0.0011])\n",
      "Epoch : 4649  Loss : 2.93  Grad : tensor([-0.0002,  0.0011])\n",
      "Epoch : 4650  Loss : 2.93  Grad : tensor([-0.0002,  0.0011])\n",
      "Epoch : 4651  Loss : 2.93  Grad : tensor([-0.0002,  0.0011])\n",
      "Epoch : 4652  Loss : 2.93  Grad : tensor([-0.0002,  0.0011])\n",
      "Epoch : 4653  Loss : 2.93  Grad : tensor([-0.0002,  0.0011])\n",
      "Epoch : 4654  Loss : 2.93  Grad : tensor([-0.0002,  0.0011])\n",
      "Epoch : 4655  Loss : 2.93  Grad : tensor([-0.0002,  0.0011])\n",
      "Epoch : 4656  Loss : 2.93  Grad : tensor([-0.0002,  0.0011])\n",
      "Epoch : 4657  Loss : 2.93  Grad : tensor([-0.0002,  0.0011])\n",
      "Epoch : 4658  Loss : 2.93  Grad : tensor([-0.0002,  0.0011])\n",
      "Epoch : 4659  Loss : 2.93  Grad : tensor([-0.0002,  0.0011])\n",
      "Epoch : 4660  Loss : 2.93  Grad : tensor([-0.0002,  0.0011])\n",
      "Epoch : 4661  Loss : 2.93  Grad : tensor([-0.0002,  0.0011])\n",
      "Epoch : 4662  Loss : 2.93  Grad : tensor([-0.0002,  0.0011])\n",
      "Epoch : 4663  Loss : 2.93  Grad : tensor([-0.0002,  0.0011])\n",
      "Epoch : 4664  Loss : 2.93  Grad : tensor([-0.0002,  0.0011])\n",
      "Epoch : 4665  Loss : 2.93  Grad : tensor([-0.0002,  0.0011])\n",
      "Epoch : 4666  Loss : 2.93  Grad : tensor([-0.0002,  0.0011])\n",
      "Epoch : 4667  Loss : 2.93  Grad : tensor([-0.0002,  0.0011])\n",
      "Epoch : 4668  Loss : 2.93  Grad : tensor([-0.0002,  0.0011])\n",
      "Epoch : 4669  Loss : 2.93  Grad : tensor([-0.0002,  0.0011])\n",
      "Epoch : 4670  Loss : 2.93  Grad : tensor([-0.0002,  0.0011])\n",
      "Epoch : 4671  Loss : 2.93  Grad : tensor([-0.0002,  0.0011])\n",
      "Epoch : 4672  Loss : 2.93  Grad : tensor([-0.0002,  0.0011])\n",
      "Epoch : 4673  Loss : 2.93  Grad : tensor([-0.0002,  0.0011])\n",
      "Epoch : 4674  Loss : 2.93  Grad : tensor([-0.0002,  0.0011])\n",
      "Epoch : 4675  Loss : 2.93  Grad : tensor([-0.0002,  0.0011])\n",
      "Epoch : 4676  Loss : 2.93  Grad : tensor([-0.0002,  0.0010])\n",
      "Epoch : 4677  Loss : 2.93  Grad : tensor([-0.0002,  0.0010])\n",
      "Epoch : 4678  Loss : 2.93  Grad : tensor([-0.0002,  0.0010])\n",
      "Epoch : 4679  Loss : 2.93  Grad : tensor([-0.0002,  0.0010])\n",
      "Epoch : 4680  Loss : 2.93  Grad : tensor([-0.0002,  0.0010])\n",
      "Epoch : 4681  Loss : 2.93  Grad : tensor([-0.0002,  0.0010])\n",
      "Epoch : 4682  Loss : 2.93  Grad : tensor([-0.0002,  0.0010])\n",
      "Epoch : 4683  Loss : 2.93  Grad : tensor([-0.0002,  0.0010])\n",
      "Epoch : 4684  Loss : 2.93  Grad : tensor([-0.0002,  0.0010])\n",
      "Epoch : 4685  Loss : 2.93  Grad : tensor([-0.0002,  0.0010])\n",
      "Epoch : 4686  Loss : 2.93  Grad : tensor([-0.0002,  0.0010])\n",
      "Epoch : 4687  Loss : 2.93  Grad : tensor([-0.0002,  0.0010])\n",
      "Epoch : 4688  Loss : 2.93  Grad : tensor([-0.0002,  0.0010])\n",
      "Epoch : 4689  Loss : 2.93  Grad : tensor([-0.0002,  0.0010])\n",
      "Epoch : 4690  Loss : 2.93  Grad : tensor([-0.0002,  0.0010])\n",
      "Epoch : 4691  Loss : 2.93  Grad : tensor([-0.0002,  0.0010])\n",
      "Epoch : 4692  Loss : 2.93  Grad : tensor([-0.0002,  0.0010])\n",
      "Epoch : 4693  Loss : 2.93  Grad : tensor([-0.0002,  0.0010])\n",
      "Epoch : 4694  Loss : 2.93  Grad : tensor([-0.0002,  0.0010])\n",
      "Epoch : 4695  Loss : 2.93  Grad : tensor([-0.0002,  0.0010])\n",
      "Epoch : 4696  Loss : 2.93  Grad : tensor([-0.0002,  0.0010])\n",
      "Epoch : 4697  Loss : 2.93  Grad : tensor([-0.0002,  0.0010])\n",
      "Epoch : 4698  Loss : 2.93  Grad : tensor([-0.0002,  0.0010])\n",
      "Epoch : 4699  Loss : 2.93  Grad : tensor([-0.0002,  0.0010])\n",
      "Epoch : 4700  Loss : 2.93  Grad : tensor([-0.0002,  0.0010])\n",
      "Epoch : 4701  Loss : 2.93  Grad : tensor([-0.0002,  0.0010])\n",
      "Epoch : 4702  Loss : 2.93  Grad : tensor([-0.0002,  0.0010])\n",
      "Epoch : 4703  Loss : 2.93  Grad : tensor([-0.0002,  0.0010])\n",
      "Epoch : 4704  Loss : 2.93  Grad : tensor([-0.0002,  0.0010])\n",
      "Epoch : 4705  Loss : 2.93  Grad : tensor([-0.0002,  0.0010])\n",
      "Epoch : 4706  Loss : 2.93  Grad : tensor([-0.0002,  0.0010])\n",
      "Epoch : 4707  Loss : 2.93  Grad : tensor([-0.0002,  0.0010])\n",
      "Epoch : 4708  Loss : 2.93  Grad : tensor([-0.0002,  0.0010])\n",
      "Epoch : 4709  Loss : 2.93  Grad : tensor([-0.0002,  0.0010])\n",
      "Epoch : 4710  Loss : 2.93  Grad : tensor([-0.0002,  0.0010])\n",
      "Epoch : 4711  Loss : 2.93  Grad : tensor([-0.0002,  0.0010])\n",
      "Epoch : 4712  Loss : 2.93  Grad : tensor([-0.0002,  0.0010])\n",
      "Epoch : 4713  Loss : 2.93  Grad : tensor([-0.0002,  0.0010])\n",
      "Epoch : 4714  Loss : 2.93  Grad : tensor([-0.0002,  0.0010])\n",
      "Epoch : 4715  Loss : 2.93  Grad : tensor([-0.0002,  0.0010])\n",
      "Epoch : 4716  Loss : 2.93  Grad : tensor([-0.0002,  0.0010])\n",
      "Epoch : 4717  Loss : 2.93  Grad : tensor([-0.0002,  0.0010])\n",
      "Epoch : 4718  Loss : 2.93  Grad : tensor([-0.0002,  0.0010])\n",
      "Epoch : 4719  Loss : 2.93  Grad : tensor([-0.0002,  0.0010])\n",
      "Epoch : 4720  Loss : 2.93  Grad : tensor([-0.0002,  0.0010])\n",
      "Epoch : 4721  Loss : 2.93  Grad : tensor([-0.0002,  0.0010])\n",
      "Epoch : 4722  Loss : 2.93  Grad : tensor([-0.0002,  0.0010])\n",
      "Epoch : 4723  Loss : 2.93  Grad : tensor([-0.0002,  0.0010])\n",
      "Epoch : 4724  Loss : 2.93  Grad : tensor([-0.0002,  0.0010])\n",
      "Epoch : 4725  Loss : 2.93  Grad : tensor([-0.0002,  0.0010])\n",
      "Epoch : 4726  Loss : 2.93  Grad : tensor([-0.0002,  0.0010])\n",
      "Epoch : 4727  Loss : 2.93  Grad : tensor([-0.0002,  0.0010])\n",
      "Epoch : 4728  Loss : 2.93  Grad : tensor([-0.0002,  0.0010])\n",
      "Epoch : 4729  Loss : 2.93  Grad : tensor([-0.0002,  0.0010])\n",
      "Epoch : 4730  Loss : 2.93  Grad : tensor([-0.0002,  0.0010])\n",
      "Epoch : 4731  Loss : 2.93  Grad : tensor([-0.0002,  0.0010])\n",
      "Epoch : 4732  Loss : 2.93  Grad : tensor([-0.0002,  0.0010])\n",
      "Epoch : 4733  Loss : 2.93  Grad : tensor([-0.0002,  0.0010])\n",
      "Epoch : 4734  Loss : 2.93  Grad : tensor([-0.0002,  0.0010])\n",
      "Epoch : 4735  Loss : 2.93  Grad : tensor([-0.0002,  0.0010])\n",
      "Epoch : 4736  Loss : 2.93  Grad : tensor([-0.0002,  0.0010])\n",
      "Epoch : 4737  Loss : 2.93  Grad : tensor([-0.0002,  0.0010])\n",
      "Epoch : 4738  Loss : 2.93  Grad : tensor([-0.0002,  0.0010])\n",
      "Epoch : 4739  Loss : 2.93  Grad : tensor([-0.0002,  0.0009])\n",
      "Epoch : 4740  Loss : 2.93  Grad : tensor([-0.0002,  0.0009])\n",
      "Epoch : 4741  Loss : 2.93  Grad : tensor([-0.0002,  0.0009])\n",
      "Epoch : 4742  Loss : 2.93  Grad : tensor([-0.0002,  0.0009])\n",
      "Epoch : 4743  Loss : 2.93  Grad : tensor([-0.0002,  0.0009])\n",
      "Epoch : 4744  Loss : 2.93  Grad : tensor([-0.0002,  0.0009])\n",
      "Epoch : 4745  Loss : 2.93  Grad : tensor([-0.0002,  0.0009])\n",
      "Epoch : 4746  Loss : 2.93  Grad : tensor([-0.0002,  0.0009])\n",
      "Epoch : 4747  Loss : 2.93  Grad : tensor([-0.0002,  0.0009])\n",
      "Epoch : 4748  Loss : 2.93  Grad : tensor([-0.0002,  0.0009])\n",
      "Epoch : 4749  Loss : 2.93  Grad : tensor([-0.0002,  0.0009])\n",
      "Epoch : 4750  Loss : 2.93  Grad : tensor([-0.0002,  0.0009])\n",
      "Epoch : 4751  Loss : 2.93  Grad : tensor([-0.0002,  0.0009])\n",
      "Epoch : 4752  Loss : 2.93  Grad : tensor([-0.0002,  0.0009])\n",
      "Epoch : 4753  Loss : 2.93  Grad : tensor([-0.0002,  0.0009])\n",
      "Epoch : 4754  Loss : 2.93  Grad : tensor([-0.0002,  0.0009])\n",
      "Epoch : 4755  Loss : 2.93  Grad : tensor([-0.0002,  0.0009])\n",
      "Epoch : 4756  Loss : 2.93  Grad : tensor([-0.0002,  0.0009])\n",
      "Epoch : 4757  Loss : 2.93  Grad : tensor([-0.0002,  0.0009])\n",
      "Epoch : 4758  Loss : 2.93  Grad : tensor([-0.0002,  0.0009])\n",
      "Epoch : 4759  Loss : 2.93  Grad : tensor([-0.0002,  0.0009])\n",
      "Epoch : 4760  Loss : 2.93  Grad : tensor([-0.0002,  0.0009])\n",
      "Epoch : 4761  Loss : 2.93  Grad : tensor([-0.0002,  0.0009])\n",
      "Epoch : 4762  Loss : 2.93  Grad : tensor([-0.0002,  0.0009])\n",
      "Epoch : 4763  Loss : 2.93  Grad : tensor([-0.0002,  0.0009])\n",
      "Epoch : 4764  Loss : 2.93  Grad : tensor([-0.0001,  0.0009])\n",
      "Epoch : 4765  Loss : 2.93  Grad : tensor([-0.0002,  0.0009])\n",
      "Epoch : 4766  Loss : 2.93  Grad : tensor([-0.0002,  0.0009])\n",
      "Epoch : 4767  Loss : 2.93  Grad : tensor([-0.0002,  0.0009])\n",
      "Epoch : 4768  Loss : 2.93  Grad : tensor([-0.0002,  0.0009])\n",
      "Epoch : 4769  Loss : 2.93  Grad : tensor([-0.0002,  0.0009])\n",
      "Epoch : 4770  Loss : 2.93  Grad : tensor([-0.0002,  0.0009])\n",
      "Epoch : 4771  Loss : 2.93  Grad : tensor([-0.0002,  0.0009])\n",
      "Epoch : 4772  Loss : 2.93  Grad : tensor([-0.0002,  0.0009])\n",
      "Epoch : 4773  Loss : 2.93  Grad : tensor([-0.0002,  0.0009])\n",
      "Epoch : 4774  Loss : 2.93  Grad : tensor([-0.0002,  0.0009])\n",
      "Epoch : 4775  Loss : 2.93  Grad : tensor([-0.0002,  0.0009])\n",
      "Epoch : 4776  Loss : 2.93  Grad : tensor([-0.0002,  0.0009])\n",
      "Epoch : 4777  Loss : 2.93  Grad : tensor([-0.0002,  0.0009])\n",
      "Epoch : 4778  Loss : 2.93  Grad : tensor([-0.0002,  0.0009])\n",
      "Epoch : 4779  Loss : 2.93  Grad : tensor([-0.0002,  0.0009])\n",
      "Epoch : 4780  Loss : 2.93  Grad : tensor([-0.0002,  0.0009])\n",
      "Epoch : 4781  Loss : 2.93  Grad : tensor([-0.0002,  0.0009])\n",
      "Epoch : 4782  Loss : 2.93  Grad : tensor([-0.0002,  0.0009])\n",
      "Epoch : 4783  Loss : 2.93  Grad : tensor([-0.0002,  0.0009])\n",
      "Epoch : 4784  Loss : 2.93  Grad : tensor([-0.0002,  0.0009])\n",
      "Epoch : 4785  Loss : 2.93  Grad : tensor([-0.0002,  0.0009])\n",
      "Epoch : 4786  Loss : 2.93  Grad : tensor([-0.0002,  0.0009])\n",
      "Epoch : 4787  Loss : 2.93  Grad : tensor([-0.0002,  0.0009])\n",
      "Epoch : 4788  Loss : 2.93  Grad : tensor([-0.0002,  0.0009])\n",
      "Epoch : 4789  Loss : 2.93  Grad : tensor([-0.0002,  0.0009])\n",
      "Epoch : 4790  Loss : 2.93  Grad : tensor([-0.0002,  0.0009])\n",
      "Epoch : 4791  Loss : 2.93  Grad : tensor([-0.0002,  0.0009])\n",
      "Epoch : 4792  Loss : 2.93  Grad : tensor([-0.0002,  0.0009])\n",
      "Epoch : 4793  Loss : 2.93  Grad : tensor([-0.0002,  0.0009])\n",
      "Epoch : 4794  Loss : 2.93  Grad : tensor([-0.0001,  0.0009])\n",
      "Epoch : 4795  Loss : 2.93  Grad : tensor([-0.0002,  0.0009])\n",
      "Epoch : 4796  Loss : 2.93  Grad : tensor([-0.0002,  0.0009])\n",
      "Epoch : 4797  Loss : 2.93  Grad : tensor([-0.0002,  0.0009])\n",
      "Epoch : 4798  Loss : 2.93  Grad : tensor([-0.0001,  0.0009])\n",
      "Epoch : 4799  Loss : 2.93  Grad : tensor([-0.0001,  0.0009])\n",
      "Epoch : 4800  Loss : 2.93  Grad : tensor([-0.0001,  0.0009])\n",
      "Epoch : 4801  Loss : 2.93  Grad : tensor([-0.0001,  0.0009])\n",
      "Epoch : 4802  Loss : 2.93  Grad : tensor([-0.0001,  0.0009])\n",
      "Epoch : 4803  Loss : 2.93  Grad : tensor([-0.0001,  0.0009])\n",
      "Epoch : 4804  Loss : 2.93  Grad : tensor([-0.0001,  0.0008])\n",
      "Epoch : 4805  Loss : 2.93  Grad : tensor([-0.0001,  0.0008])\n",
      "Epoch : 4806  Loss : 2.93  Grad : tensor([-0.0001,  0.0008])\n",
      "Epoch : 4807  Loss : 2.93  Grad : tensor([-0.0001,  0.0008])\n",
      "Epoch : 4808  Loss : 2.93  Grad : tensor([-0.0001,  0.0008])\n",
      "Epoch : 4809  Loss : 2.93  Grad : tensor([-0.0001,  0.0008])\n",
      "Epoch : 4810  Loss : 2.93  Grad : tensor([-0.0001,  0.0008])\n",
      "Epoch : 4811  Loss : 2.93  Grad : tensor([-0.0001,  0.0008])\n",
      "Epoch : 4812  Loss : 2.93  Grad : tensor([-0.0001,  0.0008])\n",
      "Epoch : 4813  Loss : 2.93  Grad : tensor([-0.0001,  0.0008])\n",
      "Epoch : 4814  Loss : 2.93  Grad : tensor([-0.0001,  0.0008])\n",
      "Epoch : 4815  Loss : 2.93  Grad : tensor([-0.0001,  0.0008])\n",
      "Epoch : 4816  Loss : 2.93  Grad : tensor([-0.0001,  0.0008])\n",
      "Epoch : 4817  Loss : 2.93  Grad : tensor([-0.0001,  0.0008])\n",
      "Epoch : 4818  Loss : 2.93  Grad : tensor([-0.0001,  0.0008])\n",
      "Epoch : 4819  Loss : 2.93  Grad : tensor([-0.0001,  0.0008])\n",
      "Epoch : 4820  Loss : 2.93  Grad : tensor([-0.0001,  0.0008])\n",
      "Epoch : 4821  Loss : 2.93  Grad : tensor([-0.0001,  0.0008])\n",
      "Epoch : 4822  Loss : 2.93  Grad : tensor([-0.0001,  0.0008])\n",
      "Epoch : 4823  Loss : 2.93  Grad : tensor([-0.0001,  0.0008])\n",
      "Epoch : 4824  Loss : 2.93  Grad : tensor([-0.0001,  0.0008])\n",
      "Epoch : 4825  Loss : 2.93  Grad : tensor([-0.0001,  0.0008])\n",
      "Epoch : 4826  Loss : 2.93  Grad : tensor([-0.0001,  0.0008])\n",
      "Epoch : 4827  Loss : 2.93  Grad : tensor([-0.0001,  0.0008])\n",
      "Epoch : 4828  Loss : 2.93  Grad : tensor([-0.0001,  0.0008])\n",
      "Epoch : 4829  Loss : 2.93  Grad : tensor([-0.0001,  0.0008])\n",
      "Epoch : 4830  Loss : 2.93  Grad : tensor([-0.0001,  0.0008])\n",
      "Epoch : 4831  Loss : 2.93  Grad : tensor([-0.0001,  0.0008])\n",
      "Epoch : 4832  Loss : 2.93  Grad : tensor([-0.0001,  0.0008])\n",
      "Epoch : 4833  Loss : 2.93  Grad : tensor([-0.0001,  0.0008])\n",
      "Epoch : 4834  Loss : 2.93  Grad : tensor([-0.0001,  0.0008])\n",
      "Epoch : 4835  Loss : 2.93  Grad : tensor([-0.0001,  0.0008])\n",
      "Epoch : 4836  Loss : 2.93  Grad : tensor([-0.0001,  0.0008])\n",
      "Epoch : 4837  Loss : 2.93  Grad : tensor([-0.0001,  0.0008])\n",
      "Epoch : 4838  Loss : 2.93  Grad : tensor([-0.0001,  0.0008])\n",
      "Epoch : 4839  Loss : 2.93  Grad : tensor([-0.0001,  0.0008])\n",
      "Epoch : 4840  Loss : 2.93  Grad : tensor([-0.0001,  0.0008])\n",
      "Epoch : 4841  Loss : 2.93  Grad : tensor([-0.0001,  0.0008])\n",
      "Epoch : 4842  Loss : 2.93  Grad : tensor([-0.0001,  0.0008])\n",
      "Epoch : 4843  Loss : 2.93  Grad : tensor([-0.0001,  0.0008])\n",
      "Epoch : 4844  Loss : 2.93  Grad : tensor([-0.0001,  0.0008])\n",
      "Epoch : 4845  Loss : 2.93  Grad : tensor([-0.0001,  0.0008])\n",
      "Epoch : 4846  Loss : 2.93  Grad : tensor([-0.0001,  0.0008])\n",
      "Epoch : 4847  Loss : 2.93  Grad : tensor([-0.0001,  0.0008])\n",
      "Epoch : 4848  Loss : 2.93  Grad : tensor([-0.0001,  0.0008])\n",
      "Epoch : 4849  Loss : 2.93  Grad : tensor([-0.0001,  0.0008])\n",
      "Epoch : 4850  Loss : 2.93  Grad : tensor([-0.0001,  0.0008])\n",
      "Epoch : 4851  Loss : 2.93  Grad : tensor([-0.0001,  0.0008])\n",
      "Epoch : 4852  Loss : 2.93  Grad : tensor([-0.0001,  0.0008])\n",
      "Epoch : 4853  Loss : 2.93  Grad : tensor([-0.0001,  0.0008])\n",
      "Epoch : 4854  Loss : 2.93  Grad : tensor([-0.0001,  0.0008])\n",
      "Epoch : 4855  Loss : 2.93  Grad : tensor([-0.0001,  0.0008])\n",
      "Epoch : 4856  Loss : 2.93  Grad : tensor([-0.0001,  0.0008])\n",
      "Epoch : 4857  Loss : 2.93  Grad : tensor([-0.0001,  0.0008])\n",
      "Epoch : 4858  Loss : 2.93  Grad : tensor([-0.0001,  0.0008])\n",
      "Epoch : 4859  Loss : 2.93  Grad : tensor([-0.0001,  0.0008])\n",
      "Epoch : 4860  Loss : 2.93  Grad : tensor([-0.0001,  0.0008])\n",
      "Epoch : 4861  Loss : 2.93  Grad : tensor([-0.0001,  0.0008])\n",
      "Epoch : 4862  Loss : 2.93  Grad : tensor([-0.0001,  0.0008])\n",
      "Epoch : 4863  Loss : 2.93  Grad : tensor([-0.0001,  0.0008])\n",
      "Epoch : 4864  Loss : 2.93  Grad : tensor([-0.0001,  0.0008])\n",
      "Epoch : 4865  Loss : 2.93  Grad : tensor([-0.0001,  0.0008])\n",
      "Epoch : 4866  Loss : 2.93  Grad : tensor([-0.0001,  0.0008])\n",
      "Epoch : 4867  Loss : 2.93  Grad : tensor([-0.0001,  0.0008])\n",
      "Epoch : 4868  Loss : 2.93  Grad : tensor([-0.0001,  0.0008])\n",
      "Epoch : 4869  Loss : 2.93  Grad : tensor([-0.0001,  0.0008])\n",
      "Epoch : 4870  Loss : 2.93  Grad : tensor([-0.0001,  0.0008])\n",
      "Epoch : 4871  Loss : 2.93  Grad : tensor([-0.0001,  0.0008])\n",
      "Epoch : 4872  Loss : 2.93  Grad : tensor([-0.0001,  0.0008])\n",
      "Epoch : 4873  Loss : 2.93  Grad : tensor([-0.0001,  0.0008])\n",
      "Epoch : 4874  Loss : 2.93  Grad : tensor([-0.0001,  0.0008])\n",
      "Epoch : 4875  Loss : 2.93  Grad : tensor([-0.0001,  0.0008])\n",
      "Epoch : 4876  Loss : 2.93  Grad : tensor([-0.0001,  0.0008])\n",
      "Epoch : 4877  Loss : 2.93  Grad : tensor([-0.0001,  0.0008])\n",
      "Epoch : 4878  Loss : 2.93  Grad : tensor([-0.0001,  0.0008])\n",
      "Epoch : 4879  Loss : 2.93  Grad : tensor([-0.0001,  0.0008])\n",
      "Epoch : 4880  Loss : 2.93  Grad : tensor([-0.0001,  0.0008])\n",
      "Epoch : 4881  Loss : 2.93  Grad : tensor([-0.0001,  0.0008])\n",
      "Epoch : 4882  Loss : 2.93  Grad : tensor([-0.0001,  0.0007])\n",
      "Epoch : 4883  Loss : 2.93  Grad : tensor([-0.0001,  0.0007])\n",
      "Epoch : 4884  Loss : 2.93  Grad : tensor([-0.0001,  0.0007])\n",
      "Epoch : 4885  Loss : 2.93  Grad : tensor([-0.0001,  0.0007])\n",
      "Epoch : 4886  Loss : 2.93  Grad : tensor([-0.0001,  0.0007])\n",
      "Epoch : 4887  Loss : 2.93  Grad : tensor([-0.0001,  0.0007])\n",
      "Epoch : 4888  Loss : 2.93  Grad : tensor([-0.0001,  0.0007])\n",
      "Epoch : 4889  Loss : 2.93  Grad : tensor([-0.0001,  0.0007])\n",
      "Epoch : 4890  Loss : 2.93  Grad : tensor([-0.0001,  0.0007])\n",
      "Epoch : 4891  Loss : 2.93  Grad : tensor([-0.0001,  0.0007])\n",
      "Epoch : 4892  Loss : 2.93  Grad : tensor([-0.0001,  0.0007])\n",
      "Epoch : 4893  Loss : 2.93  Grad : tensor([-0.0001,  0.0007])\n",
      "Epoch : 4894  Loss : 2.93  Grad : tensor([-0.0001,  0.0007])\n",
      "Epoch : 4895  Loss : 2.93  Grad : tensor([-0.0001,  0.0007])\n",
      "Epoch : 4896  Loss : 2.93  Grad : tensor([-0.0001,  0.0007])\n",
      "Epoch : 4897  Loss : 2.93  Grad : tensor([-0.0001,  0.0007])\n",
      "Epoch : 4898  Loss : 2.93  Grad : tensor([-0.0001,  0.0007])\n",
      "Epoch : 4899  Loss : 2.93  Grad : tensor([-0.0001,  0.0007])\n",
      "Epoch : 4900  Loss : 2.93  Grad : tensor([-0.0001,  0.0007])\n",
      "Epoch : 4901  Loss : 2.93  Grad : tensor([-0.0001,  0.0007])\n",
      "Epoch : 4902  Loss : 2.93  Grad : tensor([-0.0001,  0.0007])\n",
      "Epoch : 4903  Loss : 2.93  Grad : tensor([-0.0001,  0.0007])\n",
      "Epoch : 4904  Loss : 2.93  Grad : tensor([-0.0001,  0.0007])\n",
      "Epoch : 4905  Loss : 2.93  Grad : tensor([-0.0001,  0.0007])\n",
      "Epoch : 4906  Loss : 2.93  Grad : tensor([-0.0001,  0.0007])\n",
      "Epoch : 4907  Loss : 2.93  Grad : tensor([-0.0001,  0.0007])\n",
      "Epoch : 4908  Loss : 2.93  Grad : tensor([-0.0001,  0.0007])\n",
      "Epoch : 4909  Loss : 2.93  Grad : tensor([-0.0001,  0.0007])\n",
      "Epoch : 4910  Loss : 2.93  Grad : tensor([-0.0001,  0.0007])\n",
      "Epoch : 4911  Loss : 2.93  Grad : tensor([-0.0001,  0.0007])\n",
      "Epoch : 4912  Loss : 2.93  Grad : tensor([-0.0001,  0.0007])\n",
      "Epoch : 4913  Loss : 2.93  Grad : tensor([-0.0001,  0.0007])\n",
      "Epoch : 4914  Loss : 2.93  Grad : tensor([-0.0001,  0.0007])\n",
      "Epoch : 4915  Loss : 2.93  Grad : tensor([-0.0001,  0.0007])\n",
      "Epoch : 4916  Loss : 2.93  Grad : tensor([-0.0001,  0.0007])\n",
      "Epoch : 4917  Loss : 2.93  Grad : tensor([-0.0001,  0.0007])\n",
      "Epoch : 4918  Loss : 2.93  Grad : tensor([-0.0001,  0.0007])\n",
      "Epoch : 4919  Loss : 2.93  Grad : tensor([-0.0001,  0.0007])\n",
      "Epoch : 4920  Loss : 2.93  Grad : tensor([-0.0001,  0.0007])\n",
      "Epoch : 4921  Loss : 2.93  Grad : tensor([-0.0001,  0.0007])\n",
      "Epoch : 4922  Loss : 2.93  Grad : tensor([-0.0001,  0.0007])\n",
      "Epoch : 4923  Loss : 2.93  Grad : tensor([-0.0001,  0.0007])\n",
      "Epoch : 4924  Loss : 2.93  Grad : tensor([-0.0001,  0.0007])\n",
      "Epoch : 4925  Loss : 2.93  Grad : tensor([-0.0001,  0.0007])\n",
      "Epoch : 4926  Loss : 2.93  Grad : tensor([-0.0001,  0.0007])\n",
      "Epoch : 4927  Loss : 2.93  Grad : tensor([-0.0001,  0.0007])\n",
      "Epoch : 4928  Loss : 2.93  Grad : tensor([-0.0001,  0.0007])\n",
      "Epoch : 4929  Loss : 2.93  Grad : tensor([-0.0001,  0.0007])\n",
      "Epoch : 4930  Loss : 2.93  Grad : tensor([-0.0001,  0.0007])\n",
      "Epoch : 4931  Loss : 2.93  Grad : tensor([-0.0001,  0.0007])\n",
      "Epoch : 4932  Loss : 2.93  Grad : tensor([-0.0001,  0.0007])\n",
      "Epoch : 4933  Loss : 2.93  Grad : tensor([-0.0001,  0.0007])\n",
      "Epoch : 4934  Loss : 2.93  Grad : tensor([-0.0001,  0.0007])\n",
      "Epoch : 4935  Loss : 2.93  Grad : tensor([-0.0001,  0.0007])\n",
      "Epoch : 4936  Loss : 2.93  Grad : tensor([-0.0001,  0.0007])\n",
      "Epoch : 4937  Loss : 2.93  Grad : tensor([-0.0001,  0.0007])\n",
      "Epoch : 4938  Loss : 2.93  Grad : tensor([-0.0001,  0.0007])\n",
      "Epoch : 4939  Loss : 2.93  Grad : tensor([-0.0001,  0.0007])\n",
      "Epoch : 4940  Loss : 2.93  Grad : tensor([-0.0001,  0.0007])\n",
      "Epoch : 4941  Loss : 2.93  Grad : tensor([-0.0001,  0.0007])\n",
      "Epoch : 4942  Loss : 2.93  Grad : tensor([-0.0001,  0.0007])\n",
      "Epoch : 4943  Loss : 2.93  Grad : tensor([-0.0001,  0.0007])\n",
      "Epoch : 4944  Loss : 2.93  Grad : tensor([-0.0001,  0.0007])\n",
      "Epoch : 4945  Loss : 2.93  Grad : tensor([-0.0001,  0.0007])\n",
      "Epoch : 4946  Loss : 2.93  Grad : tensor([-0.0001,  0.0007])\n",
      "Epoch : 4947  Loss : 2.93  Grad : tensor([-0.0001,  0.0007])\n",
      "Epoch : 4948  Loss : 2.93  Grad : tensor([-0.0001,  0.0007])\n",
      "Epoch : 4949  Loss : 2.93  Grad : tensor([-0.0001,  0.0007])\n",
      "Epoch : 4950  Loss : 2.93  Grad : tensor([-9.9361e-05,  6.6355e-04])\n",
      "Epoch : 4951  Loss : 2.93  Grad : tensor([-9.5367e-05,  6.6292e-04])\n",
      "Epoch : 4952  Loss : 2.93  Grad : tensor([-9.6858e-05,  6.6188e-04])\n",
      "Epoch : 4953  Loss : 2.93  Grad : tensor([-0.0001,  0.0007])\n",
      "Epoch : 4954  Loss : 2.93  Grad : tensor([-0.0001,  0.0007])\n",
      "Epoch : 4955  Loss : 2.93  Grad : tensor([-0.0001,  0.0007])\n",
      "Epoch : 4956  Loss : 2.93  Grad : tensor([-0.0001,  0.0007])\n",
      "Epoch : 4957  Loss : 2.93  Grad : tensor([-0.0001,  0.0007])\n",
      "Epoch : 4958  Loss : 2.93  Grad : tensor([-0.0001,  0.0007])\n",
      "Epoch : 4959  Loss : 2.93  Grad : tensor([-9.8228e-05,  6.5479e-04])\n",
      "Epoch : 4960  Loss : 2.93  Grad : tensor([-0.0001,  0.0007])\n",
      "Epoch : 4961  Loss : 2.93  Grad : tensor([-9.8288e-05,  6.5270e-04])\n",
      "Epoch : 4962  Loss : 2.93  Grad : tensor([-0.0001,  0.0007])\n",
      "Epoch : 4963  Loss : 2.93  Grad : tensor([-0.0001,  0.0006])\n",
      "Epoch : 4964  Loss : 2.93  Grad : tensor([-0.0001,  0.0006])\n",
      "Epoch : 4965  Loss : 2.93  Grad : tensor([-0.0001,  0.0006])\n",
      "Epoch : 4966  Loss : 2.93  Grad : tensor([-0.0001,  0.0006])\n",
      "Epoch : 4967  Loss : 2.93  Grad : tensor([-9.7990e-05,  6.4683e-04])\n",
      "Epoch : 4968  Loss : 2.93  Grad : tensor([-9.7573e-05,  6.4588e-04])\n",
      "Epoch : 4969  Loss : 2.93  Grad : tensor([-0.0001,  0.0006])\n",
      "Epoch : 4970  Loss : 2.93  Grad : tensor([-0.0001,  0.0006])\n",
      "Epoch : 4971  Loss : 2.93  Grad : tensor([-0.0001,  0.0006])\n",
      "Epoch : 4972  Loss : 2.93  Grad : tensor([-0.0001,  0.0006])\n",
      "Epoch : 4973  Loss : 2.93  Grad : tensor([-0.0001,  0.0006])\n",
      "Epoch : 4974  Loss : 2.93  Grad : tensor([-0.0001,  0.0006])\n",
      "Epoch : 4975  Loss : 2.93  Grad : tensor([-0.0001,  0.0006])\n",
      "Epoch : 4976  Loss : 2.93  Grad : tensor([-0.0001,  0.0006])\n",
      "Epoch : 4977  Loss : 2.93  Grad : tensor([-9.5606e-05,  6.3694e-04])\n",
      "Epoch : 4978  Loss : 2.93  Grad : tensor([-9.8169e-05,  6.3545e-04])\n",
      "Epoch : 4979  Loss : 2.93  Grad : tensor([-0.0001,  0.0006])\n",
      "Epoch : 4980  Loss : 2.93  Grad : tensor([-0.0001,  0.0006])\n",
      "Epoch : 4981  Loss : 2.93  Grad : tensor([-0.0001,  0.0006])\n",
      "Epoch : 4982  Loss : 2.93  Grad : tensor([-0.0001,  0.0006])\n",
      "Epoch : 4983  Loss : 2.93  Grad : tensor([-0.0001,  0.0006])\n",
      "Epoch : 4984  Loss : 2.93  Grad : tensor([-0.0001,  0.0006])\n",
      "Epoch : 4985  Loss : 2.93  Grad : tensor([-0.0001,  0.0006])\n",
      "Epoch : 4986  Loss : 2.93  Grad : tensor([-9.6440e-05,  6.2808e-04])\n",
      "Epoch : 4987  Loss : 2.93  Grad : tensor([-0.0001,  0.0006])\n",
      "Epoch : 4988  Loss : 2.93  Grad : tensor([-0.0001,  0.0006])\n",
      "Epoch : 4989  Loss : 2.93  Grad : tensor([-0.0001,  0.0006])\n",
      "Epoch : 4990  Loss : 2.93  Grad : tensor([-0.0001,  0.0006])\n",
      "Epoch : 4991  Loss : 2.93  Grad : tensor([-0.0001,  0.0006])\n",
      "Epoch : 4992  Loss : 2.93  Grad : tensor([-0.0001,  0.0006])\n",
      "Epoch : 4993  Loss : 2.93  Grad : tensor([-0.0001,  0.0006])\n",
      "Epoch : 4994  Loss : 2.93  Grad : tensor([-9.2626e-05,  6.2042e-04])\n",
      "Epoch : 4995  Loss : 2.93  Grad : tensor([-9.8884e-05,  6.1837e-04])\n",
      "Epoch : 4996  Loss : 2.93  Grad : tensor([-0.0001,  0.0006])\n",
      "Epoch : 4997  Loss : 2.93  Grad : tensor([-0.0001,  0.0006])\n",
      "Epoch : 4998  Loss : 2.93  Grad : tensor([-0.0001,  0.0006])\n",
      "Epoch : 4999  Loss : 2.93  Grad : tensor([-0.0001,  0.0006])\n",
      "Epoch : 5000  Loss : 2.93  Grad : tensor([-0.0001,  0.0006])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([  5.3671, -17.3012])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as no\n",
    "training_loop(\n",
    "    n_epochs = 5000 ,\n",
    "    learning_rate = 1e-2 , \n",
    "    params = torch.tensor([1.0 ,0.0]), \n",
    "    t_u = t_un, \n",
    "    t_c = t_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 시각화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/bin/python\n",
      "Python 3.12.2\n"
     ]
    }
   ],
   "source": [
    "!which python\n",
    "!python --version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "t_p = model(t_un, *params)  # <1>\n",
    "\n",
    "fig = plt.figure(dpi=600)\n",
    "plt.xlabel(\"Temperature (°Fahrenheit)\")\n",
    "plt.ylabel(\"Temperature (°Celsius)\")\n",
    "plt.plot(t_u.numpy(), t_p.detach().numpy()) # <2>\n",
    "plt.plot(t_u.numpy(), t_c.numpy(), 'o')\n",
    "plt.savefig(\"temp_unknown_plot.png\", format=\"png\")  # bookskip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
